{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# === Phase 0: Environment Setup (CRITICAL - Must restart after this) ===\n",
        "# Run this entire cell, then restart the kernel before running any other code\n",
        "\n",
        "# Install core ML libraries (NumPy <2.0 for morl-baselines compatibility)\n",
        "!pip install \"numpy<2.0,>=1.26.4\" scipy==1.11.4\n",
        "\n",
        "# Install RL stack (pycddlib-standalone will be installed but needs patch)\n",
        "!pip install mo-gymnasium morl-baselines gymnasium torch pycddlib-standalone\n",
        "\n",
        "# Install data science libraries\n",
        "!pip install pandas matplotlib seaborn tqdm\n",
        "\n",
        "print(\"âœ… Installation complete!\")\n",
        "print(\"ğŸ”„ CRITICAL: Restart your notebook kernel NOW (Runtime â†’ Restart runtime)\")\n",
        "print(\"After restart, run Phases 1-6, then the monkey-patch below, then Phase 7.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ppxO1gkxq9Os",
        "outputId": "e1c5138c-56e8-4ea0-e947-cd2a38221566"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2.0,>=1.26.4\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.11.4\n",
            "  Downloading scipy-1.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 scipy-1.11.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "a355491f935f483f9d28ca145958b1b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mo-gymnasium\n",
            "  Downloading mo_gymnasium-1.3.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting morl-baselines\n",
            "  Downloading morl_baselines-1.2.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting pycddlib-standalone\n",
            "  Downloading pycddlib_standalone-3.0.0.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from mo-gymnasium) (1.26.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from mo-gymnasium) (2.6.1)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.12/dist-packages (from mo-gymnasium) (1.11.4)\n",
            "Collecting pymoo>=0.6.0 (from morl-baselines)\n",
            "  Downloading pymoo-0.6.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: wandb>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (0.22.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (2.37.2)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (1.0.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (0.13.2)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (1.6.7)\n",
            "Collecting fire (from morl-baselines)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0->morl-baselines) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0->morl-baselines) (1.8.0)\n",
            "Collecting cma>=3.2.2 (from pymoo>=0.6.0->morl-baselines)\n",
            "  Downloading cma-4.4.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting alive-progress (from pymoo>=0.6.0->morl-baselines)\n",
            "  Downloading alive_progress-3.3.0-py3-none-any.whl.metadata (72 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0->morl-baselines) (0.3.8)\n",
            "Collecting Deprecated (from pymoo>=0.6.0->morl-baselines)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.14.0->morl-baselines) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.14.0->morl-baselines) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb>=0.14.0->morl-baselines) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.14.0->morl-baselines) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.14.0->morl-baselines) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.14.0->morl-baselines) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb>=0.14.0->morl-baselines) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.14.0->morl-baselines) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.14.0->morl-baselines) (2.44.0)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from cvxpy->morl-baselines) (1.0.5)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy->morl-baselines) (0.11.1)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.12/dist-packages (from cvxpy->morl-baselines) (3.2.9)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->morl-baselines) (3.2.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio->morl-baselines) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->morl-baselines) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->morl-baselines) (4.67.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->morl-baselines) (0.1.12)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->morl-baselines) (0.6.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn->morl-baselines) (2.2.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from clarabel>=0.5.0->cvxpy->morl-baselines) (2.0.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.14.0->morl-baselines) (4.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy->morl-baselines) (1.5.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->morl-baselines) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->morl-baselines) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.14.0->morl-baselines) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.14.0->morl-baselines) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.14.0->morl-baselines) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb>=0.14.0->morl-baselines) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb>=0.14.0->morl-baselines) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb>=0.14.0->morl-baselines) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb>=0.14.0->morl-baselines) (2025.10.5)\n",
            "Collecting about-time==4.2.1 (from alive-progress->pymoo>=0.6.0->morl-baselines)\n",
            "  Downloading about_time-4.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting graphemeu==0.7.2 (from alive-progress->pymoo>=0.6.0->morl-baselines)\n",
            "  Downloading graphemeu-0.7.2-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from Deprecated->pymoo>=0.6.0->morl-baselines) (2.0.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.14.0->morl-baselines) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3->pymoo>=0.6.0->morl-baselines) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->clarabel>=0.5.0->cvxpy->morl-baselines) (2.23)\n",
            "Downloading mo_gymnasium-1.3.1-py3-none-any.whl (479 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m479.6/479.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading morl_baselines-1.2.0-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.8/140.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymoo-0.6.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cma-4.4.0-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.8/303.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alive_progress-3.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
            "Downloading graphemeu-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: pycddlib-standalone\n",
            "  Building wheel for pycddlib-standalone (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycddlib-standalone: filename=pycddlib_standalone-3.0.0-cp312-cp312-linux_x86_64.whl size=635355 sha256=173fa2a6cee48a0328ff00cd3617afd002d69563a10ea7c53bea16649d6504de\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/2a/7d/f2886a252eda8766707c21cdfd69d58330ec9b78a2bde292c4\n",
            "Successfully built pycddlib-standalone\n",
            "Installing collected packages: pycddlib-standalone, graphemeu, fire, Deprecated, cma, about-time, mo-gymnasium, alive-progress, pymoo, morl-baselines\n",
            "Successfully installed Deprecated-1.3.1 about-time-4.2.1 alive-progress-3.3.0 cma-4.4.0 fire-0.7.1 graphemeu-0.7.2 mo-gymnasium-1.3.1 morl-baselines-1.2.0 pycddlib-standalone-3.0.0 pymoo-0.6.1.5\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "âœ… Installation complete!\n",
            "ğŸ”„ CRITICAL: Restart your notebook kernel NOW (Runtime â†’ Restart runtime)\n",
            "After restart, run Phases 1-6, then the monkey-patch below, then Phase 7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6BzUx_Od_dg"
      },
      "source": [
        "Phase 1: Data Loading & Validation\n",
        "Goal: Load MIMIC-III, SIDER, and DDInter files; perform initial validations to avoid propagating errors downstream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILRvOpsUpRHw",
        "outputId": "ad3dccc2-7e9a-423b-c575-c2cb1ceed0b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFqycFxXd9MP",
        "outputId": "8e3417c4-cd5e-45f5-cf77-645dda9a5d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Data Loading Summary ===\n",
            "PRESCRIPTIONS: (10398, 19)\n",
            "ADMISSIONS: (129, 19)\n",
            "PATIENTS: (100, 8)\n",
            "DDInter: (65389, 5)\n",
            "SIDER drug_names: (1430, 3)\n",
            "SIDER drug_atc: (1560, 2)\n",
            "SIDER side_effects: (309849, 6)\n",
            "\n",
            "=== Key Columns Check ===\n",
            "PRESCRIPTIONS columns: ['row_id', 'subject_id', 'hadm_id', 'icustay_id', 'startdate', 'enddate', 'drug_type', 'drug']...\n",
            "DDInter columns: ['DDInterID_A', 'Drug_A', 'DDInterID_B', 'Drug_B', 'Level']\n",
            "drug_names columns: ['stitch_id_flat', 'stitch_id_stereo', 'drug_name']\n",
            "\n",
            "=== Data Preview ===\n",
            "PRESCRIPTIONS sample:\n",
            "   row_id  subject_id  hadm_id  icustay_id            startdate  \\\n",
            "0   32600       42458   159647         NaN  2146-07-21 00:00:00   \n",
            "1   32601       42458   159647         NaN  2146-07-21 00:00:00   \n",
            "2   32602       42458   159647         NaN  2146-07-21 00:00:00   \n",
            "\n",
            "               enddate drug_type                         drug  \\\n",
            "0  2146-07-22 00:00:00      MAIN  Pneumococcal Vac Polyvalent   \n",
            "1  2146-07-22 00:00:00      MAIN                    Bisacodyl   \n",
            "2  2146-07-22 00:00:00      MAIN                    Bisacodyl   \n",
            "\n",
            "                 drug_name_poe            drug_name_generic formulary_drug_cd  \\\n",
            "0  Pneumococcal Vac Polyvalent  PNEUMOcoccal Vac Polyvalent           PNEU25I   \n",
            "1                    Bisacodyl                    Bisacodyl             BISA5   \n",
            "2                    Bisacodyl           Bisacodyl (Rectal)           BISA10R   \n",
            "\n",
            "       gsn          ndc     prod_strength dose_val_rx dose_unit_rx  \\\n",
            "0  48548.0    6494300.0  25mcg/0.5mL Vial         0.5           mL   \n",
            "1   2947.0  536338101.0          5 mg Tab          10           mg   \n",
            "2   2944.0  574705050.0  10mg Suppository          10           mg   \n",
            "\n",
            "  form_val_disp form_unit_disp route  \n",
            "0             1           VIAL    IM  \n",
            "1             2            TAB    PO  \n",
            "2             1           SUPP    PR  \n",
            "\n",
            "DDInter sample:\n",
            "  DDInterID_A                         Drug_A DDInterID_B               Drug_B  \\\n",
            "0  DDInter127  Asparaginase Escherichia coli    DDInter1             Abacavir   \n",
            "1    DDInter1                       Abacavir  DDInter228  Brentuximab vedotin   \n",
            "2    DDInter1                       Abacavir  DDInter262         Cabozantinib   \n",
            "\n",
            "      Level  \n",
            "0  Moderate  \n",
            "1  Moderate  \n",
            "2  Moderate  \n",
            "\n",
            "âœ… Phase 1 Complete: All data loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Phase 1: Data Loading & Validation ===\n",
        "# Load all datasets from your Google Drive path\n",
        "\n",
        "# MIMIC-III Core Files\n",
        "prescriptions = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/data/mimic3/PRESCRIPTIONS.csv')\n",
        "admissions = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/data/mimic3/ADMISSIONS.csv')\n",
        "patients = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/data/mimic3/PATIENTS.csv')\n",
        "\n",
        "# DDInter Drug-Drug Interaction Database\n",
        "ddinter = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/data/ddinter/ddinter_downloads_code_L.csv')\n",
        "\n",
        "# SIDER Side Effect Database\n",
        "drug_names = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/data/sider/drug_names.tsv',\n",
        "                         sep='\\t', header=None, names=['stitch_id_flat','stitch_id_stereo','drug_name'])\n",
        "drug_atc = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/data/sider/drug_atc.tsv',\n",
        "                       sep='\\t', header=None, names=['stitch_id_flat','atc_code'])\n",
        "side_effects = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/data/sider/meddra_all_se.tsv.gz',\n",
        "                           sep='\\t', header=None, names=['stitch_id_flat','stitch_id_stereo','umls_cui','meddra_type','umls_label','meddra_code'])\n",
        "\n",
        "# === Validation Checks ===\n",
        "print(\"=== Data Loading Summary ===\")\n",
        "print(f\"PRESCRIPTIONS: {prescriptions.shape}\")\n",
        "print(f\"ADMISSIONS: {admissions.shape}\")\n",
        "print(f\"PATIENTS: {patients.shape}\")\n",
        "print(f\"DDInter: {ddinter.shape}\")\n",
        "print(f\"SIDER drug_names: {drug_names.shape}\")\n",
        "print(f\"SIDER drug_atc: {drug_atc.shape}\")\n",
        "print(f\"SIDER side_effects: {side_effects.shape}\")\n",
        "\n",
        "# Check key columns exist\n",
        "print(\"\\n=== Key Columns Check ===\")\n",
        "print(f\"PRESCRIPTIONS columns: {list(prescriptions.columns)[:8]}...\")\n",
        "print(f\"DDInter columns: {list(ddinter.columns)}\")\n",
        "print(f\"drug_names columns: {list(drug_names.columns)}\")\n",
        "\n",
        "# Preview first few rows to verify data integrity\n",
        "print(\"\\n=== Data Preview ===\")\n",
        "print(\"PRESCRIPTIONS sample:\")\n",
        "print(prescriptions.head(3))\n",
        "print(\"\\nDDInter sample:\")\n",
        "print(ddinter.head(3))\n",
        "\n",
        "print(\"\\nâœ… Phase 1 Complete: All data loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3mI087K4p1M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBNWZGgEeO7E"
      },
      "source": [
        "Phase 2: Drug Normalization & ATC Mapping\n",
        "Goal: Standardize drug names and align MIMIC-III drugs with SIDER's ATC codes using fuzzy matching and manual curation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import get_close_matches\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os # Import the os module\n",
        "\n",
        "# Define the output path early\n",
        "out_path = '/content/drive/MyDrive/ML Patent/polypharmacy_project/drug_to_atc_mapping.csv'\n",
        "\n",
        "# Check if the mapping file already exists\n",
        "if os.path.exists(out_path):\n",
        "    drug_mapping = pd.read_csv(out_path)\n",
        "    print(f\"âœ… Drug mapping already exists at {out_path}. Loaded {len(drug_mapping)} entries.\")\n",
        "    print(f\"Total drugs: {len(drug_mapping)}\")\n",
        "    print(f\"Matched to ATC: {drug_mapping['atc_code'].notna().sum()} ({100 * drug_mapping['atc_code'].notna().mean():.1f}%)\")\n",
        "else:\n",
        "    print(\"Drug mapping file not found. Proceeding with mapping process...\")\n",
        "    # 1. Drug Normalization\n",
        "    def normalize_drug_name(drug_string):\n",
        "        \"\"\"Normalize drug names by removing formulations and standardizing case.\"\"\"\n",
        "        if pd.isna(drug_string):\n",
        "            return \"\"\n",
        "        drug = str(drug_string).lower().strip()\n",
        "        # Remove content in parentheses and brackets\n",
        "        drug = drug.split('(')[0].split('[')[0].strip()\n",
        "        # Remove common dosage forms and routes\n",
        "        suffixes = [' tab', ' tablet', ' cap', ' capsule', ' injection', ' inj', ' iv',\n",
        "                    ' oral', ' susp', ' suspension', ' solution', ' liquid', ' cream',\n",
        "                    ' ointment', ' gel', ' drops', ' spray', ' powder', ' patch']\n",
        "        for suffix in suffixes:\n",
        "            drug = drug.replace(suffix, '')\n",
        "        return drug.strip()\n",
        "\n",
        "    # Apply to all unique drugs in prescriptions\n",
        "    mimic_drugs_clean = pd.DataFrame({\n",
        "        'original_name': prescriptions['drug'].unique(),\n",
        "        'normalized_name': [normalize_drug_name(d) for d in prescriptions['drug'].unique()]\n",
        "    })\n",
        "    print(f\"Total unique drugs to map: {len(mimic_drugs_clean)}\")\n",
        "\n",
        "    # 2. SIDER Preparation\n",
        "    # Clean SIDER drug names and create lookup dictionary\n",
        "    drug_names['drug_name_clean'] = drug_names['drug_name'].astype(str).str.lower().str.strip()\n",
        "    sider_drug_atc = drug_names.merge(drug_atc, on='stitch_id_flat', how='inner')\n",
        "\n",
        "    # Create lookup: normalized drug name -> ATC code\n",
        "    sider_lookup = {}\n",
        "    for _, row in sider_drug_atc.iterrows():\n",
        "        if pd.notna(row['drug_name_clean']) and pd.notna(row['atc_code']):\n",
        "            sider_lookup[row['drug_name_clean']] = row['atc_code']\n",
        "\n",
        "    print(f\"SIDER lookup dictionary size: {len(sider_lookup)}\")\n",
        "\n",
        "    # 3. Fuzzy Matching Function\n",
        "    def find_best_match(mimic_drug, sider_lookup, threshold=0.75):\n",
        "        \"\"\"Find best SIDER match using exact then fuzzy matching.\"\"\"\n",
        "        if not mimic_drug:\n",
        "            return None, None, 0.0\n",
        "\n",
        "        # Exact match\n",
        "        if mimic_drug in sider_lookup:\n",
        "            return mimic_drug, sider_lookup[mimic_drug], 1.0\n",
        "\n",
        "        # Fuzzy match\n",
        "        matches = get_close_matches(mimic_drug, list(sider_lookup.keys()), n=1, cutoff=threshold)\n",
        "        if matches:\n",
        "            return matches[0], sider_lookup[matches[0]], threshold\n",
        "\n",
        "        return None, None, 0.0\n",
        "\n",
        "    # 4. Batch Matching with Progress Bar\n",
        "    results = []\n",
        "    for _, row in tqdm(mimic_drugs_clean.iterrows(), total=len(mimic_drugs_clean), desc=\"Mapping drugs\"):\n",
        "        norm = row['normalized_name']\n",
        "        match, atc, conf = find_best_match(norm, sider_lookup)\n",
        "        results.append({\n",
        "            'original_name': row['original_name'],\n",
        "            'normalized_name': norm,\n",
        "            'sider_match': match,\n",
        "            'atc_code': atc,\n",
        "            'match_confidence': conf\n",
        "        })\n",
        "\n",
        "    drug_mapping = pd.DataFrame(results)\n",
        "    print(f\"Successfully mapped: {drug_mapping['atc_code'].notna().sum()}/{len(drug_mapping)}\")\n",
        "\n",
        "    # 5. Manual Mappings for Common Missed Drugs\n",
        "    manual_mappings = {\n",
        "        'heparin': 'B01AB01',\n",
        "        'insulin': 'A10AB01',\n",
        "        'aspirin': 'B01AC06',\n",
        "        'paracetamol': 'N02BE01',\n",
        "        'acetaminophen': 'N02BE01',\n",
        "        'morphine': 'N02AA01',\n",
        "        'fentanyl': 'N02AB03',\n",
        "        'midazolam': 'N05CD08',\n",
        "        'propofol': 'N01AX10',\n",
        "        'norepinephrine': 'C01CA03',\n",
        "        'dopamine': 'C01CA04',\n",
        "        'vancomycin': 'J01XA01',\n",
        "        'cefazolin': 'J01DB04',\n",
        "        'ceftriaxone': 'J01DD04',\n",
        "        'meropenem': 'J01DH02',\n",
        "        'piperacillin': 'J01CA12',\n",
        "        'tazobactam': 'J01CG02',\n",
        "        'metronidazole': 'P01AB01',\n",
        "        'fluconazole': 'J02AC01',\n",
        "        'furosemide': 'C03CA01',\n",
        "        'metoprolol': 'C07AB02'\n",
        "    }\n",
        "\n",
        "    # Apply manual mappings only to unmatched drugs\n",
        "    for idx, row in drug_mapping.iterrows():\n",
        "        if pd.isna(row['atc_code']):\n",
        "            for key, atc in manual_mappings.items():\n",
        "                if key in row['normalized_name']:\n",
        "                    drug_mapping.at[idx, 'atc_code'] = atc\n",
        "                    drug_mapping.at[idx, 'sider_match'] = key\n",
        "                    drug_mapping.at[idx, 'match_confidence'] = 0.95\n",
        "                    break\n",
        "\n",
        "    # 6. Final Statistics and Save\n",
        "    print(\"\\n=== Final Mapping Statistics ===\")\n",
        "    print(f\"Total drugs: {len(drug_mapping)}\")\n",
        "    print(f\"Auto-matched: {(drug_mapping['match_confidence'] >= 0.75).sum()}\")\n",
        "    print(f\"Manual-mapped: {(drug_mapping['match_confidence'] == 0.95).sum()}\")\n",
        "    print(f\"Unmatched: {drug_mapping['atc_code'].isna().sum()}\")\n",
        "\n",
        "    # Save mapping\n",
        "    drug_mapping.to_csv(out_path, index=False)\n",
        "    print(f\"\\nâœ… Saved mapping to: {out_path}\")\n",
        "\n",
        "    # Preview unmatched drugs for manual review\n",
        "    unmatched = drug_mapping[drug_mapping['atc_code'].isna()]['normalized_name'].tolist()\n",
        "    if unmatched:\n",
        "        print(f\"\\nUnmatched drugs (first 10): {unmatched[:10]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbCIvigXqDPk",
        "outputId": "bdb2d361-2c7d-4796-b0bc-797c47236f22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Drug mapping already exists at /content/drive/MyDrive/ML Patent/polypharmacy_project/drug_to_atc_mapping.csv. Loaded 592 entries.\n",
            "Total drugs: 592\n",
            "Matched to ATC: 60 (10.1%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edZZCKujqGFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO5pluJ4ebDV"
      },
      "source": [
        "Phase 3: Cohort Extraction (Adult Polypharmacy)\n",
        "Goal: Filter for adult admissions and identify polypharmacy episodes (â‰¥5 meds on the same day)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "admissions['admittime'] = pd.to_datetime(admissions['admittime'], errors='coerce')\n",
        "patients['dob'] = pd.to_datetime(patients['dob'], errors='coerce')\n",
        "adm_demo = admissions.merge(patients, on='subject_id', how='left')\n",
        "adm_demo['age'] = (adm_demo['admittime'].dt.year - adm_demo['dob'].dt.year)\n",
        "adm_demo.loc[adm_demo['age'] < 0, 'age'] = np.nan\n",
        "adm_demo.loc[adm_demo['age'] > 120, 'age'] = 90\n",
        "adm_demo = adm_demo[adm_demo['age'] >= 18]\n",
        "\n",
        "# Merge to Rx and get normalized ATC\n",
        "prescriptions['startdate'] = pd.to_datetime(prescriptions['startdate'], errors='coerce')\n",
        "adult_cohort = prescriptions.merge(adm_demo[['subject_id','hadm_id','age']], on=['subject_id','hadm_id'], how='inner')\n",
        "drug_mapping = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/drug_to_atc_mapping.csv')\n",
        "adult_cohort = adult_cohort.merge(drug_mapping[['original_name','atc_code']], left_on='drug', right_on='original_name', how='left')\n",
        "adult_cohort['start_day'] = adult_cohort['startdate'].dt.date\n",
        "poly_days = (adult_cohort.groupby(['hadm_id','start_day']).agg({'drug':'nunique'}).reset_index().rename(columns={'drug':'num_drugs'}))\n",
        "poly_adm = poly_days[poly_days['num_drugs'] >= 5]['hadm_id'].unique()\n",
        "poly_cohort = adult_cohort[adult_cohort['hadm_id'].isin(poly_adm)]\n",
        "\n",
        "# Save for next step\n",
        "poly_cohort.to_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/polypharmacy_cohort.csv', index=False)\n"
      ],
      "metadata": {
        "id": "kiVZ4HG4qGyI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6kfQPkM-qJOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTKOcvS7ed7E"
      },
      "source": [
        "Phase 4: Feature Engineering (Drug-Drug Interactions & Tolerability)\n",
        "Goal: Compute DDI statistics and aggregate side effect burden for each admission."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- DDI Analysis ---\n",
        "poly_cohort['drug_norm'] = poly_cohort['drug'].str.lower().str.strip()\n",
        "ddinter = ddinter.rename(columns={'Level':'severity'}) if 'Level' in ddinter.columns else ddinter\n",
        "\n",
        "output_ddi_path = '/content/drive/MyDrive/ML Patent/polypharmacy_project/polypharmacy_cohort_with_ddi.csv'\n",
        "\n",
        "if os.path.exists(output_ddi_path):\n",
        "    print(f\"âœ… DDI features already exist at {output_ddi_path}. Loading existing file.\")\n",
        "    poly_cohort = pd.read_csv(output_ddi_path)\n",
        "else:\n",
        "    print(\"DDI features file not found. Calculating DDI features...\")\n",
        "    ddi_results = []\n",
        "    for hadm_id, group in tqdm(poly_cohort.groupby('hadm_id')):\n",
        "        drugs = group['drug_norm'].unique()\n",
        "        pairs = combinations(drugs, 2)\n",
        "        ddi_count = major_count = moderate_count = 0\n",
        "        for d1, d2 in pairs:\n",
        "            match = ddinter[( (ddinter['Drug_A'].str.lower() == d1) & (ddinter['Drug_B'].str.lower() == d2)) |\n",
        "                            (ddinter['Drug_A'].str.lower() == d2) & (ddinter['Drug_B'].str.lower() == d1)]\n",
        "            if not match.empty:\n",
        "                ddi_count += 1\n",
        "                levels = match['severity'].unique()\n",
        "                if any(l.lower() == 'major' for l in levels): major_count += 1\n",
        "                if any(l.lower() == 'moderate' for l in levels): moderate_count += 1\n",
        "        ddi_results.append({'hadm_id': hadm_id, 'ddi_count': ddi_count, 'major_count': major_count, 'moderate_count': moderate_count, 'n_drugs': len(drugs)})\n",
        "    ddi_df = pd.DataFrame(ddi_results)\n",
        "\n",
        "    # Merge DDI features back to poly_cohort\n",
        "    poly_cohort = poly_cohort.merge(ddi_df, on='hadm_id', how='left')\n",
        "    poly_cohort.to_csv(output_ddi_path, index=False)\n",
        "    print(f\"âœ… DDI features saved to: {output_ddi_path}\")\n",
        "\n",
        "# --- Tolerability (Side Effects) ---\n",
        "output_tol_path = '/content/drive/MyDrive/ML Patent/polypharmacy_project/polypharmacy_tolerability.csv'\n",
        "\n",
        "if os.path.exists(output_tol_path):\n",
        "    print(f\"âœ… Tolerability features already exist at {output_tol_path}. Loading existing file.\")\n",
        "    tol_features = pd.read_csv(output_tol_path)\n",
        "else:\n",
        "    print(\"Tolerability features file not found. Calculating tolerability features...\")\n",
        "\n",
        "    # Re-merge drug_mapping to ensure 'atc_code' is available for side effect calculation\n",
        "    # This is crucial if poly_cohort was loaded from polypharmacy_cohort_with_ddi.csv which might not retain 'atc_code'\n",
        "    if 'atc_code' not in poly_cohort.columns:\n",
        "        print(\"Re-merging drug_mapping to poly_cohort for side effect calculation (atc_code missing).\")\n",
        "        # Assuming 'drug' column in poly_cohort corresponds to 'original_name' in drug_mapping\n",
        "        drug_mapping_for_se = drug_mapping[['original_name', 'atc_code']].drop_duplicates()\n",
        "        # Drop any existing 'original_name' column that might conflict before merging\n",
        "        poly_cohort = poly_cohort.drop(columns=['original_name'], errors='ignore')\n",
        "        poly_cohort = poly_cohort.merge(drug_mapping_for_se, left_on='drug', right_on='original_name', how='left')\n",
        "\n",
        "    sider_drug_atc_map = drug_names.merge(drug_atc, on='stitch_id_flat', how='inner')\n",
        "    se_atc = sider_drug_atc_map.merge(side_effects, on='stitch_id_flat', how='inner')\n",
        "    se_counts = se_atc.groupby('atc_code')['umls_cui'].nunique().reset_index()\n",
        "    se_counts.columns = ['atc_code','side_effect_count']\n",
        "\n",
        "    # Merge side effect counts to the cohort\n",
        "    poly_cohort_with_se = poly_cohort.merge(se_counts, on='atc_code', how='left')\n",
        "\n",
        "    # Aggregate tolerability for each admission\n",
        "    tol_features = poly_cohort_with_se.groupby('hadm_id').agg({\n",
        "        'side_effect_count':['sum','mean'],\n",
        "        'drug':'nunique',\n",
        "        'ddi_count':'first',\n",
        "        'major_count':'first',\n",
        "        'moderate_count':'first'\n",
        "    }).reset_index()\n",
        "    tol_features.columns = ['hadm_id', 'total_se', 'mean_se', 'n_drugs', 'ddi_count', 'major_count', 'moderate_count']\n",
        "\n",
        "    tol_features.to_csv(output_tol_path, index=False)\n",
        "    print(f\"âœ… Tolerability features saved to: {output_tol_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO6tGd7cqMHF",
        "outputId": "92b53e09-6d2e-433e-c762-8d42687d21fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DDI features already exist at /content/drive/MyDrive/ML Patent/polypharmacy_project/polypharmacy_cohort_with_ddi.csv. Loading existing file.\n",
            "âœ… Tolerability features already exist at /content/drive/MyDrive/ML Patent/polypharmacy_project/polypharmacy_tolerability.csv. Loading existing file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4yqnb2c1qNf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-DhoKajejOk"
      },
      "source": [
        "Phase 5: Modeling Table Preparation\n",
        "Goal: Join all structured features, resolve missing data, and split into train/test for RL."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tol = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/polypharmacy_tolerability.csv')\n",
        "admissions = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/data/mimic3/ADMISSIONS.csv')\n",
        "patients = pd.read_csv('/content/drive/MyDrive/ML Patent/polypharmacy_project/data/mimic3/PATIENTS.csv')\n",
        "\n",
        "admissions['admittime'] = pd.to_datetime(admissions['admittime'], errors='coerce')\n",
        "patients['dob'] = pd.to_datetime(patients['dob'], errors='coerce')\n",
        "adm_demo = admissions.merge(patients, on='subject_id', how='left')\n",
        "adm_demo['age'] = adm_demo['admittime'].dt.year - adm_demo['dob'].dt.year\n",
        "adm_demo.loc[adm_demo['age'] < 0, 'age'] = np.nan\n",
        "adm_demo.loc[adm_demo['age'] > 120, 'age'] = 90\n",
        "admissions_min = admissions[['subject_id','hadm_id']].drop_duplicates()\n",
        "model = tol.merge(admissions_min, on='hadm_id', how='left')\n",
        "age_table = adm_demo[['subject_id','hadm_id','age']].dropna().drop_duplicates()\n",
        "model = model.merge(age_table, on=['subject_id','hadm_id'], how='left')\n",
        "\n",
        "for c in ['age','n_drugs','total_se','ddi_count']:\n",
        "    model[c] = model[c].fillna(model[c].median())\n",
        "model_path = '/content/drive/MyDrive/ML Patent/polypharmacy_project/polypharmacy_model_table.csv'\n",
        "model.to_csv(model_path, index=False)\n"
      ],
      "metadata": {
        "id": "3tyKbtsXqQlW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "meOFgBGWqRXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrhMNRxZenGU"
      },
      "source": [
        "Phase 6: RL Environment & Agent (Single-Step Prototype)\n",
        "Goal: Build a Gymnasium environment reflecting your state/action/reward logic and train a multi-objective RL agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import EnvSpec  # CRITICAL IMPORT\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class PolypharmacyEnv(gym.Env):\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        super().__init__()\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.n = len(self.df)\n",
        "\n",
        "        # State: [age, n_drugs, total_se, ddi_count]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0.0, high=1.0, shape=(4,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Actions: 0=reduce risk, 1=maintain\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # CRITICAL: Define reward_space for morl-baselines\n",
        "        self.reward_space = spaces.Box(\n",
        "            low=np.array([-1.0, -10.0, -10.0], dtype=np.float32),\n",
        "            high=np.array([1.0, 0.0, 0.0], dtype=np.float32),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        self.reward_dim = 3\n",
        "\n",
        "        # CRITICAL FIX: Add EnvSpec to prevent AttributeError in morl-baselines\n",
        "        self.spec = EnvSpec(id='Polypharmacy-v0')\n",
        "\n",
        "    def _row_to_obs(self, row):\n",
        "        return np.array([\n",
        "            np.clip(row['age']/100, 0, 1),\n",
        "            np.clip(row['n_drugs']/30, 0, 1),\n",
        "            np.clip(row['total_se']/1000, 0, 1),\n",
        "            np.clip(row['ddi_count']/50, 0, 1)\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def _row_to_reward(self, row, action):\n",
        "        # Efficacy: favor 4-6 drugs (common polypharmacy range)\n",
        "        eff = 1.0 - min(abs(row['n_drugs'] - 5)/25, 1.0)\n",
        "\n",
        "        # ADR risk: negative penalty for DDIs\n",
        "        adr = -float(row['ddi_count'])/50.0\n",
        "\n",
        "        # Tolerability: negative penalty for side effects\n",
        "        tol = -float(row['total_se'])/1000.0\n",
        "\n",
        "        # Action effects\n",
        "        if action == 0:  # Reduce risk\n",
        "            adr *= 0.8\n",
        "            tol *= 0.9\n",
        "\n",
        "        return np.clip([eff, adr, tol],\n",
        "                      self.reward_space.low,\n",
        "                      self.reward_space.high)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.idx = np.random.randint(0, self.n)\n",
        "        row = self.df.iloc[self.idx]\n",
        "        return self._row_to_obs(row), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.df.iloc[self.idx]\n",
        "        reward = self._row_to_reward(row, action)\n",
        "        terminated, truncated = True, False  # One-step episodes\n",
        "        obs, _ = self.reset()\n",
        "        return obs, reward, terminated, truncated, {\"hadm_id\": row['hadm_id']}\n",
        "\n",
        "# Load your modeling table from Phase 5\n",
        "model_path = '/content/drive/MyDrive/ML Patent/polypharmacy_project/polypharmacy_model_table.csv'\n",
        "model = pd.read_csv(model_path)\n",
        "\n",
        "# Split dataset\n",
        "hadms = model['hadm_id'].unique()\n",
        "np.random.shuffle(hadms)\n",
        "split = int(0.8 * len(hadms))\n",
        "train_ids, test_ids = hadms[:split], hadms[split:]\n",
        "\n",
        "train_df = model[model['hadm_id'].isin(train_ids)].reset_index(drop=True)\n",
        "test_df = model[model['hadm_id'].isin(test_ids)].reset_index(drop=True)\n",
        "\n",
        "# Create environments\n",
        "train_env = PolypharmacyEnv(train_df)\n",
        "test_env = PolypharmacyEnv(test_df)\n",
        "\n",
        "# Verify the fix\n",
        "print(f\"âœ… Environment spec ID: {train_env.spec.id}\")\n",
        "print(f\"âœ… Reward space: {train_env.reward_space}\")\n",
        "print(\"âœ… Phase 6 Complete: Environment ready for MORL training!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyYHvzGuqYXe",
        "outputId": "5bb979c3-047c-42e1-f7c5-cb932762fb91"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Environment spec ID: Polypharmacy-v0\n",
            "âœ… Reward space: Box([ -1. -10. -10.], [1. 0. 0.], (3,), float32)\n",
            "âœ… Phase 6 Complete: Environment ready for MORL training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym\n",
        "# from gymnasium import spaces\n",
        "# from gymnasium.envs.registration import EnvSpec  # CRITICAL IMPORT\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# class PolypharmacyEnv(gym.Env):\n",
        "#     def __init__(self, df: pd.DataFrame):\n",
        "#         super().__init__()\n",
        "#         self.df = df.reset_index(drop=True)\n",
        "#         self.n = len(self.df)\n",
        "\n",
        "#         # State: [age, n_drugs, total_se, ddi_count]\n",
        "#         self.observation_space = spaces.Box(\n",
        "#             low=0.0, high=1.0, shape=(4,), dtype=np.float32\n",
        "#         )\n",
        "\n",
        "#         # Actions: 0=reduce risk, 1=maintain\n",
        "#         self.action_space = spaces.Discrete(2)\n",
        "\n",
        "#         # CRITICAL: Define reward_space for morl-baselines\n",
        "#         self.reward_space = spaces.Box(\n",
        "#             low=np.array([-1.0, -10.0, -10.0], dtype=np.float32),\n",
        "#             high=np.array([1.0, 0.0, 0.0], dtype=np.float32),\n",
        "#             dtype=np.float32\n",
        "#         )\n",
        "#         self.reward_dim = 3\n",
        "\n",
        "#         # CRITICAL FIX: Add EnvSpec to prevent AttributeError in morl-baselines\n",
        "#         self.spec = EnvSpec(id='Polypharmacy-v0')\n",
        "\n",
        "#     def _row_to_obs(self, row):\n",
        "#         return np.array([\n",
        "#             np.clip(row['age']/100, 0, 1),\n",
        "#             np.clip(row['n_drugs']/30, 0, 1),\n",
        "#             np.clip(row['total_se']/1000, 0, 1),\n",
        "#             np.clip(row['ddi_count']/50, 0, 1)\n",
        "#         ], dtype=np.float32)\n",
        "\n",
        "#     def _row_to_reward(self, row, action):\n",
        "#         # Efficacy: favor 4-6 drugs (common polypharmacy range)\n",
        "#         eff = 1.0 - min(abs(row['n_drugs'] - 5)/25, 1.0)\n",
        "\n",
        "#         # ADR risk: negative penalty for DDIs\n",
        "#         adr = -float(row['ddi_count'])/50.0\n",
        "\n",
        "#         # Tolerability: negative penalty for side effects\n",
        "#         tol = -float(row['total_se'])/1000.0\n",
        "\n",
        "#         # Action effects\n",
        "#         if action == 0:  # Reduce risk\n",
        "#             adr *= 0.8\n",
        "#             tol *= 0.9\n",
        "\n",
        "#         return np.clip([eff, adr, tol],\n",
        "#                       self.reward_space.low,\n",
        "#                       self.reward_space.high)\n",
        "\n",
        "#     def reset(self, seed=None, options=None):\n",
        "#         super().reset(seed=seed)\n",
        "#         self.idx = np.random.randint(0, self.n)\n",
        "#         row = self.df.iloc[self.idx]\n",
        "#         return self._row_to_obs(row), {}\n",
        "\n",
        "#     def step(self, action):\n",
        "#         row = self.df.iloc[self.idx]\n",
        "#         reward = self._row_to_reward(row, action)\n",
        "#         terminated, truncated = True, False  # One-step episodes\n",
        "#         obs, _ = self.reset()\n",
        "#         return obs, reward, terminated, truncated, {\"hadm_id\": row['hadm_id']}"
      ],
      "metadata": {
        "id": "-NVac3wBedWy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import morl_baselines, pkgutil\n",
        "\n",
        "for _, mod, _ in pkgutil.walk_packages(\n",
        "    morl_baselines.__path__, prefix=\"morl_baselines.\"\n",
        "):\n",
        "    print(mod)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLVF2zXlqcPH",
        "outputId": "b181d0bf-52c0-45cc-a0ee-3a4e52dbc2bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "morl_baselines.common\n",
            "morl_baselines.common.accrued_reward_buffer\n",
            "morl_baselines.common.buffer\n",
            "morl_baselines.common.diverse_buffer\n",
            "morl_baselines.common.evaluation\n",
            "morl_baselines.common.experiments\n",
            "morl_baselines.common.model_based\n",
            "morl_baselines.common.model_based.probabilistic_ensemble\n",
            "morl_baselines.common.model_based.tabular_model\n",
            "morl_baselines.common.model_based.utils\n",
            "morl_baselines.common.morl_algorithm\n",
            "morl_baselines.common.networks\n",
            "morl_baselines.common.pareto\n",
            "morl_baselines.common.performance_indicators\n",
            "morl_baselines.common.prioritized_buffer\n",
            "morl_baselines.common.scalarization\n",
            "morl_baselines.common.utils\n",
            "morl_baselines.common.weights\n",
            "morl_baselines.multi_policy\n",
            "morl_baselines.multi_policy.capql\n",
            "morl_baselines.multi_policy.capql.capql\n",
            "morl_baselines.multi_policy.envelope\n",
            "morl_baselines.multi_policy.envelope.envelope\n",
            "morl_baselines.multi_policy.gpi_pd\n",
            "morl_baselines.multi_policy.gpi_pd.gpi_pd\n",
            "morl_baselines.multi_policy.gpi_pd.gpi_pd_continuous_action\n",
            "morl_baselines.multi_policy.linear_support\n",
            "morl_baselines.multi_policy.linear_support.linear_support\n",
            "morl_baselines.multi_policy.morld\n",
            "morl_baselines.multi_policy.morld.morld\n",
            "morl_baselines.multi_policy.multi_policy_moqlearning\n",
            "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning\n",
            "morl_baselines.multi_policy.pareto_q_learning\n",
            "morl_baselines.multi_policy.pareto_q_learning.pql\n",
            "morl_baselines.multi_policy.pcn\n",
            "morl_baselines.multi_policy.pcn.pcn\n",
            "morl_baselines.multi_policy.pgmorl\n",
            "morl_baselines.multi_policy.pgmorl.pgmorl\n",
            "morl_baselines.single_policy\n",
            "morl_baselines.single_policy.esr\n",
            "morl_baselines.single_policy.esr.eupg\n",
            "morl_baselines.single_policy.ser\n",
            "morl_baselines.single_policy.ser.mo_ppo\n",
            "morl_baselines.single_policy.ser.mo_q_learning\n",
            "morl_baselines.single_policy.ser.mosac_continuous_action\n",
            "morl_baselines.single_policy.ser.mosac_discrete_action\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import morl_baselines.multi_policy.linear_support.linear_support as ls\n",
        "\n",
        "print(inspect.getsource(ls))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6MGChojqkYI",
        "outputId": "30d92ad7-f21b-4dd7-f7f7-b28c30fba727"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\"\"Linear Support implementation.\"\"\"\n",
            "\n",
            "import random\n",
            "from copy import deepcopy\n",
            "from typing import List, Optional\n",
            "\n",
            "import cvxpy as cp\n",
            "import numpy as np\n",
            "from cvxpy import SolverError\n",
            "from gymnasium.core import Env\n",
            "\n",
            "from morl_baselines.common.evaluation import policy_evaluation_mo\n",
            "from morl_baselines.common.morl_algorithm import MOPolicy\n",
            "from morl_baselines.common.performance_indicators import hypervolume\n",
            "from morl_baselines.common.weights import extrema_weights\n",
            "\n",
            "\n",
            "try:\n",
            "    import cdd\n",
            "except ImportError as e:\n",
            "    raise ImportError(\n",
            "        \"To use this feature, you need to install the optional dependency pycddlib==2.1.6 as stated in pyproject.toml of morl-baselines\"\n",
            "    ) from e\n",
            "\n",
            "\n",
            "np.set_printoptions(precision=4)\n",
            "\n",
            "\n",
            "class LinearSupport:\n",
            "    \"\"\"Linear Support for computing corner weights when using linear utility functions.\n",
            "\n",
            "    Implements both\n",
            "\n",
            "    Optimistic Linear Support (OLS) algorithm:\n",
            "    Paper: (Section 3.3 of http://roijers.info/pub/thesis.pdf).\n",
            "\n",
            "    Generalized Policy Improvement Linear Support (GPI-LS) algorithm:\n",
            "    Paper: https://arxiv.org/abs/2301.07784\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        num_objectives: int,\n",
            "        epsilon: float = 0.0,\n",
            "        verbose: bool = True,\n",
            "    ):\n",
            "        \"\"\"Initialize Linear Support.\n",
            "\n",
            "        Args:\n",
            "            num_objectives (int): Number of objectives\n",
            "            epsilon (float, optional): Minimum improvement per iteration. Defaults to 0.0.\n",
            "            verbose (bool): Defaults to False.\n",
            "        \"\"\"\n",
            "        self.num_objectives = num_objectives\n",
            "        self.epsilon = epsilon\n",
            "        self.visited_weights = []  # List of already tested weight vectors\n",
            "        self.ccs = []\n",
            "        self.weight_support = []  # List of weight vectors for each value vector in the CCS\n",
            "        self.queue = []\n",
            "        self.iteration = 0\n",
            "        self.ols_ended = False\n",
            "        self.verbose = verbose\n",
            "        for w in extrema_weights(self.num_objectives):\n",
            "            self.queue.append((float(\"inf\"), w))\n",
            "\n",
            "    def next_weight(\n",
            "        self, algo: str = \"ols\", gpi_agent: Optional[MOPolicy] = None, env: Optional[Env] = None, rep_eval: int = 1\n",
            "    ) -> np.ndarray:\n",
            "        \"\"\"Returns the next weight vector with highest priority.\n",
            "\n",
            "        Args:\n",
            "            algo (str): Algorithm to use. Either 'ols' or 'gpi-ls'.\n",
            "            gpi_agent (Optional[MOPolicy]): Agent to use for GPI-LS.\n",
            "            env (Optional[Env]): Environment to use for GPI-LS.\n",
            "            rep_eval (int): Number of times to evaluate the agent in GPI-LS.\n",
            "\n",
            "        Returns:\n",
            "            np.ndarray: Next weight vector\n",
            "        \"\"\"\n",
            "        if len(self.ccs) > 0:\n",
            "            W_corner = self.compute_corner_weights()\n",
            "            if self.verbose:\n",
            "                print(\"W_corner:\", W_corner, \"W_corner size:\", len(W_corner))\n",
            "\n",
            "            self.queue = []\n",
            "            for wc in W_corner:\n",
            "                if algo == \"ols\":\n",
            "                    priority = self.ols_priority(wc)\n",
            "\n",
            "                elif algo == \"gpi-ls\":\n",
            "                    if gpi_agent is None:\n",
            "                        raise ValueError(\"GPI-LS requires passing a GPI agent.\")\n",
            "                    gpi_expanded_set = [policy_evaluation_mo(gpi_agent, env, wc, rep=rep_eval)[3] for wc in W_corner]\n",
            "                    priority = self.gpi_ls_priority(wc, gpi_expanded_set)\n",
            "\n",
            "                if self.epsilon is None or priority >= self.epsilon:\n",
            "                    # OLS does not try the same weight vector twice\n",
            "                    if not (algo == \"ols\" and any([np.allclose(wc, wv) for wv in self.visited_weights])):\n",
            "                        self.queue.append((priority, wc))\n",
            "\n",
            "            if len(self.queue) > 0:\n",
            "                # Sort in descending order of priority\n",
            "                self.queue.sort(key=lambda t: t[0], reverse=True)\n",
            "                # If all priorities are 0, shuffle the queue to avoid repearting weights every iteration\n",
            "                if self.queue[0][0] == 0.0:\n",
            "                    random.shuffle(self.queue)\n",
            "\n",
            "        if self.verbose:\n",
            "            print(\"CCS:\", self.ccs, \"CCS size:\", len(self.ccs))\n",
            "\n",
            "        if len(self.queue) == 0:\n",
            "            if self.verbose:\n",
            "                print(\"There are no corner weights in the queue. Returning None.\")\n",
            "            self.ols_ended = True\n",
            "            return None\n",
            "        else:\n",
            "            next_w = self.queue.pop(0)[1]\n",
            "            if self.verbose:\n",
            "                print(\"Next weight:\", next_w)\n",
            "            return next_w\n",
            "\n",
            "    def get_weight_support(self) -> List[np.ndarray]:\n",
            "        \"\"\"Returns the weight support of the CCS.\n",
            "\n",
            "        Returns:\n",
            "            List[np.ndarray]: List of weight vectors of the CCS\n",
            "\n",
            "        \"\"\"\n",
            "        return deepcopy(self.weight_support)\n",
            "\n",
            "    def get_corner_weights(self, top_k: Optional[int] = None) -> List[np.ndarray]:\n",
            "        \"\"\"Returns the corner weights of the current CCS.\n",
            "\n",
            "        Args:\n",
            "            top_k: If not None, returns the top_k corner weights.\n",
            "\n",
            "        Returns:\n",
            "            List[np.ndarray]: List of corner weights.\n",
            "        \"\"\"\n",
            "        weights = [w.copy() for (p, w) in self.queue]\n",
            "        if top_k is not None:\n",
            "            return weights[:top_k]\n",
            "        else:\n",
            "            return weights\n",
            "\n",
            "    def ended(self) -> bool:\n",
            "        \"\"\"Returns True if there are no more corner weights to test.\n",
            "\n",
            "        Warning: This method must be called AFTER calling next_weight().\n",
            "        Ex: w = ols.next_weight()\n",
            "            if ols.ended():\n",
            "                print(\"OLS ended.\")\n",
            "        \"\"\"\n",
            "        return self.ols_ended\n",
            "\n",
            "    def add_solution(self, value: np.ndarray, w: np.ndarray) -> List[int]:\n",
            "        \"\"\"Add new value vector optimal to weight w.\n",
            "\n",
            "        Args:\n",
            "            value (np.ndarray): New value vector\n",
            "            w (np.ndarray): Weight vector\n",
            "\n",
            "        Returns:\n",
            "            List of indices of value vectors removed from the CCS for being dominated.\n",
            "        \"\"\"\n",
            "        if self.verbose:\n",
            "            print(f\"Adding value: {value} to CCS.\")\n",
            "\n",
            "        self.iteration += 1\n",
            "        self.visited_weights.append(w)\n",
            "\n",
            "        if self.is_dominated(value):\n",
            "            if self.verbose:\n",
            "                print(f\"Value {value} is dominated. Discarding.\")\n",
            "            return [len(self.ccs)]\n",
            "\n",
            "        removed_indx = self.remove_obsolete_values(value)\n",
            "\n",
            "        self.ccs.append(value)\n",
            "        self.weight_support.append(w)\n",
            "\n",
            "        return removed_indx\n",
            "\n",
            "    def ols_priority(self, w: np.ndarray) -> float:\n",
            "        \"\"\"Get the priority of a weight vector for OLS.\n",
            "\n",
            "        Args:\n",
            "            w: Weight vector\n",
            "\n",
            "        Returns:\n",
            "            Priority of the weight vector.\n",
            "        \"\"\"\n",
            "        max_value_ccs = self.max_scalarized_value(w)\n",
            "        max_optimistic_value = self.max_value_lp(w)\n",
            "        priority = max_optimistic_value - max_value_ccs\n",
            "        return priority\n",
            "\n",
            "    def gpi_ls_priority(self, w: np.ndarray, gpi_expanded_set: List[np.ndarray]) -> float:\n",
            "        \"\"\"Get the priority of a weight vector for GPI-LS.\n",
            "\n",
            "        Args:\n",
            "            w: Weight vector\n",
            "\n",
            "        Returns:\n",
            "            Priority of the weight vector.\n",
            "        \"\"\"\n",
            "\n",
            "        def best_vector(values, w):\n",
            "            max_v = values[0]\n",
            "            for i in range(1, len(values)):\n",
            "                if values[i] @ w > max_v @ w:\n",
            "                    max_v = values[i]\n",
            "            return max_v\n",
            "\n",
            "        max_value_ccs = self.max_scalarized_value(w)\n",
            "        max_value_gpi = best_vector(gpi_expanded_set, w)\n",
            "        max_value_gpi = np.dot(max_value_gpi, w)\n",
            "        priority = max_value_gpi - max_value_ccs\n",
            "\n",
            "        return priority\n",
            "\n",
            "    def max_scalarized_value(self, w: np.ndarray) -> Optional[float]:\n",
            "        \"\"\"Returns the maximum scalarized value for weight vector w.\n",
            "\n",
            "        Args:\n",
            "            w: Weight vector\n",
            "\n",
            "        Returns:\n",
            "            Maximum scalarized value for weight vector w.\n",
            "        \"\"\"\n",
            "        if len(self.ccs) == 0:\n",
            "            return None\n",
            "        return np.max([np.dot(v, w) for v in self.ccs])\n",
            "\n",
            "    def remove_obsolete_weights(self, new_value: np.ndarray) -> List[np.ndarray]:\n",
            "        \"\"\"Remove from the queue the weight vectors for which the new value vector is better than previous values.\n",
            "\n",
            "        Args:\n",
            "            new_value: New value vector\n",
            "\n",
            "        Returns:\n",
            "            List of weight vectors removed from the queue.\n",
            "        \"\"\"\n",
            "        if len(self.ccs) == 0:\n",
            "            return []\n",
            "        W_del = []\n",
            "        inds_remove = []\n",
            "        for i, (priority, cw) in enumerate(self.queue):\n",
            "            if np.dot(cw, new_value) > self.max_scalarized_value(cw):\n",
            "                W_del.append(cw)\n",
            "                inds_remove.append(i)\n",
            "        for i in reversed(inds_remove):\n",
            "            self.queue.pop(i)\n",
            "        return W_del\n",
            "\n",
            "    def remove_obsolete_values(self, value: np.ndarray) -> List[int]:\n",
            "        \"\"\"Removes the values vectors which are no longer optimal for any weight vector after adding the new value vector.\n",
            "\n",
            "        Args:\n",
            "            value (np.ndarray): New value vector\n",
            "\n",
            "        Returns:\n",
            "            The indices of the removed values.\n",
            "        \"\"\"\n",
            "        removed_indx = []\n",
            "        for i in reversed(range(len(self.ccs))):\n",
            "            weights_optimal = [\n",
            "                w\n",
            "                for w in self.visited_weights\n",
            "                if np.dot(self.ccs[i], w) == self.max_scalarized_value(w) and np.dot(value, w) < np.dot(self.ccs[i], w)\n",
            "            ]\n",
            "            if len(weights_optimal) == 0:\n",
            "                print(\"removed value\", self.ccs[i])\n",
            "                removed_indx.append(i)\n",
            "                self.ccs.pop(i)\n",
            "                self.weight_support.pop(i)\n",
            "        return removed_indx\n",
            "\n",
            "    def max_value_lp(self, w_new: np.ndarray) -> float:\n",
            "        \"\"\"Returns an upper-bound for the maximum value of the scalarized objective.\n",
            "\n",
            "        Args:\n",
            "            w_new: New weight vector\n",
            "\n",
            "        Returns:\n",
            "            Upper-bound for the maximum value of the scalarized objective.\n",
            "        \"\"\"\n",
            "        # No upper bound if no values in CCS\n",
            "        if len(self.ccs) == 0:\n",
            "            return float(\"inf\")\n",
            "\n",
            "        w = cp.Parameter(self.num_objectives)\n",
            "        w.value = w_new\n",
            "        v = cp.Variable(self.num_objectives)\n",
            "\n",
            "        W_ = np.vstack(self.visited_weights)\n",
            "        W = cp.Parameter(W_.shape)\n",
            "        W.value = W_\n",
            "\n",
            "        V_ = np.array([self.max_scalarized_value(weight) for weight in self.visited_weights])\n",
            "        V = cp.Parameter(V_.shape)\n",
            "        V.value = V_\n",
            "\n",
            "        # Maximum value for weight vector w\n",
            "        objective = cp.Maximize(w @ v)\n",
            "        # such that it is consistent with other optimal values for other visited weights\n",
            "        constraints = [W @ v <= V]\n",
            "        prob = cp.Problem(objective, constraints)\n",
            "        try:\n",
            "            result = prob.solve(verbose=False)\n",
            "        except SolverError:\n",
            "            print(\"ECOS solver error, trying another one.\")\n",
            "            result = prob.solve(solver=cp.SCS, verbose=False)\n",
            "        return result\n",
            "\n",
            "    def compute_corner_weights(self) -> List[np.ndarray]:\n",
            "        \"\"\"Returns the corner weights for the current set of values.\n",
            "\n",
            "        See http://roijers.info/pub/thesis.pdf Definition 19.\n",
            "        Obs: there is a typo in the definition of the corner weights in the thesis, the >= sign should be <=.\n",
            "\n",
            "        Returns:\n",
            "            List of corner weights.\n",
            "        \"\"\"\n",
            "        A = np.vstack(self.ccs)\n",
            "        A = np.round_(A, decimals=4)  # Round to avoid numerical issues\n",
            "        A = np.concatenate((A, -np.ones(A.shape[0]).reshape(-1, 1)), axis=1)\n",
            "\n",
            "        A_plus = np.ones(A.shape[1]).reshape(1, -1)\n",
            "        A_plus[0, -1] = 0\n",
            "        A = np.concatenate((A, A_plus), axis=0)\n",
            "        A_plus = -np.ones(A.shape[1]).reshape(1, -1)\n",
            "        A_plus[0, -1] = 0\n",
            "        A = np.concatenate((A, A_plus), axis=0)\n",
            "\n",
            "        for i in range(self.num_objectives):\n",
            "            A_plus = np.zeros(A.shape[1]).reshape(1, -1)\n",
            "            A_plus[0, i] = -1\n",
            "            A = np.concatenate((A, A_plus), axis=0)\n",
            "\n",
            "        b = np.zeros(len(self.ccs) + 2 + self.num_objectives)\n",
            "        b[len(self.ccs)] = 1\n",
            "        b[len(self.ccs) + 1] = -1\n",
            "\n",
            "        def compute_poly_vertices(A, b):\n",
            "            # Based on https://stackoverflow.com/questions/65343771/solve-linear-inequalities\n",
            "            b = b.reshape((b.shape[0], 1))\n",
            "            mat = cdd.Matrix(np.hstack([b, -A]), number_type=\"float\")\n",
            "            mat.rep_type = cdd.RepType.INEQUALITY\n",
            "            P = cdd.Polyhedron(mat)\n",
            "            g = P.get_generators()\n",
            "            V = np.array(g)\n",
            "            vertices = []\n",
            "            for i in range(V.shape[0]):\n",
            "                if V[i, 0] != 1:\n",
            "                    continue\n",
            "                if i not in g.lin_set:\n",
            "                    vertices.append(V[i, 1:])\n",
            "            return vertices\n",
            "\n",
            "        vertices = compute_poly_vertices(A, b)\n",
            "        corners = []\n",
            "        for v in vertices:\n",
            "            corner_weight = v[:-1]\n",
            "            # Make sure the corner weight is positive and sum to 1\n",
            "            corner_weight = np.abs(corner_weight)\n",
            "            corner_weight /= corner_weight.sum()\n",
            "            corners.append(corner_weight)\n",
            "\n",
            "        return corners\n",
            "\n",
            "    def is_dominated(self, value: np.ndarray) -> bool:\n",
            "        \"\"\"Checks if the value is dominated by any of the values in the CCS.\n",
            "\n",
            "        Args:\n",
            "            value: Value vector\n",
            "\n",
            "        Returns:\n",
            "            True if the value is dominated by any of the values in the CCS, False otherwise.\n",
            "        \"\"\"\n",
            "        if len(self.ccs) == 0:\n",
            "            return False\n",
            "        for w in self.visited_weights:\n",
            "            if np.dot(value, w) >= self.max_scalarized_value(w):\n",
            "                return False\n",
            "        return True\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "\n",
            "    def _solve(w):\n",
            "        return np.array(list(map(float, input().split())), dtype=np.float32)\n",
            "\n",
            "    num_objectives = 2\n",
            "    ols = LinearSupport(num_objectives=num_objectives, epsilon=0.0001, verbose=True)\n",
            "    w = ols.next_weight()\n",
            "    while not ols.ended():\n",
            "        print(\"w:\", w)\n",
            "        value = _solve(w)\n",
            "        ols.add_solution(value, w)\n",
            "\n",
            "        print(\"hv:\", hypervolume(np.zeros(num_objectives), ols.ccs))\n",
            "        w = ols.next_weight()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Phase 6.5: pycddlib 3.x Compatibility Patch ===\n",
        "# # Run this AFTER Phases 1-6, BEFORE Phase 7\n",
        "\n",
        "# import numpy as np\n",
        "# import cdd\n",
        "# import morl_baselines.multi_policy.linear_support.linear_support as ls_mod\n",
        "\n",
        "# # Save original method\n",
        "# LinearSupport = ls_mod.LinearSupport\n",
        "# _orig_compute_corner_weights = LinearSupport.compute_corner_weights\n",
        "\n",
        "# def patched_compute_corner_weights(self):\n",
        "#     \"\"\"Patched compute_corner_weights for pycddlib 3.x matrix_from_array API\"\"\"\n",
        "#     # === original pre-processing (unchanged) ===\n",
        "#     A = np.vstack(self.ccs)\n",
        "#     A = np.round_(A, 4)\n",
        "#     A = np.concatenate((A, -np.ones((A.shape[0], 1))), axis=1)\n",
        "\n",
        "#     A_plus = np.ones((1, A.shape[1])); A_plus[0, -1] = 0\n",
        "#     A = np.concatenate((A, A_plus), axis=0)\n",
        "#     A_plus = -np.ones((1, A.shape[1])); A_plus[0, -1] = 0\n",
        "#     A = np.concatenate((A, A_plus), axis=0)\n",
        "\n",
        "#     for i in range(self.num_objectives):\n",
        "#         A_plus = np.zeros((1, A.shape[1])); A_plus[0, i] = -1\n",
        "#         A = np.concatenate((A, A_plus), axis=0)\n",
        "\n",
        "#     b = np.zeros(len(self.ccs) + 2 + self.num_objectives)\n",
        "#     b[len(self.ccs)] = 1\n",
        "#     b[len(self.ccs) + 1] = -1\n",
        "\n",
        "#     # === NEW pycddlib 3.x compatible compute_poly_vertices ===\n",
        "#     def compute_poly_vertices(A, b):\n",
        "#         b = b.reshape(-1, 1)\n",
        "#         mat = cdd.matrix_from_array(np.hstack([b, -A]))  # NEW API\n",
        "#         mat.rep_type = cdd.RepType.INEQUALITY\n",
        "\n",
        "#         P = cdd.Polyhedron(mat)\n",
        "#         G = np.array(P.get_generators())\n",
        "\n",
        "#         verts = []\n",
        "#         for row in G:\n",
        "#             if row[0] == 1:  # vertex-type generator\n",
        "#                 verts.append(row[1:])\n",
        "#         return verts\n",
        "\n",
        "#     vertices = compute_poly_vertices(A, b)\n",
        "\n",
        "#     # === post-processing (unchanged) ===\n",
        "#     corners = []\n",
        "#     for v in vertices:\n",
        "#         cw = np.abs(v[:-1])\n",
        "#         s = cw.sum()\n",
        "#         if s > 0:\n",
        "#             corners.append(cw / s)\n",
        "\n",
        "#     return corners\n",
        "\n",
        "# # APPLY PATCH\n",
        "# LinearSupport.compute_corner_weights = patched_compute_corner_weights\n",
        "\n",
        "# print(\"âœ… LinearSupport patched successfully for pycddlib 3.x\")\n"
      ],
      "metadata": {
        "id": "3pWD-DWOrNMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Phase 6.5: SciPy-based Polytope Solver (Python 3.12 Compatible) ===\n",
        "# # Replaces pycddlib with SciPy.spatial.HalfspaceIntersection\n",
        "# # Run AFTER Phases 1-6, BEFORE Phase 7\n",
        "\n",
        "# import numpy as np\n",
        "# from scipy.spatial import HalfspaceIntersection\n",
        "# from scipy.spatial.qhull import QhullError\n",
        "# import morl_baselines.multi_policy.linear_support.linear_support as ls_mod\n",
        "\n",
        "# # Save original\n",
        "# _orig_compute_poly_vertices = ls_mod.compute_poly_vertices\n",
        "\n",
        "# def _scipy_compute_poly_vertices(A, b):\n",
        "#     \"\"\"\n",
        "#     SciPy-based polytope vertex computation.\n",
        "#     Replaces cddlib's compute_poly_vertices with Qhull.\n",
        "#     \"\"\"\n",
        "#     # Ensure inputs are 2D arrays\n",
        "#     b = b.reshape(-1, 1)\n",
        "#     A = np.asarray(A)\n",
        "\n",
        "#     # Build halfspaces: b + A*x >= 0  ->  A*x + b >= 0\n",
        "#     # SciPy format: [b, A] where halfspace is b + A*x >= 0\n",
        "#     halfspaces = np.hstack([b, -A])  # Note: -A because of SciPy's convention\n",
        "\n",
        "#     # Add feasible interior point (simplex center)\n",
        "#     # Use the average of constraint points as interior\n",
        "#     try:\n",
        "#         # For bounded polytopes, use origin as interior if feasible\n",
        "#         interior_point = np.zeros(A.shape[1])\n",
        "\n",
        "#         # If origin fails, use least squares solution\n",
        "#         try:\n",
        "#             hs = HalfspaceIntersection(halfspaces, interior_point)\n",
        "#         except QhullError:\n",
        "#             # Fallback: solve for interior point via least squares\n",
        "#             interior_point = np.linalg.lstsq(A, -b.flatten(), rcond=None)[0]\n",
        "#             hs = HalfspaceIntersection(halfspaces, interior_point)\n",
        "\n",
        "#         # Extract vertices\n",
        "#         vertices = hs.intersections\n",
        "\n",
        "#         # Filter out near-duplicate vertices (numerical tolerance)\n",
        "#         unique_vertices = []\n",
        "#         for v in vertices:\n",
        "#             if not any(np.allclose(v, uv, atol=1e-6) for uv in unique_vertices):\n",
        "#                 unique_vertices.append(v)\n",
        "\n",
        "#         return np.array(unique_vertices)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         # If all fails, return empty array (no vertices)\n",
        "#         print(f\"Warning: SciPy polytope computation failed: {e}\")\n",
        "#         return np.array([])\n",
        "\n",
        "# # Apply patch\n",
        "# ls_mod.compute_poly_vertices = _scipy_compute_poly_vertices\n",
        "\n",
        "# print(\"âœ… SciPy-based polytope solver patched successfully\")\n",
        "# print(\"âœ… Compatible with Python 3.12, no C extensions, no segfaults\")\n"
      ],
      "metadata": {
        "id": "WpJB_o51rWFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # === CORRECT Phase 6.5: SciPy Patch (Target the Right Function) ===\n",
        "\n",
        "# import numpy as np\n",
        "# from scipy.spatial import HalfspaceIntersection, QhullError\n",
        "# import morl_baselines.multi_policy.linear_support.linear_support as ls_mod\n",
        "\n",
        "# # Get the LinearSupport class\n",
        "# LinearSupport = ls_mod.LinearSupport\n",
        "\n",
        "# # Save the original class method\n",
        "# _orig_compute_corner_weights = LinearSupport.compute_corner_weights\n",
        "\n",
        "# def _patched_compute_corner_weights(self):\n",
        "#     \"\"\"\n",
        "#     Patched compute_corner_weights using SciPy instead of pycddlib.\n",
        "#     This replaces the internal polytope computation.\n",
        "#     \"\"\"\n",
        "#     # === original pre-processing (unchanged) ===\n",
        "#     A = np.vstack(self.ccs)\n",
        "#     A = np.round_(A, 4)\n",
        "#     A = np.concatenate((A, -np.ones((A.shape[0], 1))), axis=1)\n",
        "\n",
        "#     A_plus = np.ones((1, A.shape[1])); A_plus[0, -1] = 0\n",
        "#     A = np.concatenate((A, A_plus), axis=0)\n",
        "#     A_plus = -np.ones((1, A.shape[1])); A_plus[0, -1] = 0\n",
        "#     A = np.concatenate((A, A_plus), axis=0)\n",
        "\n",
        "#     for i in range(self.num_objectives):\n",
        "#         A_plus = np.zeros((1, A.shape[1])); A_plus[0, i] = -1\n",
        "#         A = np.concatenate((A, A_plus), axis=0)\n",
        "\n",
        "#     b = np.zeros(len(self.ccs) + 2 + self.num_objectives)\n",
        "#     b[len(self.ccs)] = 1\n",
        "#     b[len(self.ccs) + 1] = -1\n",
        "\n",
        "#     # === NEW: SciPy-based polytope solver (replaces cddlib) ===\n",
        "#     def compute_poly_vertices_scipy(A, b):\n",
        "#         \"\"\"Compute polytope vertices using SciPy.spatial\"\"\"\n",
        "#         b = b.reshape(-1, 1)\n",
        "\n",
        "#         # Build halfspaces: b + A*x >= 0\n",
        "#         halfspaces = np.hstack([b, -A])\n",
        "\n",
        "#         # Add feasible interior point (origin works if feasible)\n",
        "#         try:\n",
        "#             interior_point = np.zeros(A.shape[1])\n",
        "#             hs = HalfspaceIntersection(halfspaces, interior_point)\n",
        "#         except QhullError:\n",
        "#             # Fallback: use least squares solution as interior point\n",
        "#             interior_point = np.linalg.lstsq(A, -b.flatten(), rcond=None)[0]\n",
        "#             hs = HalfspaceIntersection(halfspaces, interior_point)\n",
        "\n",
        "#         # Extract unique vertices\n",
        "#         vertices = hs.intersections\n",
        "#         unique_vertices = []\n",
        "#         for v in vertices:\n",
        "#             if not any(np.allclose(v, uv, atol=1e-6) for uv in unique_vertices):\n",
        "#                 unique_vertices.append(v)\n",
        "\n",
        "#         return np.array(unique_vertices)\n",
        "\n",
        "#     # Compute vertices using SciPy\n",
        "#     vertices = compute_poly_vertices_scipy(A, b)\n",
        "\n",
        "#     # === post-processing (unchanged) ===\n",
        "#     corners = []\n",
        "#     for v in vertices:\n",
        "#         cw = np.abs(v[:-1])\n",
        "#         s = cw.sum()\n",
        "#         if s > 0:\n",
        "#             corners.append(cw / s)\n",
        "\n",
        "#     return corners\n",
        "\n",
        "# # APPLY PATCH to the CLASS METHOD\n",
        "# LinearSupport.compute_corner_weights = _patched_compute_corner_weights\n",
        "\n",
        "# print(\"âœ… LinearSupport.compute_corner_weights patched with SciPy solver\")\n",
        "# print(\"âœ… No pycddlib needed, no segfaults, Python 3.12 compatible\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h6yjzeGvv_t",
        "outputId": "8f31da00-e197-4c8b-f416-e7a14880e0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LinearSupport.compute_corner_weights patched with SciPy solver\n",
            "âœ… No pycddlib needed, no segfaults, Python 3.12 compatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # === CORRECT Phase 6.5: SciPy Patch (Target the Right Function) ===\n",
        "\n",
        "# import numpy as np\n",
        "# from scipy.spatial import HalfspaceIntersection, QhullError\n",
        "# import morl_baselines.multi_policy.linear_support.linear_support as ls_mod\n",
        "\n",
        "# # Get the LinearSupport class\n",
        "# LinearSupport = ls_mod.LinearSupport\n",
        "\n",
        "# # Save the original class method\n",
        "# _orig_compute_corner_weights = LinearSupport.compute_corner_weights\n",
        "\n",
        "# def _patched_compute_corner_weights(self):\n",
        "#     \"\"\"\n",
        "#     Patched compute_corner_weights using SciPy instead of pycddlib.\n",
        "#     This replaces the internal polytope computation.\n",
        "#     \"\"\"\n",
        "#     # === original pre-processing (unchanged) ===\n",
        "#     A = np.vstack(self.ccs)\n",
        "#     A = np.round_(A, 4)\n",
        "#     A = np.concatenate((A, -np.ones((A.shape[0], 1))), axis=1)\n",
        "\n",
        "#     A_plus = np.ones((1, A.shape[1])); A_plus[0, -1] = 0\n",
        "#     A = np.concatenate((A, A_plus), axis=0)\n",
        "#     A_plus = -np.ones((1, A.shape[1])); A_plus[0, -1] = 0\n",
        "#     A = np.concatenate((A, A_plus), axis=0)\n",
        "\n",
        "#     for i in range(self.num_objectives):\n",
        "#         A_plus = np.zeros((1, A.shape[1])); A_plus[0, i] = -1\n",
        "#         A = np.concatenate((A, A_plus), axis=0)\n",
        "\n",
        "#     b = np.zeros(len(self.ccs) + 2 + self.num_objectives)\n",
        "#     b[len(self.ccs)] = 1\n",
        "#     b[len(self.ccs) + 1] = -1\n",
        "\n",
        "#     # === NEW: SciPy-based polytope solver (replaces cddlib) ===\n",
        "#     def compute_poly_vertices_scipy(A, b):\n",
        "#         \"\"\"Compute polytope vertices using SciPy.spatial\"\"\"\n",
        "#         b = b.reshape(-1, 1)\n",
        "\n",
        "#         # Build halfspaces: b + A*x >= 0\n",
        "#         halfspaces = np.hstack([b, -A])\n",
        "\n",
        "#         # Add feasible interior point (origin works if feasible)\n",
        "#         try:\n",
        "#             interior_point = np.zeros(A.shape[1])\n",
        "#             hs = HalfspaceIntersection(halfspaces, interior_point)\n",
        "#         except QhullError:\n",
        "#             # Fallback: use least squares solution as interior point\n",
        "#             interior_point = np.linalg.lstsq(A, -b.flatten(), rcond=None)[0]\n",
        "#             hs = HalfspaceIntersection(halfspaces, interior_point)\n",
        "\n",
        "#         # Extract unique vertices\n",
        "#         vertices = hs.intersections\n",
        "#         unique_vertices = []\n",
        "#         for v in vertices:\n",
        "#             if not any(np.allclose(v, uv, atol=1e-6) for uv in unique_vertices):\n",
        "#                 unique_vertices.append(v)\n",
        "\n",
        "#         return np.array(unique_vertices)\n",
        "\n",
        "#     # Compute vertices using SciPy\n",
        "#     vertices = compute_poly_vertices_scipy(A, b)\n",
        "\n",
        "#     # === post-processing (unchanged) ===\n",
        "#     corners = []\n",
        "#     for v in vertices:\n",
        "#         cw = np.abs(v[:-1])\n",
        "#         s = cw.sum()\n",
        "#         if s > 0:\n",
        "#             corners.append(cw / s)\n",
        "\n",
        "#     return corners\n",
        "\n",
        "# # APPLY PATCH to the CLASS METHOD\n",
        "# LinearSupport.compute_corner_weights = _patched_compute_corner_weights\n",
        "\n",
        "# print(\"âœ… LinearSupport.compute_corner_weights patched with SciPy solver\")\n",
        "# print(\"âœ… No pycddlib needed, no segfaults, Python 3.12 compatible\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2Shpu8Fvype",
        "outputId": "5d1c1425-1304-4119-c2ea-c3d6c7538624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LinearSupport.compute_corner_weights patched with SciPy solver\n",
            "âœ… No pycddlib needed, no segfaults, Python 3.12 compatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import morl_baselines.multi_policy.linear_support.linear_support as ls_mod\n",
        "\n",
        "# print(\"Module attributes:\")\n",
        "# print([attr for attr in dir(ls_mod) if not attr.startswith('_')])\n",
        "\n",
        "# print(\"\\nLinearSupport class methods:\")\n",
        "# print([attr for attr in dir(ls_mod.LinearSupport) if not attr.startswith('_')])\n",
        "\n",
        "# print(\"\\nLinearSupport.compute_corner_weights exists?\",\n",
        "#       hasattr(ls_mod.LinearSupport, 'compute_corner_weights'))\n",
        "\n",
        "# print(\"\\nModule-level compute_poly_vertices exists?\",\n",
        "#       hasattr(ls_mod, 'compute_poly_vertices'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtJUhPdbwbnh",
        "outputId": "026a1ca6-9376-4545-8235-3443a15024cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module attributes:\n",
            "['Env', 'LinearSupport', 'List', 'MOPolicy', 'Optional', 'SolverError', 'cdd', 'cp', 'deepcopy', 'extrema_weights', 'hypervolume', 'np', 'policy_evaluation_mo', 'random']\n",
            "\n",
            "LinearSupport class methods:\n",
            "['add_solution', 'compute_corner_weights', 'ended', 'get_corner_weights', 'get_weight_support', 'gpi_ls_priority', 'is_dominated', 'max_scalarized_value', 'max_value_lp', 'next_weight', 'ols_priority', 'remove_obsolete_values', 'remove_obsolete_weights']\n",
            "\n",
            "LinearSupport.compute_corner_weights exists? True\n",
            "\n",
            "Module-level compute_poly_vertices exists? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Phase 6.5: Robust SciPy-based Polytope Solver (Handles Numerical Issues) ===\n",
        "\n",
        "import numpy as np\n",
        "from scipy.spatial import HalfspaceIntersection\n",
        "import morl_baselines.multi_policy.linear_support.linear_support as ls_mod\n",
        "\n",
        "# Get the LinearSupport class\n",
        "LinearSupport = ls_mod.LinearSupport\n",
        "\n",
        "# Save original method\n",
        "_orig_compute_corner_weights = LinearSupport.compute_corner_weights\n",
        "\n",
        "def _patched_compute_corner_weights(self):\n",
        "    \"\"\"\n",
        "    Patched compute_corner_weights with robust SciPy polytope solver.\n",
        "    Handles QhullError with epsilon padding and fallback strategies.\n",
        "    \"\"\"\n",
        "    # === original pre-processing (unchanged) ===\n",
        "    A = np.vstack(self.ccs)\n",
        "    A = np.round_(A, 4)\n",
        "    A = np.concatenate((A, -np.ones((A.shape[0], 1))), axis=1)\n",
        "\n",
        "    A_plus = np.ones((1, A.shape[1])); A_plus[0, -1] = 0\n",
        "    A = np.concatenate((A, A_plus), axis=0)\n",
        "    A_plus = -np.ones((1, A.shape[1])); A_plus[0, -1] = 0\n",
        "    A = np.concatenate((A, A_plus), axis=0)\n",
        "\n",
        "    for i in range(self.num_objectives):\n",
        "        A_plus = np.zeros((1, A.shape[1])); A_plus[0, i] = -1\n",
        "        A = np.concatenate((A, A_plus), axis=0)\n",
        "\n",
        "    b = np.zeros(len(self.ccs) + 2 + self.num_objectives)\n",
        "    b[len(self.ccs)] = 1\n",
        "    b[len(self.ccs) + 1] = -1\n",
        "\n",
        "    # === ROBUST: SciPy-based polytope solver with epsilon padding ===\n",
        "    def compute_poly_vertices_robust(A, b, epsilon=1e-6, max_iterations=10):\n",
        "        \"\"\"\n",
        "        Compute polytope vertices with robust interior point finding.\n",
        "        Adds epsilon padding to ensure interior point is strictly feasible.\n",
        "        \"\"\"\n",
        "        b = b.reshape(-1, 1)\n",
        "        halfspaces = np.hstack([b, -A])\n",
        "        n_halfspaces, n_dim = halfspaces.shape\n",
        "\n",
        "        # Strategy 1: Try origin with epsilon shift\n",
        "        for attempt in range(max_iterations):\n",
        "            try:\n",
        "                # Add small epsilon to move away from boundaries\n",
        "                offset = epsilon * (attempt + 1)\n",
        "                interior_point = np.full(n_dim - 1, offset)  # -1 because of [b, A] format\n",
        "\n",
        "                # Ensure it's strictly feasible: check all halfspaces\n",
        "                # halfspace: b + A*x >= 0\n",
        "                for i in range(n_halfspaces):\n",
        "                    val = halfspaces[i, 0] + np.dot(halfspaces[i, 1:], interior_point)\n",
        "                    if val <= 0:\n",
        "                        raise ValueError(f\"Halfspace {i} violated: {val}\")\n",
        "\n",
        "                hs = HalfspaceIntersection(halfspaces, interior_point)\n",
        "                break  # Success!\n",
        "\n",
        "            except Exception as e:\n",
        "                if attempt == max_iterations - 1:\n",
        "                    # Strategy 2: Use linear programming relaxation\n",
        "                    print(f\"Warning: Standard interior point failed ({e}), using LP relaxation\")\n",
        "\n",
        "                    # Solve min ||x||Â² subject to A*x + b >= epsilon\n",
        "                    # This is a simple quadratic programming approximation\n",
        "                    from scipy.optimize import minimize\n",
        "\n",
        "                    def objective(x):\n",
        "                        return np.sum(x**2)\n",
        "\n",
        "                    def constraint(i):\n",
        "                        return lambda x: halfspaces[i, 0] + np.dot(halfspaces[i, 1:], x) - epsilon\n",
        "\n",
        "                    constraints = [{'type': 'ineq', 'fun': constraint(i)}\n",
        "                                  for i in range(n_halfspaces)]\n",
        "\n",
        "                    # Start from origin\n",
        "                    result = minimize(objective, np.zeros(n_dim - 1),\n",
        "                                    constraints=constraints, method='SLSQP')\n",
        "\n",
        "                    if result.success:\n",
        "                        interior_point = result.x\n",
        "                        hs = HalfspaceIntersection(halfspaces, interior_point)\n",
        "                    else:\n",
        "                        # Strategy 3: Return empty (degenerate polytope)\n",
        "                        print(\"Warning: Polytope is degenerate or infeasible, returning empty\")\n",
        "                        return np.array([])\n",
        "                else:\n",
        "                    # Try again with larger epsilon\n",
        "                    continue\n",
        "\n",
        "        # Extract vertices and remove duplicates\n",
        "        vertices = hs.intersections\n",
        "        unique_vertices = []\n",
        "\n",
        "        for v in vertices:\n",
        "            # Check if vertex is close to any existing unique vertex\n",
        "            is_duplicate = False\n",
        "            for uv in unique_vertices:\n",
        "                if np.linalg.norm(v - uv) < epsilon * 10:\n",
        "                    is_duplicate = True\n",
        "                    break\n",
        "            if not is_duplicate:\n",
        "                unique_vertices.append(v)\n",
        "\n",
        "        return np.array(unique_vertices)\n",
        "\n",
        "    # Compute vertices\n",
        "    vertices = compute_poly_vertices_robust(A, b)\n",
        "\n",
        "    # === post-processing (unchanged) ===\n",
        "    corners = []\n",
        "    for v in vertices:\n",
        "        cw = np.abs(v[:-1])\n",
        "        s = cw.sum()\n",
        "        if s > 0:\n",
        "            corners.append(cw / s)\n",
        "\n",
        "    return corners\n",
        "\n",
        "# APPLY PATCH to CLASS METHOD\n",
        "LinearSupport.compute_corner_weights = _patched_compute_corner_weights\n",
        "\n",
        "print(\"âœ… Robust SciPy-based LinearSupport patch applied\")\n",
        "print(\"âœ… Handles numerical precision issues with epsilon padding\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHX4qFMfzUdC",
        "outputId": "db13b782-7a54-4c35-bf7b-d418a7e729cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Robust SciPy-based LinearSupport patch applied\n",
            "âœ… Handles numerical precision issues with epsilon padding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P9KPrG4ettV"
      },
      "source": [
        "Phase 7: MORL Training and Evaluation\n",
        "Goal: Train a multi-objective agent and visualize the Pareto front."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4abiTrAYrY00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Phase 7: MORL Training, Evaluation & Saving ===\n",
        "# from morl_baselines.multi_policy.gpi_pd.gpi_pd import GPIPD\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import json\n",
        "# import os\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Ensure save directory exists\n",
        "# save_dir = '/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results'\n",
        "# os.makedirs(save_dir, exist_ok=True)\n",
        "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# # Initialize agent\n",
        "# agent = GPIPD(\n",
        "#     env=train_env,\n",
        "#     learning_rate=3e-4,\n",
        "#     gamma=0.99,\n",
        "#     batch_size=256,\n",
        "#     net_arch=[128, 128],\n",
        "#     buffer_size=int(5e4),\n",
        "#     initial_epsilon=1.0,\n",
        "#     final_epsilon=0.05,\n",
        "#     epsilon_decay_steps=20000,\n",
        "#     learning_starts=1000,\n",
        "#     gradient_updates=1,\n",
        "#     target_net_update_freq=500,\n",
        "#     tau=1.0,\n",
        "#     log=False,\n",
        "#     dyna=False\n",
        "# )\n",
        "# ref_point = train_env.reward_space.low.copy()\n",
        "\n",
        "# # Train agent\n",
        "# print(\"Starting training...\")\n",
        "# agent.train(\n",
        "#     total_timesteps=30000,\n",
        "#     ref_point=ref_point,\n",
        "#     eval_env=test_env,\n",
        "#     eval_freq=5000\n",
        "# )\n",
        "# print(\"âœ… Training complete!\")\n",
        "\n",
        "# # Save trained model\n",
        "# model_path = os.path.join(save_dir, f'gpipd_model_{timestamp}.pkl')\n",
        "# agent.save(model_path)\n",
        "# print(f\"ğŸ’¾ Saved trained model: {model_path}\")\n",
        "\n",
        "# # Evaluation function\n",
        "# def evaluate(agent, env, n_episodes=200):\n",
        "#     out = []\n",
        "#     episode_info = []\n",
        "#     for i in range(n_episodes):\n",
        "#         obs, _ = env.reset()\n",
        "#         done = False\n",
        "#         total = np.zeros(3, dtype=np.float32)\n",
        "#         while not done:\n",
        "#             action, _ = agent.predict(obs, deterministic=True)\n",
        "#             obs, r, done, trunc, info = env.step(action)\n",
        "#             total += r\n",
        "#         out.append(total)\n",
        "#         episode_info.append(info)\n",
        "#     return np.vstack(out), episode_info\n",
        "\n",
        "# # Run evaluation\n",
        "# print(\"Evaluating agent...\")\n",
        "# results, episode_infos = evaluate(agent, test_env)\n",
        "\n",
        "# # Convert to DataFrame for analysis\n",
        "# res_df = pd.DataFrame(results, columns=['efficacy','neg_ddi','neg_tol'])\n",
        "# res_df['hadm_id'] = [info.get('hadm_id') for info in episode_infos]\n",
        "\n",
        "# # Save evaluation results\n",
        "# results_path = os.path.join(save_dir, f'evaluation_results_{timestamp}.csv')\n",
        "# res_df.to_csv(results_path, index=False)\n",
        "# print(f\"ğŸ’¾ Saved evaluation results: {results_path}\")\n",
        "\n",
        "# # Save hyperparameters\n",
        "# hyperparams = {\n",
        "#     'learning_rate': 3e-4,\n",
        "#     'gamma': 0.99,\n",
        "#     'batch_size': 256,\n",
        "#     'net_arch': [128, 128],\n",
        "#     'buffer_size': int(5e4),\n",
        "#     'epsilon_decay_steps': 20000,\n",
        "#     'total_timesteps': 30000,\n",
        "#     'n_eval_episodes': 200,\n",
        "#     'ref_point': ref_point.tolist(),\n",
        "#     'dyna': False,\n",
        "#     'pycddlib_patch': 'matrix_from_array API'\n",
        "# }\n",
        "# hyperparams_path = os.path.join(save_dir, f'hyperparameters_{timestamp}.json')\n",
        "# with open(hyperparams_path, 'w') as f:\n",
        "#     json.dump(hyperparams, f, indent=2)\n",
        "# print(f\"ğŸ’¾ Saved hyperparameters: {hyperparams_path}\")\n",
        "\n",
        "# # Generate and save Pareto front plot\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# scatter = plt.scatter(res_df['efficacy'], res_df['neg_ddi'],\n",
        "#                      c=res_df['neg_tol'], cmap='viridis', alpha=0.6, s=50)\n",
        "# plt.xlabel('Efficacy Proxy (Higher Better)', fontsize=12)\n",
        "# plt.ylabel('DDI Risk (Less Negative Better)', fontsize=12)\n",
        "# plt.colorbar(scatter, label='Tolerability (Less Negative Better)')\n",
        "# plt.title('Pareto-optimal Prescription Policies\\nMulti-Objective RL on MIMIC-III Polypharmacy',\n",
        "#           fontsize=14, fontweight='bold')\n",
        "# plt.grid(True, alpha=0.3)\n",
        "\n",
        "# # Add statistics text\n",
        "# mean_vals = res_df.mean()\n",
        "# std_vals = res_df.std()\n",
        "# stats_text = f\"Mean Efficacy: {mean_vals['efficacy']:.3f} Â± {std_vals['efficacy']:.3f}\\nMean DDI: {mean_vals['neg_ddi']:.3f} Â± {std_vals['neg_ddi']:.3f}\\nMean Tolerability: {mean_vals['neg_tol']:.3f} Â± {std_vals['neg_tol']:.3f}\"\n",
        "# plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,\n",
        "#          fontsize=10, verticalalignment='top',\n",
        "#          bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "# plt.tight_layout()\n",
        "\n",
        "# # Save plot\n",
        "# plot_path = os.path.join(save_dir, f'pareto_front_{timestamp}.png')\n",
        "# plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "# print(f\"ğŸ’¾ Saved Pareto plot: {plot_path}\")\n",
        "# plt.show()\n",
        "\n",
        "# # Summary statistics\n",
        "# print(\"\\n=== Final Performance Summary ===\")\n",
        "# print(f\"Mean Â± Std (Efficacy): {mean_vals['efficacy']:.4f} Â± {std_vals['efficacy']:.4f}\")\n",
        "# print(f\"Mean Â± Std (DDI Risk): {mean_vals['neg_ddi']:.4f} Â± {std_vals['neg_ddi']:.4f}\")\n",
        "# print(f\"Mean Â± Std (Tolerability): {mean_vals['neg_tol']:.4f} Â± {std_vals['neg_tol']:.4f}\")\n",
        "\n",
        "# # Save summary report\n",
        "# summary = {\n",
        "#     'timestamp': timestamp,\n",
        "#     'n_eval_episodes': len(res_df),\n",
        "#     'mean_efficacy': float(mean_vals['efficacy']),\n",
        "#     'std_efficacy': float(std_vals['efficacy']),\n",
        "#     'mean_ddi': float(mean_vals['neg_ddi']),\n",
        "#     'std_ddi': float(std_vals['neg_ddi']),\n",
        "#     'mean_tolerability': float(mean_vals['neg_tol']),\n",
        "#     'std_tolerability': float(std_vals['neg_tol']),\n",
        "#     'model_path': model_path,\n",
        "#     'results_path': results_path,\n",
        "#     'plot_path': plot_path,\n",
        "#     'hyperparams_path': hyperparams_path,\n",
        "#     'model_based_learning': False,\n",
        "#     'pycddlib_patch_applied': True\n",
        "# }\n",
        "# summary_path = os.path.join(save_dir, f'summary_{timestamp}.json')\n",
        "# with open(summary_path, 'w') as f:\n",
        "#     json.dump(summary, f, indent=2)\n",
        "# print(f\"ğŸ’¾ Saved summary report: {summary_path}\")\n",
        "\n",
        "# print(f\"\\nğŸ‰ Phase 7 Complete! All artifacts saved to: {save_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "jTFiMe7zrOQS",
        "outputId": "bb081b53-f2aa-4f93-9188-c9a662b11624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "CCS: [] CCS size: 0\n",
            "Next weight: [1. 0. 0.]\n",
            "Next weight vector: [1. 0. 0.]\n",
            "Adding value: [ 0.16  -0.144 -1.937] to CCS.\n",
            "Adding value: [ 0.096   0.     -2.7441] to CCS.\n",
            "Adding value: [ 0.128   0.     -2.0666] to CCS.\n",
            "removed value [ 0.096   0.     -2.7441]\n",
            "Warning: Standard interior point failed (Halfspace 3 violated: -0.99997), using LP relaxation\n",
            "Warning: Polytope is degenerate or infeasible, returning empty\n",
            "W_corner: [] W_corner size: 0\n",
            "CCS: [array([ 0.16 , -0.144, -1.937], dtype=float32), array([ 0.128 ,  0.    , -2.0666], dtype=float32)] CCS size: 2\n",
            "There are no corner weights in the queue. Returning None.\n",
            "âœ… Training complete!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GPIPD' object has no attribute 'experiment_name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-644423878.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Save trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'gpipd_model_{timestamp}.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ’¾ Saved trained model: {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/morl_baselines/multi_policy/gpi_pd/gpi_pd.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, save_replay_buffer, save_dir, filename)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_replay_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0msaved_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"replay_buffer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GPIPD' object has no attribute 'experiment_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Phase 7: MORL Training, Evaluation & Saving (COMPLETE, FIXED) ===\n",
        "# from morl_baselines.multi_policy.gpi_pd.gpi_pd import GPIPD\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import json\n",
        "# import os\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Ensure save directory exists\n",
        "# save_dir = '/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results'\n",
        "# os.makedirs(save_dir, exist_ok=True)\n",
        "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# # Initialize agent\n",
        "# agent = GPIPD(\n",
        "#     env=train_env,\n",
        "#     learning_rate=3e-4,\n",
        "#     gamma=0.99,\n",
        "#     batch_size=256,\n",
        "#     net_arch=[128, 128],\n",
        "#     buffer_size=int(5e4),\n",
        "#     initial_epsilon=1.0,\n",
        "#     final_epsilon=0.05,\n",
        "#     epsilon_decay_steps=20000,\n",
        "#     learning_starts=1000,\n",
        "#     gradient_updates=1,\n",
        "#     target_net_update_freq=500,\n",
        "#     tau=1.0,\n",
        "#     log=False,\n",
        "#     dyna=False\n",
        "# )\n",
        "# ref_point = train_env.reward_space.low.copy()\n",
        "\n",
        "# # Train agent\n",
        "# print(\"Starting training...\")\n",
        "# agent.train(\n",
        "#     total_timesteps=30000,\n",
        "#     ref_point=ref_point,\n",
        "#     eval_env=test_env,\n",
        "#     eval_freq=5000\n",
        "# )\n",
        "# print(\"âœ… Training complete!\")\n",
        "\n",
        "# # FIX: Add missing attributes that morl-baselines expects\n",
        "# agent.experiment_name = \"gpipd_polypharmacy\"\n",
        "# agent.save_dir = save_dir\n",
        "\n",
        "# # Save trained model\n",
        "# model_path = os.path.join(save_dir, f'gpipd_model_{timestamp}')\n",
        "# agent.save(model_path)\n",
        "# print(f\"ğŸ’¾ Saved trained model: {model_path}.tar\")\n",
        "\n",
        "# # Evaluation function\n",
        "# def evaluate(agent, env, n_episodes=200):\n",
        "#     out = []\n",
        "#     episode_info = []\n",
        "#     for i in range(n_episodes):\n",
        "#         obs, _ = env.reset()\n",
        "#         done = False\n",
        "#         total = np.zeros(3, dtype=np.float32)\n",
        "#         while not done:\n",
        "#             action, _ = agent.predict(obs, deterministic=True)\n",
        "#             obs, r, done, trunc, info = env.step(action)\n",
        "#             total += r\n",
        "#         out.append(total)\n",
        "#         episode_info.append(info)\n",
        "#     return np.vstack(out), episode_info\n",
        "\n",
        "# # Run evaluation\n",
        "# print(\"Evaluating agent...\")\n",
        "# results, episode_infos = evaluate(agent, test_env)\n",
        "\n",
        "# # Convert to DataFrame for analysis\n",
        "# res_df = pd.DataFrame(results, columns=['efficacy','neg_ddi','neg_tol'])\n",
        "# res_df['hadm_id'] = [info.get('hadm_id') for info in episode_infos]\n",
        "\n",
        "# # Save evaluation results\n",
        "# results_path = os.path.join(save_dir, f'evaluation_results_{timestamp}.csv')\n",
        "# res_df.to_csv(results_path, index=False)\n",
        "# print(f\"ğŸ’¾ Saved evaluation results: {results_path}\")\n",
        "\n",
        "# # Save hyperparameters\n",
        "# hyperparams = {\n",
        "#     'learning_rate': 3e-4,\n",
        "#     'gamma': 0.99,\n",
        "#     'batch_size': 256,\n",
        "#     'net_arch': [128, 128],\n",
        "#     'buffer_size': int(5e4),\n",
        "#     'epsilon_decay_steps': 20000,\n",
        "#     'total_timesteps': 30000,\n",
        "#     'n_eval_episodes': 200,\n",
        "#     'ref_point': ref_point.tolist(),\n",
        "#     'dyna': False,\n",
        "#     'pycddlib_patch': 'scipy_based'\n",
        "# }\n",
        "# hyperparams_path = os.path.join(save_dir, f'hyperparameters_{timestamp}.json')\n",
        "# with open(hyperparams_path, 'w') as f:\n",
        "#     json.dump(hyperparams, f, indent=2)\n",
        "# print(f\"ğŸ’¾ Saved hyperparameters: {hyperparams_path}\")\n",
        "\n",
        "# # Generate and save Pareto front plot\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# scatter = plt.scatter(res_df['efficacy'], res_df['neg_ddi'],\n",
        "#                      c=res_df['neg_tol'], cmap='viridis', alpha=0.6, s=50)\n",
        "# plt.xlabel('Efficacy Proxy (Higher Better)', fontsize=12)\n",
        "# plt.ylabel('DDI Risk (Less Negative Better)', fontsize=12)\n",
        "# plt.colorbar(scatter, label='Tolerability (Less Negative Better)')\n",
        "# plt.title('Pareto-optimal Prescription Policies\\nMulti-Objective RL on MIMIC-III Polypharmacy',\n",
        "#           fontsize=14, fontweight='bold')\n",
        "# plt.grid(True, alpha=0.3)\n",
        "\n",
        "# # Add statistics text\n",
        "# mean_vals = res_df.mean()\n",
        "# std_vals = res_df.std()\n",
        "# stats_text = f\"Mean Efficacy: {mean_vals['efficacy']:.3f} Â± {std_vals['efficacy']:.3f}\\nMean DDI: {mean_vals['neg_ddi']:.3f} Â± {std_vals['neg_ddi']:.3f}\\nMean Tolerability: {mean_vals['neg_tol']:.3f} Â± {std_vals['neg_tol']:.3f}\"\n",
        "# plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,\n",
        "#          fontsize=10, verticalalignment='top',\n",
        "#          bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "# plt.tight_layout()\n",
        "\n",
        "# # Save plot\n",
        "# plot_path = os.path.join(save_dir, f'pareto_front_{timestamp}.png')\n",
        "# plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "# print(f\"ğŸ’¾ Saved Pareto plot: {plot_path}\")\n",
        "# plt.show()\n",
        "\n",
        "# # Summary statistics\n",
        "# print(\"\\n=== Final Performance Summary ===\")\n",
        "# print(f\"Mean Â± Std (Efficacy): {mean_vals['efficacy']:.4f} Â± {std_vals['efficacy']:.4f}\")\n",
        "# print(f\"Mean Â± Std (DDI Risk): {mean_vals['neg_ddi']:.4f} Â± {std_vals['neg_ddi']:.4f}\")\n",
        "# print(f\"Mean Â± Std (Tolerability): {mean_vals['neg_tol']:.4f} Â± {std_vals['neg_tol']:.4f}\")\n",
        "\n",
        "# # Save summary report\n",
        "# summary = {\n",
        "#     'timestamp': timestamp,\n",
        "#     'n_eval_episodes': len(res_df),\n",
        "#     'mean_efficacy': float(mean_vals['efficacy']),\n",
        "#     'std_efficacy': float(std_vals['efficacy']),\n",
        "#     'mean_ddi': float(mean_vals['neg_ddi']),\n",
        "#     'std_ddi': float(std_vals['neg_ddi']),\n",
        "#     'mean_tolerability': float(mean_vals['neg_tol']),\n",
        "#     'std_tolerability': float(std_vals['neg_tol']),\n",
        "#     'model_path': model_path,\n",
        "#     'results_path': results_path,\n",
        "#     'plot_path': plot_path,\n",
        "#     'hyperparams_path': hyperparams_path,\n",
        "#     'model_based_learning': False,\n",
        "#     'pycddlib_patch_applied': True\n",
        "# }\n",
        "# summary_path = os.path.join(save_dir, f'summary_{timestamp}.json')\n",
        "# with open(summary_path, 'w') as f:\n",
        "#     json.dump(summary, f, indent=2)\n",
        "# print(f\"ğŸ’¾ Saved summary report: {summary_path}\")\n",
        "\n",
        "# print(f\"\\nğŸ‰ Phase 7 Complete! All artifacts saved to: {save_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "SNAweu1JsCvq",
        "outputId": "f8c90ddf-9db4-4897-ed26-874e95cc1b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "CCS: [] CCS size: 0\n",
            "Next weight: [1. 0. 0.]\n",
            "Next weight vector: [1. 0. 0.]\n",
            "Adding value: [ 0.12   -0.064  -1.6117] to CCS.\n",
            "Adding value: [ 0.096  -0.144  -1.7006] to CCS.\n",
            "Value [ 0.096  -0.144  -1.7006] is dominated. Discarding.\n",
            "Adding value: [ 0.464   0.     -0.8406] to CCS.\n",
            "removed value [ 0.12   -0.064  -1.6117]\n",
            "Warning: Standard interior point failed (Halfspace 2 violated: -0.99997), using LP relaxation\n",
            "Warning: Polytope is degenerate or infeasible, returning empty\n",
            "W_corner: [] W_corner size: 0\n",
            "CCS: [array([ 0.464 ,  0.    , -0.8406], dtype=float32)] CCS size: 1\n",
            "There are no corner weights in the queue. Returning None.\n",
            "âœ… Training complete!\n",
            "ğŸ’¾ Saved trained model: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/gpipd_model_20251117_172144.tar\n",
            "Evaluating agent...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GPIPD' object has no attribute 'predict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1461082540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating agent...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_infos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Convert to DataFrame for analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1461082540.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(agent, env, n_episodes)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GPIPD' object has no attribute 'predict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Phase 7a: Training (Conditional) ===\n",
        "from morl_baselines.multi_policy.gpi_pd.gpi_pd import GPIPD\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "save_dir = '/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Check if model already exists\n",
        "existing_models = [f for f in os.listdir(save_dir) if f.startswith('gpipd_model_') and f.endswith('.tar')]\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "if existing_models:\n",
        "    # Use latest model\n",
        "    latest_model = sorted(existing_models)[-1]\n",
        "    model_timestamp = latest_model.replace('gpipd_model_', '').replace('.tar', '')\n",
        "    print(f\"âœ… Found existing model: {latest_model}\")\n",
        "    print(\"â­ï¸  Skipping training...\")\n",
        "\n",
        "    # We'll just load this model in Phase 7b\n",
        "    agent = None  # Placeholder\n",
        "\n",
        "else:\n",
        "    # No model found - train from scratch\n",
        "    print(\"ğŸ”„ No model found. Starting training...\")\n",
        "\n",
        "    # Initialize agent\n",
        "    agent = GPIPD(\n",
        "        env=train_env,\n",
        "        learning_rate=3e-4,\n",
        "        gamma=0.99,\n",
        "        batch_size=256,\n",
        "        net_arch=[128, 128],\n",
        "        buffer_size=int(5e4),\n",
        "        initial_epsilon=1.0,\n",
        "        final_epsilon=0.05,\n",
        "        epsilon_decay_steps=20000,\n",
        "        learning_starts=1000,\n",
        "        gradient_updates=1,\n",
        "        target_net_update_freq=500,\n",
        "        tau=1.0,\n",
        "        log=False,\n",
        "        dyna=False\n",
        "    )\n",
        "    ref_point = train_env.reward_space.low.copy()\n",
        "\n",
        "    # Train\n",
        "    print(\"Starting training...\")\n",
        "    agent.train(\n",
        "        total_timesteps=30000,\n",
        "        ref_point=ref_point,\n",
        "        eval_env=test_env,\n",
        "        eval_freq=5000\n",
        "    )\n",
        "    print(\"âœ… Training complete!\")\n",
        "\n",
        "    # FIX: Add missing attributes\n",
        "    agent.experiment_name = \"gpipd_polypharmacy\"\n",
        "    agent.save_dir = save_dir\n",
        "\n",
        "    # Save model\n",
        "    model_path = os.path.join(save_dir, f'gpipd_model_{timestamp}')\n",
        "    agent.save(model_path)\n",
        "    print(f\"ğŸ’¾ Saved trained model: {model_path}.tar\")\n",
        "\n",
        "    # Store for Phase 7b\n",
        "    agent = agent  # Keep reference\n",
        "\n",
        "print(\"\\nâœ… Phase 7a complete. Ready for evaluation (Phase 7b).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRCZm7P_41sS",
        "outputId": "a2153c4c-dc8d-4c2a-bc0b-47d6f5b9605c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Found existing model: gpipd_model_20251117_175812.tar\n",
            "â­ï¸  Skipping training...\n",
            "\n",
            "âœ… Phase 7a complete. Ready for evaluation (Phase 7b).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lha /content/drive/MyDrive/ML\\ Patent/polypharmacy_project/phase7_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeBezvFy7_7K",
        "outputId": "aa1f013c-3c3e-4807-a312-08b75d6c6d19"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4.2M\n",
            "-rw------- 1 root root 1.9K Nov 18 05:26 clinical_interp.md\n",
            "-rw------- 1 root root 5.0K Nov 18 04:58 evaluation_results_20251117_175812.csv\n",
            "-rw------- 1 root root 4.0M Nov 17 17:58 gpipd_model_20251117_175812.tar\n",
            "-rw------- 1 root root  269 Nov 18 04:58 hyperparameters_20251117_175812.json\n",
            "-rw------- 1 root root 1.9K Nov 18 05:26 manuscript.md\n",
            "-rw------- 1 root root 179K Nov 18 04:58 pareto_front_20251117_175812.png\n",
            "drwx------ 2 root root 4.0K Nov 18 04:49 phase8_results_20251118_044945\n",
            "drwx------ 2 root root 4.0K Nov 18 04:57 phase8_results_20251118_045718\n",
            "drwx------ 2 root root 4.0K Nov 18 05:24 phase8_results_20251118_050009\n",
            "-rw------- 1 root root  884 Nov 18 04:58 summary_20251117_175812.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Phase 7a: Training or Loading + Clean Save ===\n",
        "from morl_baselines.multi_policy.gpi_pd.gpi_pd import GPIPD\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Detect existing models\n",
        "existing_models = [f for f in os.listdir(save_dir) if f.startswith(\"gpipd_model_\") and f.endswith(\".tar\")]\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "trained_now = False\n",
        "\n",
        "# === CASE 1: MODEL EXISTS ===\n",
        "if existing_models:\n",
        "    latest_tar = sorted(existing_models)[-1]\n",
        "    print(f\"ğŸ“¦ Found existing model: {latest_tar}\")\n",
        "\n",
        "    # Full file path including .tar\n",
        "    model_path = os.path.join(save_dir, latest_tar)\n",
        "\n",
        "    print(f\"â¡ï¸  Loading GPIPD model from: {model_path}\")\n",
        "\n",
        "    # Recreate GPIPD agent architecture (required before load)\n",
        "    agent = GPIPD(\n",
        "        env=train_env,\n",
        "        learning_rate=3e-4,\n",
        "        gamma=0.99,\n",
        "        batch_size=256,\n",
        "        net_arch=[128, 128],\n",
        "        buffer_size=int(5e4),\n",
        "        initial_epsilon=0.05,\n",
        "        final_epsilon=0.05,\n",
        "        log=False,\n",
        "        dyna=False\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: load the .tar directly\n",
        "    agent.load(model_path)\n",
        "\n",
        "# === CASE 2: NO MODEL â†’ TRAIN NEW ===\n",
        "else:\n",
        "    print(\"ğŸš€ No model found. Training new GPIPD...\")\n",
        "\n",
        "    agent = GPIPD(\n",
        "        env=train_env,\n",
        "        learning_rate=3e-4,\n",
        "        gamma=0.99,\n",
        "        batch_size=256,\n",
        "        net_arch=[128, 128],\n",
        "        buffer_size=int(5e4),\n",
        "        initial_epsilon=1.0,\n",
        "        final_epsilon=0.05,\n",
        "        epsilon_decay_steps=20000,\n",
        "        learning_starts=1000,\n",
        "        gradient_updates=1,\n",
        "        target_net_update_freq=500,\n",
        "        tau=1.0,\n",
        "        log=False,\n",
        "        dyna=False\n",
        "    )\n",
        "\n",
        "    ref_point = train_env.reward_space.low.copy()\n",
        "    agent.train(\n",
        "        total_timesteps=30000,\n",
        "        ref_point=ref_point,\n",
        "        eval_env=test_env,\n",
        "        eval_freq=5000\n",
        "    )\n",
        "\n",
        "    agent.experiment_name = \"gpipd_polypharmacy\"\n",
        "    agent.save_dir = save_dir\n",
        "\n",
        "    # Save with .tar\n",
        "    model_path = os.path.join(save_dir, f\"gpipd_model_{timestamp}.tar\")\n",
        "    agent.save(model_path.replace(\".tar\", \"\"))   # GPIPD.save adds .tar automatically\n",
        "\n",
        "    print(f\"ğŸ’¾ Saved full model: {model_path}\")\n",
        "    trained_now = True\n",
        "\n",
        "# =====================================================\n",
        "# === CLEAN MODEL EXPORT FOR STREAMLIT ===============\n",
        "# =====================================================\n",
        "\n",
        "print(\"ğŸ”§ Extracting q_networks + weight_supportâ€¦\")\n",
        "\n",
        "clean_path = \"/content/gpipd_model_clean.pt\"\n",
        "\n",
        "torch.save({\n",
        "    \"q_networks\": [q.state_dict() for q in agent.q_networks],\n",
        "    \"weight_support\": agent.weight_support,\n",
        "}, clean_path)\n",
        "\n",
        "print(f\"âœ… Saved clean deployable model: {clean_path}\")\n",
        "print(\"Phase 7a complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "qUi2Q42bpv3_",
        "outputId": "a2a6b36f-6bae-43c6-f437-6267c05293db"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Found existing model: gpipd_model_20251117_175812.tar\n",
            "â¡ï¸  Loading GPIPD model from: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/gpipd_model_20251117_175812.tar\n",
            "ğŸ”§ Extracting q_networks + weight_supportâ€¦\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GPIPD' object has no attribute 'q_networks'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1038121512.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m torch.save({\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;34m\"q_networks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_networks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;34m\"weight_support\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m }, clean_path)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GPIPD' object has no attribute 'q_networks'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ”§ Extracting q_nets + weight_supportâ€¦\")\n",
        "\n",
        "clean_path = \"/content/gpipd_model_clean.pt\"\n",
        "\n",
        "torch.save({\n",
        "    \"q_nets\": [q.state_dict() for q in agent.q_nets],   # FIXED\n",
        "    \"weight_support\": agent.weight_support,             # OK\n",
        "}, clean_path)\n",
        "\n",
        "print(f\"âœ… Saved clean deployable model: {clean_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niF75taGq00C",
        "outputId": "aa874263-2158-49ab-c58e-9622d6d8dc00"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Extracting q_nets + weight_supportâ€¦\n",
            "âœ… Saved clean deployable model: /content/gpipd_model_clean.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# # Force-save model (GPIPD uses policy_net + target_net + replay buffer)\n",
        "# forced_model_path = model_path + \".pth\"\n",
        "\n",
        "# torch.save({\n",
        "#     \"policy_net\": agent.policy_net.state_dict(),\n",
        "#     \"target_net\": agent.target_net.state_dict(),\n",
        "#     \"replay_buffer\": agent.replay_buffer,   # stored as Python object\n",
        "#     \"epsilon\": agent.epsilon,\n",
        "#     \"iteration\": agent.iteration,\n",
        "# }, forced_model_path)\n",
        "\n",
        "# print(f\"ğŸ”¥ FORCE-SAVED model to: {forced_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "p-ifc8wO7_xy",
        "outputId": "c9a525fb-b4d8-4333-a403-d12a1c9b4772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GPIPD' object has no attribute 'policy_net'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3358625233.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m torch.save({\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m\"policy_net\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"target_net\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"replay_buffer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# stored as Python object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GPIPD' object has no attribute 'policy_net'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[type(v) for v in agent.__dict__.values()]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdpfeQw-9m7n",
        "outputId": "aa2df3d8-798d-47d2-f436-167e1c298626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[__main__.PolypharmacyEnv,\n",
              " tuple,\n",
              " int,\n",
              " gymnasium.spaces.discrete.Discrete,\n",
              " tuple,\n",
              " numpy.int64,\n",
              " int,\n",
              " torch.device,\n",
              " int,\n",
              " int,\n",
              " NoneType,\n",
              " numpy.random._generator.Generator,\n",
              " NoneType,\n",
              " float,\n",
              " float,\n",
              " numpy.float64,\n",
              " int,\n",
              " float,\n",
              " float,\n",
              " int,\n",
              " float,\n",
              " NoneType,\n",
              " bool,\n",
              " int,\n",
              " list,\n",
              " int,\n",
              " int,\n",
              " int,\n",
              " int,\n",
              " float,\n",
              " bool,\n",
              " list,\n",
              " list,\n",
              " torch.optim.adam.Adam,\n",
              " bool,\n",
              " bool,\n",
              " morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer,\n",
              " float,\n",
              " float,\n",
              " bool,\n",
              " list,\n",
              " NoneType,\n",
              " NoneType,\n",
              " function,\n",
              " int,\n",
              " bool,\n",
              " int,\n",
              " int,\n",
              " int,\n",
              " int,\n",
              " int,\n",
              " int,\n",
              " float,\n",
              " float,\n",
              " bool,\n",
              " list,\n",
              " list,\n",
              " str,\n",
              " str]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(agent))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUUqIUiH9b0f",
        "outputId": "908de22c-566c-4293-8185-6f9f5d3a80bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['_MOPolicy__report', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_act', '_envelope_target', '_reset_priorities', '_rollout_dynamics', '_sample_batch_experiences', 'action_dim', 'action_shape', 'action_space', 'alpha', 'batch_size', 'buffer_size', 'close_wandb', 'device', 'drop_rate', 'dyna', 'dynamics', 'dynamics_buffer', 'dynamics_buffer_size', 'dynamics_ensemble_size', 'dynamics_net_arch', 'dynamics_normalize_inputs', 'dynamics_num_elites', 'dynamics_rollout_batch_size', 'dynamics_rollout_freq', 'dynamics_rollout_len', 'dynamics_rollout_starts', 'dynamics_train_freq', 'dynamics_uncertainty_threshold', 'env', 'epsilon', 'epsilon_decay_steps', 'eval', 'experiment_name', 'extract_env_info', 'final_epsilon', 'gamma', 'get_buffer', 'get_config', 'get_policy_net', 'get_save_dict', 'global_step', 'gpi_action', 'gpi_pd', 'gradient_updates', 'id', 'initial_epsilon', 'layer_norm', 'learning_rate', 'learning_starts', 'load', 'log', 'max_action', 'max_grad_norm', 'min_priority', 'net_arch', 'np_random', 'num_episodes', 'num_nets', 'observation_dim', 'observation_shape', 'per', 'police_indices', 'policy_eval', 'policy_eval_esr', 'q_nets', 'q_optim', 'real_ratio', 'register_additional_config', 'replay_buffer', 'reward_dim', 'save', 'save_dir', 'seed', 'set_buffer', 'set_weight_support', 'set_weights', 'setup_wandb', 'target_net_update_freq', 'target_q_nets', 'tau', 'train', 'train_iteration', 'update', 'use_gpi', 'weight_support']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "print(inspect.getsource(agent.__class__))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI9HHwdb9j-i",
        "outputId": "7cda4645-5d75-4c6a-db50-717eebae2a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class GPIPD(MOPolicy, MOAgent):\n",
            "    \"\"\"GPI-PD Algorithm.\n",
            "\n",
            "    Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization\n",
            "    Lucas N. Alegre, Ana L. C. Bazzan, Diederik M. Roijers, Ann NowÃ©, Bruno C. da Silva\n",
            "    AAMAS 2023\n",
            "    Paper: https://arxiv.org/abs/2301.07784\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        env,\n",
            "        learning_rate: float = 3e-4,\n",
            "        initial_epsilon: float = 0.01,\n",
            "        final_epsilon: float = 0.01,\n",
            "        epsilon_decay_steps: int = None,  # None == fixed epsilon\n",
            "        tau: float = 1.0,\n",
            "        target_net_update_freq: int = 1000,  # ignored if tau != 1.0\n",
            "        buffer_size: int = int(1e6),\n",
            "        net_arch: List = [256, 256, 256, 256],\n",
            "        num_nets: int = 2,\n",
            "        batch_size: int = 128,\n",
            "        learning_starts: int = 100,\n",
            "        gradient_updates: int = 20,\n",
            "        gamma: float = 0.99,\n",
            "        max_grad_norm: Optional[float] = None,\n",
            "        use_gpi: bool = True,\n",
            "        dyna: bool = True,\n",
            "        per: bool = True,\n",
            "        gpi_pd: bool = True,\n",
            "        alpha_per: float = 0.6,\n",
            "        min_priority: float = 0.01,\n",
            "        drop_rate: float = 0.01,\n",
            "        layer_norm: bool = True,\n",
            "        dynamics_normalize_inputs: bool = False,\n",
            "        dynamics_uncertainty_threshold: float = 1.5,\n",
            "        dynamics_train_freq: Callable = lambda timestep: 250,\n",
            "        dynamics_rollout_len: int = 1,\n",
            "        dynamics_rollout_starts: int = 5000,\n",
            "        dynamics_rollout_freq: int = 250,\n",
            "        dynamics_rollout_batch_size: int = 25000,\n",
            "        dynamics_buffer_size: int = 100000,\n",
            "        dynamics_net_arch: List = [256, 256, 256],\n",
            "        dynamics_ensemble_size: int = 5,\n",
            "        dynamics_num_elites: int = 2,\n",
            "        real_ratio: float = 0.5,\n",
            "        project_name: str = \"MORL-Baselines\",\n",
            "        experiment_name: str = \"GPI-PD\",\n",
            "        wandb_entity: Optional[str] = None,\n",
            "        log: bool = True,\n",
            "        seed: Optional[int] = None,\n",
            "        device: Union[th.device, str] = \"auto\",\n",
            "    ):\n",
            "        \"\"\"Initialize the GPI-PD algorithm.\n",
            "\n",
            "        Args:\n",
            "            env: The environment to learn from.\n",
            "            learning_rate: The learning rate.\n",
            "            initial_epsilon: The initial epsilon value.\n",
            "            final_epsilon: The final epsilon value.\n",
            "            epsilon_decay_steps: The number of steps to decay epsilon.\n",
            "            tau: The soft update coefficient.\n",
            "            target_net_update_freq: The target network update frequency.\n",
            "            buffer_size: The size of the replay buffer.\n",
            "            net_arch: The network architecture.\n",
            "            num_nets: The number of networks.\n",
            "            batch_size: The batch size.\n",
            "            learning_starts: The number of steps before learning starts.\n",
            "            gradient_updates: The number of gradient updates per step.\n",
            "            gamma: The discount factor.\n",
            "            max_grad_norm: The maximum gradient norm.\n",
            "            use_gpi: Whether to use GPI.\n",
            "            dyna: Whether to use Dyna.\n",
            "            per: Whether to use PER.\n",
            "            gpi_pd: Whether to use GPI-PD.\n",
            "            alpha_per: The alpha parameter for PER.\n",
            "            min_priority: The minimum priority for PER.\n",
            "            drop_rate: The dropout rate.\n",
            "            layer_norm: Whether to use layer normalization.\n",
            "            dynamics_normalize_inputs: Whether to normalize inputs to the dynamics model.\n",
            "            dynamics_uncertainty_threshold: The uncertainty threshold for the dynamics model.\n",
            "            dynamics_train_freq: The dynamics model training frequency.\n",
            "            dynamics_rollout_len: The rollout length for the dynamics model.\n",
            "            dynamics_rollout_starts: The number of steps before the first rollout.\n",
            "            dynamics_rollout_freq: The rollout frequency.\n",
            "            dynamics_rollout_batch_size: The rollout batch size.\n",
            "            dynamics_buffer_size: The size of the dynamics model buffer.\n",
            "            dynamics_net_arch: The network architecture for the dynamics model.\n",
            "            dynamics_ensemble_size: The ensemble size for the dynamics model.\n",
            "            dynamics_num_elites: The number of elites for the dynamics model.\n",
            "            real_ratio: The ratio of real transitions to sample.\n",
            "            project_name: The name of the project.\n",
            "            experiment_name: The name of the experiment.\n",
            "            wandb_entity: The name of the wandb entity.\n",
            "            log: Whether to log.\n",
            "            seed: The seed for random number generators.\n",
            "            device: The device to use.\n",
            "        \"\"\"\n",
            "        MOAgent.__init__(self, env, device=device, seed=seed)\n",
            "        MOPolicy.__init__(self, device=device)\n",
            "        self.learning_rate = learning_rate\n",
            "        self.initial_epsilon = initial_epsilon\n",
            "        self.epsilon = initial_epsilon\n",
            "        self.epsilon_decay_steps = epsilon_decay_steps\n",
            "        self.final_epsilon = final_epsilon\n",
            "        self.tau = tau\n",
            "        self.target_net_update_freq = target_net_update_freq\n",
            "        self.gamma = gamma\n",
            "        self.max_grad_norm = max_grad_norm\n",
            "        self.use_gpi = use_gpi\n",
            "        self.buffer_size = buffer_size\n",
            "        self.net_arch = net_arch\n",
            "        self.learning_starts = learning_starts\n",
            "        self.batch_size = batch_size\n",
            "        self.gradient_updates = gradient_updates\n",
            "        self.num_nets = num_nets\n",
            "        self.drop_rate = drop_rate\n",
            "        self.layer_norm = layer_norm\n",
            "\n",
            "        # Q-Networks\n",
            "        self.q_nets = [\n",
            "            QNet(\n",
            "                self.observation_shape,\n",
            "                self.action_dim,\n",
            "                self.reward_dim,\n",
            "                net_arch=net_arch,\n",
            "                drop_rate=drop_rate,\n",
            "                layer_norm=layer_norm,\n",
            "            ).to(self.device)\n",
            "            for _ in range(self.num_nets)\n",
            "        ]\n",
            "        self.target_q_nets = [\n",
            "            QNet(\n",
            "                self.observation_shape,\n",
            "                self.action_dim,\n",
            "                self.reward_dim,\n",
            "                net_arch=net_arch,\n",
            "                drop_rate=drop_rate,\n",
            "                layer_norm=layer_norm,\n",
            "            ).to(self.device)\n",
            "            for _ in range(self.num_nets)\n",
            "        ]\n",
            "        for q, target_q in zip(self.q_nets, self.target_q_nets):\n",
            "            target_q.load_state_dict(q.state_dict())\n",
            "            for param in target_q.parameters():\n",
            "                param.requires_grad = False\n",
            "        self.q_optim = optim.Adam(chain(*[net.parameters() for net in self.q_nets]), lr=self.learning_rate)\n",
            "\n",
            "        # Prioritized experience replay parameters\n",
            "        self.per = per\n",
            "        self.gpi_pd = gpi_pd\n",
            "        if self.per:\n",
            "            self.replay_buffer = PrioritizedReplayBuffer(\n",
            "                self.observation_shape, 1, rew_dim=self.reward_dim, max_size=buffer_size, action_dtype=np.uint8\n",
            "            )\n",
            "        else:\n",
            "            self.replay_buffer = ReplayBuffer(\n",
            "                self.observation_shape, 1, rew_dim=self.reward_dim, max_size=buffer_size, action_dtype=np.uint8\n",
            "            )\n",
            "        self.min_priority = min_priority\n",
            "        self.alpha = alpha_per\n",
            "\n",
            "        # model-based parameters\n",
            "        self.dyna = dyna\n",
            "        self.dynamics_net_arch = dynamics_net_arch\n",
            "        self.dynamics = None\n",
            "        self.dynamics_buffer = None\n",
            "        if self.dyna:\n",
            "            self.dynamics = ProbabilisticEnsemble(\n",
            "                input_dim=self.observation_dim + self.action_dim,\n",
            "                output_dim=self.observation_dim + self.reward_dim,\n",
            "                arch=self.dynamics_net_arch,\n",
            "                normalize_inputs=dynamics_normalize_inputs,\n",
            "                ensemble_size=dynamics_ensemble_size,\n",
            "                num_elites=dynamics_num_elites,\n",
            "                device=self.device,\n",
            "            )\n",
            "            self.dynamics_buffer = ReplayBuffer(\n",
            "                self.observation_shape, 1, rew_dim=self.reward_dim, max_size=dynamics_buffer_size, action_dtype=np.uint8\n",
            "            )\n",
            "        self.dynamics_train_freq = dynamics_train_freq\n",
            "        self.dynamics_buffer_size = dynamics_buffer_size\n",
            "        self.dynamics_normalize_inputs = dynamics_normalize_inputs\n",
            "        self.dynamics_num_elites = dynamics_num_elites\n",
            "        self.dynamics_ensemble_size = dynamics_ensemble_size\n",
            "        self.dynamics_rollout_len = dynamics_rollout_len\n",
            "        self.dynamics_rollout_starts = dynamics_rollout_starts\n",
            "        self.dynamics_rollout_freq = dynamics_rollout_freq\n",
            "        self.dynamics_rollout_batch_size = dynamics_rollout_batch_size\n",
            "        self.dynamics_uncertainty_threshold = dynamics_uncertainty_threshold\n",
            "        self.real_ratio = real_ratio\n",
            "\n",
            "        # logging\n",
            "        self.log = log\n",
            "        if self.log:\n",
            "            self.setup_wandb(project_name, experiment_name, wandb_entity)\n",
            "\n",
            "    def get_config(self):\n",
            "        \"\"\"Return the configuration of the agent.\"\"\"\n",
            "        return {\n",
            "            \"env_id\": self.env.unwrapped.spec.id,\n",
            "            \"learning_rate\": self.learning_rate,\n",
            "            \"initial_epsilon\": self.initial_epsilon,\n",
            "            \"epsilon_decay_steps:\": self.epsilon_decay_steps,\n",
            "            \"batch_size\": self.batch_size,\n",
            "            \"per\": self.per,\n",
            "            \"gpi_pd\": self.gpi_pd,\n",
            "            \"alpha_per\": self.alpha,\n",
            "            \"min_priority\": self.min_priority,\n",
            "            \"tau\": self.tau,\n",
            "            \"num_nets\": self.num_nets,\n",
            "            \"clip_grand_norm\": self.max_grad_norm,\n",
            "            \"target_net_update_freq\": self.target_net_update_freq,\n",
            "            \"gamma\": self.gamma,\n",
            "            \"net_arch\": self.net_arch,\n",
            "            \"dynamics_model_arch\": self.dynamics_net_arch,\n",
            "            \"gradient_updates\": self.gradient_updates,\n",
            "            \"buffer_size\": self.buffer_size,\n",
            "            \"learning_starts\": self.learning_starts,\n",
            "            \"dyna\": self.dyna,\n",
            "            \"dynamics_rollout_len\": self.dynamics_rollout_len,\n",
            "            \"dynamics_uncertainty_threshold\": self.dynamics_uncertainty_threshold,\n",
            "            \"dynamics_rollout_starts\": self.dynamics_rollout_starts,\n",
            "            \"dynamics_rollout_freq\": self.dynamics_rollout_freq,\n",
            "            \"dynamics_rollout_batch_size\": self.dynamics_rollout_batch_size,\n",
            "            \"dynamics_buffer_size\": self.dynamics_buffer_size,\n",
            "            \"dynamics_normalize_inputs\": self.dynamics_normalize_inputs,\n",
            "            \"dynamics_ensemble_size\": self.dynamics_ensemble_size,\n",
            "            \"dynamics_num_elites\": self.dynamics_num_elites,\n",
            "            \"real_ratio\": self.real_ratio,\n",
            "            \"drop_rate\": self.drop_rate,\n",
            "            \"layer_norm\": self.layer_norm,\n",
            "            \"seed\": self.seed,\n",
            "        }\n",
            "\n",
            "    def save(self, save_replay_buffer=True, save_dir=\"weights/\", filename=None):\n",
            "        \"\"\"Save the model parameters and the replay buffer.\"\"\"\n",
            "        if not os.path.isdir(save_dir):\n",
            "            os.makedirs(save_dir)\n",
            "        saved_params = {}\n",
            "        for i, psi_net in enumerate(self.q_nets):\n",
            "            saved_params[f\"psi_net_{i}_state_dict\"] = psi_net.state_dict()\n",
            "        saved_params[\"psi_nets_optimizer_state_dict\"] = self.q_optim.state_dict()\n",
            "        saved_params[\"M\"] = self.weight_support\n",
            "        if self.dyna:\n",
            "            saved_params[\"dynamics_state_dict\"] = self.dynamics.state_dict()\n",
            "        if save_replay_buffer:\n",
            "            saved_params[\"replay_buffer\"] = self.replay_buffer\n",
            "        filename = self.experiment_name if filename is None else filename\n",
            "        th.save(saved_params, save_dir + \"/\" + filename + \".tar\")\n",
            "\n",
            "    def load(self, path, load_replay_buffer=True):\n",
            "        \"\"\"Load the model parameters and the replay buffer.\"\"\"\n",
            "        params = th.load(path, map_location=self.device, weights_only=False)\n",
            "        for i, (psi_net, target_psi_net) in enumerate(zip(self.q_nets, self.target_q_nets)):\n",
            "            psi_net.load_state_dict(params[f\"psi_net_{i}_state_dict\"])\n",
            "            target_psi_net.load_state_dict(params[f\"psi_net_{i}_state_dict\"])\n",
            "        self.q_optim.load_state_dict(params[\"psi_nets_optimizer_state_dict\"])\n",
            "        self.weight_support = params[\"M\"]\n",
            "        if self.dyna:\n",
            "            self.dynamics.load_state_dict(params[\"dynamics_state_dict\"])\n",
            "        if load_replay_buffer and \"replay_buffer\" in params:\n",
            "            self.replay_buffer = params[\"replay_buffer\"]\n",
            "\n",
            "    def _sample_batch_experiences(self):\n",
            "        if not self.dyna or self.global_step < self.dynamics_rollout_starts or len(self.dynamics_buffer) == 0:\n",
            "            return self.replay_buffer.sample(self.batch_size, to_tensor=True, device=self.device)\n",
            "        else:\n",
            "            num_real_samples = int(self.batch_size * self.real_ratio)  # real_ratio% of real world data\n",
            "            if self.per:\n",
            "                s_obs, s_actions, s_rewards, s_next_obs, s_dones, idxes = self.replay_buffer.sample(\n",
            "                    num_real_samples, to_tensor=True, device=self.device\n",
            "                )\n",
            "            else:\n",
            "                s_obs, s_actions, s_rewards, s_next_obs, s_dones = self.replay_buffer.sample(\n",
            "                    num_real_samples, to_tensor=True, device=self.device\n",
            "                )\n",
            "            m_obs, m_actions, m_rewards, m_next_obs, m_dones = self.dynamics_buffer.sample(\n",
            "                self.batch_size - num_real_samples, to_tensor=True, device=self.device\n",
            "            )\n",
            "            experience_tuples = (\n",
            "                th.cat([s_obs, m_obs], dim=0),\n",
            "                th.cat([s_actions, m_actions], dim=0),\n",
            "                th.cat([s_rewards, m_rewards], dim=0),\n",
            "                th.cat([s_next_obs, m_next_obs], dim=0),\n",
            "                th.cat([s_dones, m_dones], dim=0),\n",
            "            )\n",
            "            if self.per:\n",
            "                return experience_tuples + (idxes,)\n",
            "            return experience_tuples\n",
            "\n",
            "    @th.no_grad()\n",
            "    def _rollout_dynamics(self, w: th.Tensor):\n",
            "        # Dyna Planning\n",
            "        num_times = int(np.ceil(self.dynamics_rollout_batch_size / 10000))\n",
            "        batch_size = min(self.dynamics_rollout_batch_size, 10000)\n",
            "        num_added_imagined_transitions = 0\n",
            "        for iteration in range(num_times):\n",
            "            obs = self.replay_buffer.sample_obs(batch_size, to_tensor=False)\n",
            "            model_env = ModelEnv(self.dynamics, self.env.unwrapped.spec.id, rew_dim=len(w))\n",
            "\n",
            "            for h in range(self.dynamics_rollout_len):\n",
            "                obs = th.tensor(obs).to(self.device)\n",
            "                M = th.stack(self.weight_support)\n",
            "                M = M.unsqueeze(0).repeat(len(obs), 1, 1)\n",
            "                obs_m = obs.unsqueeze(1).repeat(1, M.size(1), 1)\n",
            "\n",
            "                psi_values = self.q_nets[0](obs_m, M)\n",
            "                q_values = th.einsum(\"r,bar->ba\", w, psi_values).view(obs.size(0), len(self.weight_support), self.action_dim)\n",
            "                max_q, ac = th.max(q_values, dim=2)\n",
            "                pi = th.argmax(max_q, dim=1)\n",
            "                actions = ac.gather(1, pi.unsqueeze(1))\n",
            "                actions_one_hot = F.one_hot(actions, num_classes=self.action_dim).squeeze(1)\n",
            "\n",
            "                next_obs_pred, r_pred, dones, info = model_env.step(obs, actions_one_hot, deterministic=False)\n",
            "                uncertainties = info[\"uncertainty\"]\n",
            "                obs, actions = obs.cpu().numpy(), actions.cpu().numpy()\n",
            "\n",
            "                for i in range(len(obs)):\n",
            "                    if uncertainties[i] < self.dynamics_uncertainty_threshold:\n",
            "                        self.dynamics_buffer.add(obs[i], actions[i], r_pred[i], next_obs_pred[i], dones[i])\n",
            "                        num_added_imagined_transitions += 1\n",
            "\n",
            "                nonterm_mask = ~dones.squeeze(-1)\n",
            "                if nonterm_mask.sum() == 0:\n",
            "                    break\n",
            "                obs = next_obs_pred[nonterm_mask]\n",
            "\n",
            "        if self.log:\n",
            "            wandb.log(\n",
            "                {\n",
            "                    \"dynamics/uncertainty_mean\": uncertainties.mean(),\n",
            "                    \"dynamics/uncertainty_max\": uncertainties.max(),\n",
            "                    \"dynamics/uncertainty_min\": uncertainties.min(),\n",
            "                    \"dynamics/model_buffer_size\": len(self.dynamics_buffer),\n",
            "                    \"dynamics/imagined_transitions\": num_added_imagined_transitions,\n",
            "                    \"global_step\": self.global_step,\n",
            "                },\n",
            "            )\n",
            "\n",
            "    def update(self, weight: th.Tensor):\n",
            "        \"\"\"Update the parameters of the networks.\"\"\"\n",
            "        critic_losses = []\n",
            "        for g in range(self.gradient_updates if self.global_step >= self.dynamics_rollout_starts else 1):\n",
            "            if self.per:\n",
            "                s_obs, s_actions, s_rewards, s_next_obs, s_dones, idxes = self._sample_batch_experiences()\n",
            "            else:\n",
            "                s_obs, s_actions, s_rewards, s_next_obs, s_dones = self._sample_batch_experiences()\n",
            "\n",
            "            if len(self.weight_support) > 1:\n",
            "                s_obs, s_actions, s_rewards, s_next_obs, s_dones = (\n",
            "                    s_obs.repeat(2, *(1 for _ in range(s_obs.dim() - 1))),\n",
            "                    s_actions.repeat(2, 1),\n",
            "                    s_rewards.repeat(2, 1),\n",
            "                    s_next_obs.repeat(2, *(1 for _ in range(s_obs.dim() - 1))),\n",
            "                    s_dones.repeat(2, 1),\n",
            "                )\n",
            "                # Half of the batch uses the given weight vector, the other half uses weights sampled from the support set\n",
            "                w = th.vstack(\n",
            "                    [weight for _ in range(s_obs.size(0) // 2)] + random.choices(self.weight_support, k=s_obs.size(0) // 2)\n",
            "                )\n",
            "            else:\n",
            "                w = weight.repeat(s_obs.size(0), 1)\n",
            "\n",
            "            if len(self.weight_support) > 5:\n",
            "                sampled_w = th.stack([weight] + random.sample(self.weight_support, k=4))\n",
            "            else:\n",
            "                sampled_w = th.stack(self.weight_support)\n",
            "\n",
            "            with th.no_grad():\n",
            "                # Compute min_i Q_i(s', a, w) . w\n",
            "                next_q_values = th.stack([target_psi_net(s_next_obs, w) for target_psi_net in self.target_q_nets])\n",
            "                scalarized_next_q_values = th.einsum(\"nbar,br->nba\", next_q_values, w)  # q_i(s', a, w)\n",
            "                min_inds = th.argmin(scalarized_next_q_values, dim=0)\n",
            "                min_inds = min_inds.reshape(1, next_q_values.size(1), next_q_values.size(2), 1).expand(\n",
            "                    1, next_q_values.size(1), next_q_values.size(2), next_q_values.size(3)\n",
            "                )\n",
            "                next_q_values = next_q_values.gather(0, min_inds).squeeze(0)\n",
            "\n",
            "                # Compute max_a Q(s', a, w) . w\n",
            "                max_q = th.einsum(\"br,bar->ba\", w, next_q_values)\n",
            "                max_acts = th.argmax(max_q, dim=1)\n",
            "\n",
            "                q_targets = next_q_values.gather(\n",
            "                    1, max_acts.long().reshape(-1, 1, 1).expand(next_q_values.size(0), 1, next_q_values.size(2))\n",
            "                )\n",
            "                target_q = q_targets.reshape(-1, self.reward_dim)\n",
            "                target_q = s_rewards + (1 - s_dones) * self.gamma * target_q\n",
            "\n",
            "                if self.gpi_pd:\n",
            "                    target_q_envelope, _ = self._envelope_target(s_next_obs, w, sampled_w)\n",
            "                    target_q_envelope = s_rewards + (1 - s_dones) * self.gamma * target_q_envelope\n",
            "\n",
            "            losses = []\n",
            "            td_errors = []\n",
            "            gtd_errors = []\n",
            "            for psi_net in self.q_nets:\n",
            "                psi_value = psi_net(s_obs, w)\n",
            "                psi_value = psi_value.gather(\n",
            "                    1, s_actions.long().reshape(-1, 1, 1).expand(psi_value.size(0), 1, psi_value.size(2))\n",
            "                )\n",
            "                psi_value = psi_value.reshape(-1, self.reward_dim)\n",
            "\n",
            "                if self.gpi_pd:\n",
            "                    gtd_error = psi_value - target_q_envelope\n",
            "\n",
            "                td_error = psi_value - target_q\n",
            "                loss = huber(td_error.abs(), min_priority=self.min_priority)\n",
            "                losses.append(loss)\n",
            "                if self.gpi_pd:\n",
            "                    gtd_errors.append(gtd_error.abs())\n",
            "                if self.per:\n",
            "                    td_errors.append(td_error.abs())\n",
            "            critic_loss = (1 / self.num_nets) * sum(losses)\n",
            "\n",
            "            self.q_optim.zero_grad()\n",
            "            critic_loss.backward()\n",
            "\n",
            "            if self.max_grad_norm is not None:\n",
            "                if self.log and self.global_step % 100 == 0:\n",
            "                    wandb.log(\n",
            "                        {\n",
            "                            \"losses/grad_norm\": get_grad_norm(self.q_nets[0].parameters()).item(),\n",
            "                            \"global_step\": self.global_step,\n",
            "                        },\n",
            "                    )\n",
            "                for psi_net in self.q_nets:\n",
            "                    th.nn.utils.clip_grad_norm_(psi_net.parameters(), self.max_grad_norm)\n",
            "            self.q_optim.step()\n",
            "            critic_losses.append(critic_loss.item())\n",
            "\n",
            "            if self.per or self.gpi_pd:\n",
            "                if self.gpi_pd:\n",
            "                    gtd_error = th.max(th.stack(gtd_errors), dim=0)[0]\n",
            "                    gtd_error = gtd_error[: len(idxes)].detach()\n",
            "                    gper = th.einsum(\"br,br->b\", w[: len(idxes)], gtd_error).abs()\n",
            "                    gpriority = gper.cpu().numpy().flatten()\n",
            "                    gpriority = gpriority.clip(min=self.min_priority) ** self.alpha\n",
            "\n",
            "                if self.per:\n",
            "                    td_error = th.max(th.stack(td_errors), dim=0)[0]\n",
            "                    td_error = td_error[: len(idxes)].detach()\n",
            "                    per = th.einsum(\"br,br->b\", w[: len(idxes)], td_error).abs()\n",
            "                    priority = per.cpu().numpy().flatten()\n",
            "                    priority = priority.clip(min=self.min_priority) ** self.alpha\n",
            "\n",
            "                if self.gpi_pd:\n",
            "                    self.replay_buffer.update_priorities(idxes, gpriority)\n",
            "                else:\n",
            "                    self.replay_buffer.update_priorities(idxes, priority)\n",
            "\n",
            "        if self.tau != 1 or self.global_step % self.target_net_update_freq == 0:\n",
            "            for psi_net, target_psi_net in zip(self.q_nets, self.target_q_nets):\n",
            "                polyak_update(psi_net.parameters(), target_psi_net.parameters(), self.tau)\n",
            "\n",
            "        if self.epsilon_decay_steps is not None:\n",
            "            self.epsilon = linearly_decaying_value(\n",
            "                self.initial_epsilon, self.epsilon_decay_steps, self.global_step, self.learning_starts, self.final_epsilon\n",
            "            )\n",
            "\n",
            "        if self.log and self.global_step % 100 == 0:\n",
            "            if self.per:\n",
            "                wandb.log(\n",
            "                    {\n",
            "                        \"metrics/mean_priority\": np.mean(priority),\n",
            "                        \"metrics/max_priority\": np.max(priority),\n",
            "                        \"metrics/mean_td_error_w\": per.abs().mean().item(),\n",
            "                    },\n",
            "                    commit=False,\n",
            "                )\n",
            "            if self.gpi_pd:\n",
            "                wandb.log(\n",
            "                    {\n",
            "                        \"metrics/mean_gpriority\": np.mean(gpriority),\n",
            "                        \"metrics/max_gpriority\": np.max(gpriority),\n",
            "                        \"metrics/mean_gtd_error_w\": gper.abs().mean().item(),\n",
            "                        \"metrics/mean_absolute_diff_gtd_td\": (gper - per).abs().mean().item(),\n",
            "                    },\n",
            "                    commit=False,\n",
            "                )\n",
            "            wandb.log(\n",
            "                {\n",
            "                    \"losses/critic_loss\": np.mean(critic_losses),\n",
            "                    \"metrics/epsilon\": self.epsilon,\n",
            "                    \"global_step\": self.global_step,\n",
            "                },\n",
            "            )\n",
            "\n",
            "    @th.no_grad()\n",
            "    def gpi_action(self, obs: th.Tensor, w: th.Tensor, return_policy_index=False, include_w=False):\n",
            "        \"\"\"Select an action using GPI.\"\"\"\n",
            "        if include_w:\n",
            "            M = th.stack(self.weight_support + [w])\n",
            "        else:\n",
            "            M = th.stack(self.weight_support)\n",
            "\n",
            "        obs_m = obs.repeat(M.size(0), *(1 for _ in range(obs.dim())))\n",
            "        q_values = self.q_nets[0](obs_m, M)\n",
            "\n",
            "        scalar_q_values = th.einsum(\"r,bar->ba\", w, q_values)  # q(s,a,w_i) = q(s,a,w_i) . w\n",
            "        max_q, a = th.max(scalar_q_values, dim=1)\n",
            "        policy_index = th.argmax(max_q)  # max_i max_a q(s,a,w_i)\n",
            "        action = a[policy_index].detach().item()\n",
            "\n",
            "        if return_policy_index:\n",
            "            return action, policy_index.item()\n",
            "        return action\n",
            "\n",
            "    @th.no_grad()\n",
            "    def eval(self, obs: np.ndarray, w: np.ndarray) -> int:\n",
            "        \"\"\"Select an action for the given obs and weight vector.\"\"\"\n",
            "        obs = th.as_tensor(obs).float().to(self.device)\n",
            "        w = th.as_tensor(w).float().to(self.device)\n",
            "        for q_net in self.q_nets:\n",
            "            q_net.eval()\n",
            "        if self.use_gpi:\n",
            "            action = self.gpi_action(obs, w, include_w=False)\n",
            "        else:\n",
            "            action = self.max_action(obs, w)\n",
            "        for q_net in self.q_nets:\n",
            "            q_net.train()\n",
            "        return action\n",
            "\n",
            "    def _act(self, obs: th.Tensor, w: th.Tensor) -> int:\n",
            "        if self.np_random.random() < self.epsilon:\n",
            "            return self.env.action_space.sample()\n",
            "        else:\n",
            "            if self.use_gpi:\n",
            "                action, policy_index = self.gpi_action(obs, w, return_policy_index=True)\n",
            "                self.police_indices.append(policy_index)\n",
            "                return action\n",
            "            else:\n",
            "                return self.max_action(obs, w)\n",
            "\n",
            "    @th.no_grad()\n",
            "    def max_action(self, obs: th.Tensor, w: th.Tensor) -> int:\n",
            "        \"\"\"Select the greedy action.\"\"\"\n",
            "        psi = th.min(th.stack([psi_net(obs, w) for psi_net in self.q_nets]), dim=0)[0]\n",
            "        # psi = self.psi_nets[0](obs, w)\n",
            "        q = th.einsum(\"r,bar->ba\", w, psi)\n",
            "        max_act = th.argmax(q, dim=1)\n",
            "        return max_act.detach().item()\n",
            "\n",
            "    @th.no_grad()\n",
            "    def _reset_priorities(self, w: th.Tensor):\n",
            "        inds = np.arange(self.replay_buffer.size)\n",
            "        priorities = np.repeat(0.1, self.replay_buffer.size)\n",
            "        (\n",
            "            obs_s,\n",
            "            actions_s,\n",
            "            rewards_s,\n",
            "            next_obs_s,\n",
            "            dones_s,\n",
            "        ) = self.replay_buffer.get_all_data(to_tensor=False)\n",
            "        num_batches = int(np.ceil(obs_s.shape[0] / 1000))\n",
            "        for i in range(num_batches):\n",
            "            b = i * 1000\n",
            "            e = min((i + 1) * 1000, obs_s.shape[0])\n",
            "            obs, actions, rewards, next_obs, dones = obs_s[b:e], actions_s[b:e], rewards_s[b:e], next_obs_s[b:e], dones_s[b:e]\n",
            "            obs, actions, rewards, next_obs, dones = (\n",
            "                th.tensor(obs).to(self.device),\n",
            "                th.tensor(actions).to(self.device),\n",
            "                th.tensor(rewards).to(self.device),\n",
            "                th.tensor(next_obs).to(self.device),\n",
            "                th.tensor(dones).to(self.device),\n",
            "            )\n",
            "            q_values = self.q_nets[0](obs, w.repeat(obs.size(0), 1))\n",
            "            q_a = q_values.gather(1, actions.long().reshape(-1, 1, 1).expand(q_values.size(0), 1, q_values.size(2))).squeeze(1)\n",
            "\n",
            "            if self.gpi_pd:\n",
            "                max_next_q, _ = self._envelope_target(next_obs, w.repeat(next_obs.size(0), 1), th.stack(self.weight_support))\n",
            "            else:\n",
            "                next_q_values = self.q_nets[0](next_obs, w.repeat(next_obs.size(0), 1))\n",
            "                max_q = th.einsum(\"r,bar->ba\", w, next_q_values)\n",
            "                max_acts = th.argmax(max_q, dim=1)\n",
            "                q_targets = self.target_q_nets[0](next_obs, w.repeat(next_obs.size(0), 1))\n",
            "                q_targets = q_targets.gather(\n",
            "                    1, max_acts.long().reshape(-1, 1, 1).expand(q_targets.size(0), 1, q_targets.size(2))\n",
            "                )\n",
            "                max_next_q = q_targets.reshape(-1, self.reward_dim)\n",
            "\n",
            "            gtderror = th.einsum(\"r,br->b\", w, (rewards + (1 - dones) * self.gamma * max_next_q - q_a)).abs()\n",
            "            priorities[b:e] = gtderror.clamp(min=self.min_priority).pow(self.alpha).cpu().detach().numpy().flatten()\n",
            "\n",
            "        self.replay_buffer.update_priorities(inds, priorities)\n",
            "\n",
            "    @th.no_grad()\n",
            "    def _envelope_target(self, obs: th.Tensor, w: th.Tensor, sampled_w: th.Tensor):\n",
            "        W = sampled_w.unsqueeze(0).repeat(obs.size(0), 1, 1)\n",
            "        next_obs = obs.unsqueeze(1).repeat(1, sampled_w.size(0), 1)\n",
            "\n",
            "        next_q_target = th.stack(\n",
            "            [\n",
            "                target_net(next_obs, W).view(obs.size(0), sampled_w.size(0), self.action_dim, self.reward_dim)\n",
            "                for target_net in self.target_q_nets\n",
            "            ]\n",
            "        )\n",
            "\n",
            "        q_values = th.einsum(\"br,nbpar->nbpa\", w, next_q_target)\n",
            "        min_inds = th.argmin(q_values, dim=0)\n",
            "        min_inds = min_inds.reshape(1, next_q_target.size(1), next_q_target.size(2), next_q_target.size(3), 1).expand(\n",
            "            1, next_q_target.size(1), next_q_target.size(2), next_q_target.size(3), next_q_target.size(4)\n",
            "        )\n",
            "        next_q_target = next_q_target.gather(0, min_inds).squeeze(0)\n",
            "\n",
            "        q_values = th.einsum(\"br,bpar->bpa\", w, next_q_target)\n",
            "        max_q, ac = th.max(q_values, dim=2)\n",
            "        pi = th.argmax(max_q, dim=1)\n",
            "\n",
            "        max_next_q = next_q_target.gather(\n",
            "            2,\n",
            "            ac.unsqueeze(2).unsqueeze(3).expand(next_q_target.size(0), next_q_target.size(1), 1, next_q_target.size(3)),\n",
            "        ).squeeze(2)\n",
            "        max_next_q = max_next_q.gather(1, pi.reshape(-1, 1, 1).expand(max_next_q.size(0), 1, max_next_q.size(2))).squeeze(1)\n",
            "        return max_next_q, next_q_target\n",
            "\n",
            "    def set_weight_support(self, weight_list: List[np.ndarray]):\n",
            "        \"\"\"Set the weight support set.\"\"\"\n",
            "        weights_no_repeats = unique_tol(weight_list)\n",
            "        self.weight_support = [th.tensor(w).float().to(self.device) for w in weights_no_repeats]\n",
            "\n",
            "    def train_iteration(\n",
            "        self,\n",
            "        total_timesteps: int,\n",
            "        weight: np.ndarray,\n",
            "        weight_support: List[np.ndarray],\n",
            "        change_w_every_episode: bool = True,\n",
            "        reset_num_timesteps: bool = True,\n",
            "        eval_env: Optional[gym.Env] = None,\n",
            "        eval_freq: int = 1000,\n",
            "        reset_learning_starts: bool = False,\n",
            "    ):\n",
            "        \"\"\"Train the agent for one iteration.\n",
            "\n",
            "        Args:\n",
            "            total_timesteps (int): Number of timesteps to train for\n",
            "            weight (np.ndarray): Weight vector\n",
            "            weight_support (List[np.ndarray]): Weight support set\n",
            "            change_w_every_episode (bool): Whether to change the weight vector at the end of each episode\n",
            "            reset_num_timesteps (bool): Whether to reset the number of timesteps\n",
            "            eval_env (Optional[gym.Env]): Environment to evaluate on\n",
            "            eval_freq (int): Number of timesteps between evaluations\n",
            "            reset_learning_starts (bool): Whether to reset the learning starts\n",
            "        \"\"\"\n",
            "        weight_support = unique_tol(weight_support)  # remove duplicates\n",
            "        self.set_weight_support(weight_support)\n",
            "        tensor_w = th.tensor(weight).float().to(self.device)\n",
            "\n",
            "        self.police_indices = []\n",
            "        self.global_step = 0 if reset_num_timesteps else self.global_step\n",
            "        self.num_episodes = 0 if reset_num_timesteps else self.num_episodes\n",
            "        if reset_learning_starts:  # Resets epsilon-greedy exploration\n",
            "            self.learning_starts = self.global_step\n",
            "\n",
            "        if self.per and len(self.replay_buffer) > 0:\n",
            "            self._reset_priorities(tensor_w)\n",
            "\n",
            "        obs, info = self.env.reset()\n",
            "        for _ in range(1, total_timesteps + 1):\n",
            "            self.global_step += 1\n",
            "\n",
            "            if self.global_step < self.learning_starts:\n",
            "                action = self.env.action_space.sample()\n",
            "            else:\n",
            "                action = self._act(th.as_tensor(obs).float().to(self.device), tensor_w)\n",
            "\n",
            "            next_obs, vec_reward, terminated, truncated, info = self.env.step(action)\n",
            "\n",
            "            self.replay_buffer.add(obs, action, vec_reward, next_obs, terminated)\n",
            "\n",
            "            if self.global_step >= self.learning_starts:\n",
            "                if self.dyna:\n",
            "                    if self.global_step % self.dynamics_train_freq(self.global_step) == 0:\n",
            "                        m_obs, m_actions, m_rewards, m_next_obs, m_dones = self.replay_buffer.get_all_data()\n",
            "                        one_hot = np.zeros((len(m_obs), self.action_dim))\n",
            "                        one_hot[np.arange(len(m_obs)), m_actions.astype(int).reshape(len(m_obs))] = 1\n",
            "                        X = np.hstack((m_obs, one_hot))\n",
            "                        Y = np.hstack((m_rewards, m_next_obs - m_obs))\n",
            "                        mean_holdout_loss = self.dynamics.fit(X, Y)\n",
            "                        if self.log:\n",
            "                            wandb.log(\n",
            "                                {\"dynamics/mean_holdout_loss\": mean_holdout_loss, \"global_step\": self.global_step},\n",
            "                            )\n",
            "\n",
            "                    if self.global_step >= self.dynamics_rollout_starts and self.global_step % self.dynamics_rollout_freq == 0:\n",
            "                        self._rollout_dynamics(tensor_w)\n",
            "\n",
            "                self.update(tensor_w)\n",
            "\n",
            "            if eval_env is not None and self.log and self.global_step % eval_freq == 0:\n",
            "                self.policy_eval(eval_env, weights=weight, log=self.log)\n",
            "\n",
            "                if self.dyna and self.global_step >= self.dynamics_rollout_starts:\n",
            "                    plot = visualize_eval(self, eval_env, self.dynamics, weight, compound=False, horizon=1000)\n",
            "                    wandb.log({\"dynamics/predictions\": wandb.Image(plot), \"global_step\": self.global_step})\n",
            "                    plot.close()\n",
            "\n",
            "            if terminated or truncated:\n",
            "                obs, _ = self.env.reset()\n",
            "                self.num_episodes += 1\n",
            "\n",
            "                if self.log and \"episode\" in info.keys():\n",
            "                    log_episode_info(info[\"episode\"], np.dot, weight, self.global_step)\n",
            "                    wandb.log(\n",
            "                        {\"metrics/policy_index\": np.array(self.police_indices), \"global_step\": self.global_step},\n",
            "                    )\n",
            "                    self.police_indices = []\n",
            "\n",
            "                if change_w_every_episode:\n",
            "                    weight = random.choice(weight_support)\n",
            "                    tensor_w = th.tensor(weight).float().to(self.device)\n",
            "            else:\n",
            "                obs = next_obs\n",
            "\n",
            "    def train(\n",
            "        self,\n",
            "        total_timesteps: int,\n",
            "        eval_env,\n",
            "        ref_point: np.ndarray,\n",
            "        known_pareto_front: Optional[List[np.ndarray]] = None,\n",
            "        num_eval_weights_for_front: int = 100,\n",
            "        num_eval_episodes_for_front: int = 5,\n",
            "        num_eval_weights_for_eval: int = 50,\n",
            "        timesteps_per_iter: int = 10000,\n",
            "        weight_selection_algo: str = \"gpi-ls\",\n",
            "        eval_freq: int = 1000,\n",
            "        eval_mo_freq: int = 10000,\n",
            "        checkpoints: bool = True,\n",
            "    ):\n",
            "        \"\"\"Train agent.\n",
            "\n",
            "        Args:\n",
            "            total_timesteps (int): Number of timesteps to train for.\n",
            "            eval_env (gym.Env): Environment to evaluate on.\n",
            "            ref_point (np.ndarray): Reference point for hypervolume calculation.\n",
            "            known_pareto_front (Optional[List[np.ndarray]]): Optimal Pareto front if known.\n",
            "            num_eval_weights_for_front: Number of weights to evaluate for the Pareto front.\n",
            "            num_eval_episodes_for_front: number of episodes to run when evaluating the policy.\n",
            "            num_eval_weights_for_eval (int): Number of weights use when evaluating the Pareto front, e.g., for computing expected utility.\n",
            "            timesteps_per_iter (int): Number of timesteps to train for per iteration.\n",
            "            weight_selection_algo (str): Weight selection algorithm to use.\n",
            "            eval_freq (int): Number of timesteps between evaluations.\n",
            "            eval_mo_freq (int): Number of timesteps between multi-objective evaluations.\n",
            "            checkpoints (bool): Whether to save checkpoints.\n",
            "        \"\"\"\n",
            "        if self.log:\n",
            "            self.register_additional_config(\n",
            "                {\n",
            "                    \"total_timesteps\": total_timesteps,\n",
            "                    \"ref_point\": ref_point.tolist(),\n",
            "                    \"known_front\": known_pareto_front,\n",
            "                    \"num_eval_weights_for_front\": num_eval_weights_for_front,\n",
            "                    \"num_eval_episodes_for_front\": num_eval_episodes_for_front,\n",
            "                    \"num_eval_weights_for_eval\": num_eval_weights_for_eval,\n",
            "                    \"timesteps_per_iter\": timesteps_per_iter,\n",
            "                    \"weight_selection_algo\": weight_selection_algo,\n",
            "                    \"eval_freq\": eval_freq,\n",
            "                    \"eval_mo_freq\": eval_mo_freq,\n",
            "                }\n",
            "            )\n",
            "        max_iter = total_timesteps // timesteps_per_iter\n",
            "        linear_support = LinearSupport(num_objectives=self.reward_dim, epsilon=0.0 if weight_selection_algo == \"ols\" else None)\n",
            "\n",
            "        weight_history = []\n",
            "\n",
            "        eval_weights = equally_spaced_weights(self.reward_dim, n=num_eval_weights_for_front)\n",
            "\n",
            "        for iter in range(1, max_iter + 1):\n",
            "            if weight_selection_algo == \"ols\" or weight_selection_algo == \"gpi-ls\":\n",
            "                if weight_selection_algo == \"gpi-ls\":\n",
            "                    self.set_weight_support(linear_support.get_weight_support())\n",
            "                    use_gpi = self.use_gpi\n",
            "                    self.use_gpi = True\n",
            "                    w = linear_support.next_weight(\n",
            "                        algo=\"gpi-ls\", gpi_agent=self, env=eval_env, rep_eval=num_eval_episodes_for_front\n",
            "                    )\n",
            "                    self.use_gpi = use_gpi\n",
            "                else:\n",
            "                    w = linear_support.next_weight(algo=\"ols\")\n",
            "\n",
            "                if w is None:\n",
            "                    break\n",
            "            else:\n",
            "                raise ValueError(f\"Unknown algorithm {weight_selection_algo}.\")\n",
            "\n",
            "            print(\"Next weight vector:\", w)\n",
            "            weight_history.append(w)\n",
            "            if weight_selection_algo == \"gpi-ls\":\n",
            "                M = linear_support.get_weight_support() + linear_support.get_corner_weights(top_k=4) + [w]\n",
            "            elif weight_selection_algo == \"ols\":\n",
            "                M = linear_support.get_weight_support() + [w]\n",
            "            else:\n",
            "                M = None\n",
            "\n",
            "            self.train_iteration(\n",
            "                total_timesteps=timesteps_per_iter,\n",
            "                weight=w,\n",
            "                weight_support=M,\n",
            "                change_w_every_episode=weight_selection_algo == \"gpi-ls\",\n",
            "                eval_env=eval_env,\n",
            "                eval_freq=eval_freq,\n",
            "                reset_num_timesteps=False,\n",
            "                reset_learning_starts=False,\n",
            "            )\n",
            "\n",
            "            if weight_selection_algo == \"ols\":\n",
            "                value = policy_evaluation_mo(self, eval_env, w, rep=num_eval_episodes_for_front)[3]\n",
            "                linear_support.add_solution(value, w)\n",
            "            elif weight_selection_algo == \"gpi-ls\":\n",
            "                for wcw in M:\n",
            "                    n_value = policy_evaluation_mo(self, eval_env, wcw, rep=num_eval_episodes_for_front)[3]\n",
            "                    linear_support.add_solution(n_value, wcw)\n",
            "\n",
            "            if self.log and self.global_step % eval_mo_freq == 0:\n",
            "                # Evaluation\n",
            "                gpi_returns_test_tasks = [\n",
            "                    policy_evaluation_mo(self, eval_env, ew, rep=num_eval_episodes_for_front)[3] for ew in eval_weights\n",
            "                ]\n",
            "                log_all_multi_policy_metrics(\n",
            "                    current_front=gpi_returns_test_tasks,\n",
            "                    hv_ref_point=ref_point,\n",
            "                    reward_dim=self.reward_dim,\n",
            "                    global_step=self.global_step,\n",
            "                    n_sample_weights=num_eval_weights_for_eval,\n",
            "                    ref_front=known_pareto_front,\n",
            "                )\n",
            "                # This is the EU computed in the paper\n",
            "                mean_gpi_returns_test_tasks = np.mean(\n",
            "                    [np.dot(ew, q) for ew, q in zip(eval_weights, gpi_returns_test_tasks)], axis=0\n",
            "                )\n",
            "                wandb.log({\"eval/Mean Utility - GPI\": mean_gpi_returns_test_tasks, \"iteration\": iter})\n",
            "\n",
            "            if checkpoints:\n",
            "                self.save(filename=f\"GPI-PD {weight_selection_algo} iter={iter}\", save_replay_buffer=False)\n",
            "\n",
            "        self.close_wandb()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "agent.experiment_name = f\"gpipd_model_{timestamp}\"\n",
        "agent.save_dir = save_dir\n",
        "\n",
        "# Save the model\n",
        "agent.save(save_dir=save_dir, save_replay_buffer=True)\n",
        "\n",
        "# Final absolute path\n",
        "model_file = f\"{save_dir}/{agent.experiment_name}.tar\"\n",
        "print(\"ğŸ”¥ Saved model to:\", model_file)\n",
        "\n",
        "# Verify\n",
        "!ls -lha \"$save_dir\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZsZb2dW95Fw",
        "outputId": "217e3351-5174-4bdc-cdc2-2464f025064c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¥ Saved model to: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/gpipd_model_20251117_175812.tar\n",
            "total 4.0M\n",
            "-rw------- 1 root root 4.0M Nov 17 17:58 gpipd_model_20251117_175812.tar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Phase 7b: Evaluation (Loads Model If Not Already Loaded) ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "# Load the latest model\n",
        "latest_model = sorted(existing_models)[-1]\n",
        "model_path = os.path.join(save_dir, latest_model)   # FIXED\n",
        "\n",
        "timestamp = latest_model.replace('gpipd_model_', '').replace('.tar', '')\n",
        "\n",
        "print(f\"Loading model: {latest_model}...\")\n",
        "\n",
        "agent = GPIPD(\n",
        "    env=train_env,\n",
        "    learning_rate=3e-4,\n",
        "    gamma=0.99,\n",
        "    batch_size=256,\n",
        "    net_arch=[128, 128],\n",
        "    buffer_size=int(5e4),\n",
        "    initial_epsilon=0.05,\n",
        "    final_epsilon=0.05,\n",
        "    log=False,\n",
        "    dyna=False\n",
        ")\n",
        "\n",
        "# Load trained weights\n",
        "agent.load(path=model_path)\n",
        "print(f\"âœ… Model loaded from: {model_path}\")\n",
        "\n",
        "# === FIX: Proper GPIPD predict method (no policy_net required) ===\n",
        "\n",
        "w_eval = np.ones(agent.reward_dim, dtype=np.float32) / agent.reward_dim\n",
        "\n",
        "def patched_predict(agent, obs, deterministic=True):\n",
        "    return agent.eval(obs, w_eval), {}\n",
        "\n",
        "agent.predict = lambda obs, deterministic=True: patched_predict(agent, obs, deterministic)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(agent, env, n_episodes=200):\n",
        "    out = []\n",
        "    episode_info = []\n",
        "    for i in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        total = np.zeros(3, dtype=np.float32)\n",
        "        while not done:\n",
        "            action, _ = agent.predict(obs, deterministic=True)\n",
        "            obs, r, done, trunc, info = env.step(action)\n",
        "            total += r\n",
        "        out.append(total)\n",
        "        episode_info.append(info)\n",
        "    return np.vstack(out), episode_info\n",
        "\n",
        "# Run evaluation\n",
        "print(\"Evaluating agent...\")\n",
        "results, episode_infos = evaluate(agent, test_env)\n",
        "\n",
        "# Convert to DataFrame for analysis\n",
        "res_df = pd.DataFrame(results, columns=['efficacy','neg_ddi','neg_tol'])\n",
        "res_df['hadm_id'] = [info.get('hadm_id') for info in episode_infos]\n",
        "\n",
        "# Save evaluation results\n",
        "results_path = os.path.join(save_dir, f'evaluation_results_{timestamp}.csv')\n",
        "res_df.to_csv(results_path, index=False)\n",
        "print(f\"ğŸ’¾ Saved evaluation results: {results_path}\")\n",
        "\n",
        "# Save hyperparameters\n",
        "hyperparams = {\n",
        "    'learning_rate': 3e-4,\n",
        "    'gamma': 0.99,\n",
        "    'batch_size': 256,\n",
        "    'net_arch': [128, 128],\n",
        "    'buffer_size': int(5e4),\n",
        "    'epsilon_decay_steps': 20000,\n",
        "    'total_timesteps': 30000,\n",
        "    'n_eval_episodes': 200,\n",
        "    'ref_point': train_env.reward_space.low.tolist(),\n",
        "    'dyna': False,\n",
        "    'pycddlib_patch': 'scipy_based'\n",
        "}\n",
        "hyperparams_path = os.path.join(save_dir, f'hyperparameters_{timestamp}.json')\n",
        "with open(hyperparams_path, 'w') as f:\n",
        "    json.dump(hyperparams, f, indent=2)\n",
        "print(f\"ğŸ’¾ Saved hyperparameters: {hyperparams_path}\")\n",
        "\n",
        "# Generate and save Pareto front plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(res_df['efficacy'], res_df['neg_ddi'],\n",
        "                     c=res_df['neg_tol'], cmap='viridis', alpha=0.6, s=50)\n",
        "plt.xlabel('Efficacy Proxy (Higher Better)', fontsize=12)\n",
        "plt.ylabel('DDI Risk (Less Negative Better)', fontsize=12)\n",
        "plt.colorbar(scatter, label='Tolerability (Less Negative Better)')\n",
        "plt.title('Pareto-optimal Prescription Policies\\nMulti-Objective RL on MIMIC-III Polypharmacy',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add statistics text\n",
        "mean_vals = res_df.mean()\n",
        "std_vals = res_df.std()\n",
        "stats_text = f\"Mean Efficacy: {mean_vals['efficacy']:.3f} Â± {std_vals['efficacy']:.3f}\\nMean DDI: {mean_vals['neg_ddi']:.3f} Â± {std_vals['neg_ddi']:.3f}\\nMean Tolerability: {mean_vals['neg_tol']:.3f} Â± {std_vals['neg_tol']:.3f}\"\n",
        "plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,\n",
        "         fontsize=10, verticalalignment='top',\n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save plot\n",
        "plot_path = os.path.join(save_dir, f'pareto_front_{timestamp}.png')\n",
        "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"ğŸ’¾ Saved Pareto plot: {plot_path}\")\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n=== Final Performance Summary ===\")\n",
        "print(f\"Mean Â± Std (Efficacy): {mean_vals['efficacy']:.4f} Â± {std_vals['efficacy']:.4f}\")\n",
        "print(f\"Mean Â± Std (DDI Risk): {mean_vals['neg_ddi']:.4f} Â± {std_vals['neg_ddi']:.4f}\")\n",
        "print(f\"Mean Â± Std (Tolerability): {mean_vals['neg_tol']:.4f} Â± {std_vals['neg_tol']:.4f}\")\n",
        "\n",
        "# Save summary report\n",
        "summary = {\n",
        "    'timestamp': timestamp,\n",
        "    'n_eval_episodes': len(res_df),\n",
        "    'mean_efficacy': float(mean_vals['efficacy']),\n",
        "    'std_efficacy': float(std_vals['efficacy']),\n",
        "    'mean_ddi': float(mean_vals['neg_ddi']),\n",
        "    'std_ddi': float(std_vals['neg_ddi']),\n",
        "    'mean_tolerability': float(mean_vals['neg_tol']),\n",
        "    'std_tolerability': float(std_vals['neg_tol']),\n",
        "    'model_path': model_path,\n",
        "    'results_path': results_path,\n",
        "    'plot_path': plot_path,\n",
        "    'hyperparams_path': hyperparams_path,\n",
        "    'model_based_learning': False,\n",
        "    'pycddlib_patch': 'scipy_based'\n",
        "}\n",
        "summary_path = os.path.join(save_dir, f'summary_{timestamp}.json')\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(f\"ğŸ’¾ Saved summary report: {summary_path}\")\n",
        "\n",
        "print(f\"\\nğŸ‰ Phase 7b Evaluation Complete! All artifacts saved to: {save_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "JFiul7Q15lRv",
        "outputId": "9e84d599-868f-4421-934f-553b2de12df7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'existing_models' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2059094199.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the latest model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlatest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexisting_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest_model\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# FIXED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'existing_models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze Results"
      ],
      "metadata": {
        "id": "vAO3g0PBSM6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 8 â€” Post-training analysis & publication-grade figures\n",
        "# Save as phase8_analysis.py or paste in notebook cell and run.\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from math import ceil\n",
        "import torch as th\n",
        "\n",
        "# ---------- USER CONFIG ----------\n",
        "SAVE_DIR = \"/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results\"\n",
        "RESULTS_PREFIX = \"phase8_results\"\n",
        "EPISODES_PER_WEIGHT = 50          # episodes per weight\n",
        "N_WEIGHTS = 40                    # number of weights to sample on simplex\n",
        "TOP_K = 5                         # top/bottom episodes to save\n",
        "RANDOM_SEED = 42\n",
        "# ---------------------------------\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "th.manual_seed(RANDOM_SEED)\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_dir = os.path.join(SAVE_DIR, f\"{RESULTS_PREFIX}_{timestamp}\")\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "print(\"Phase 8 results folder:\", results_dir)\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def find_latest_model_tar(save_dir):\n",
        "    files = sorted(glob.glob(os.path.join(save_dir, \"gpipd_model_*.tar\")), key=os.path.getmtime)\n",
        "    return files[-1] if files else None\n",
        "\n",
        "def sample_weights_dirichlet(d, n, seed=None):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    a = rng.gamma(1.0, 1.0, size=(n, d))\n",
        "    a = a / a.sum(axis=1, keepdims=True)\n",
        "    return a\n",
        "\n",
        "def nondominated(points):\n",
        "    \"\"\"Return mask of nondominated points (higher is better).\"\"\"\n",
        "    N = points.shape[0]\n",
        "    mask = np.ones(N, dtype=bool)\n",
        "    for i in range(N):\n",
        "        if not mask[i]:\n",
        "            continue\n",
        "        # j dominates i if j>=i in all dims and > in at least one\n",
        "        ge = np.all(points >= points[i], axis=1)\n",
        "        gt = np.any(points > points[i], axis=1)\n",
        "        dominated = np.any(ge & gt)\n",
        "        if dominated:\n",
        "            mask[i] = False\n",
        "    return mask\n",
        "\n",
        "# ---------- load / validate agent ----------\n",
        "try:\n",
        "    agent  # if already loaded in notebook\n",
        "    print(\"Using existing 'agent' in workspace.\")\n",
        "except NameError:\n",
        "    latest = find_latest_model_tar(SAVE_DIR)\n",
        "    if latest is None:\n",
        "        raise FileNotFoundError(f\"No gpipd_model_*.tar found in {SAVE_DIR}. Run Phase 7 first.\")\n",
        "    print(\"Loading model:\", latest)\n",
        "    from morl_baselines.multi_policy.gpi_pd.gpi_pd import GPIPD\n",
        "    # Build shell agent (must match architecture used in training)\n",
        "    agent = GPIPD(env=train_env, learning_rate=3e-4, gamma=0.99, batch_size=256,\n",
        "                  net_arch=[128,128], buffer_size=int(5e4), initial_epsilon=0.05, final_epsilon=0.05,\n",
        "                  log=False, dyna=False)\n",
        "    agent.load(path=latest)\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "# quick checks\n",
        "reward_dim = agent.reward_dim\n",
        "action_space = agent.action_space\n",
        "print(f\"Agent reward_dim={reward_dim}, action_space={action_space}\")\n",
        "\n",
        "# ---------- sample weights ----------\n",
        "W = sample_weights_dirichlet(reward_dim, N_WEIGHTS, seed=RANDOM_SEED)\n",
        "print(f\"Sampled {len(W)} weights on simplex.\")\n",
        "\n",
        "# ---------- evaluation loop ----------\n",
        "all_entries = []  # list of dicts for JSON/CSV\n",
        "for wi, w in enumerate(W):\n",
        "    w_np = w.astype(np.float32)\n",
        "    print(f\"Weight {wi+1}/{len(W)}: {np.round(w_np,3)}\")\n",
        "    for ep in range(EPISODES_PER_WEIGHT):\n",
        "        obs, _ = test_env.reset()\n",
        "        done = False\n",
        "        total = np.zeros(reward_dim, dtype=np.float32)\n",
        "        actions = []\n",
        "        info = {}\n",
        "        while not done:\n",
        "            # Use agent.eval(obs, w) for action selection\n",
        "            a = agent.eval(obs, w_np)\n",
        "            obs, r, done, trunc, info = test_env.step(int(a))\n",
        "            total += r\n",
        "            actions.append(int(a))\n",
        "        entry = {\n",
        "            \"weight_index\": int(wi),\n",
        "            \"weight\": w_np.tolist(),\n",
        "            \"episode_index\": int(ep),\n",
        "            \"total_rewards\": total.tolist(),\n",
        "            \"hadm_id\": info.get(\"hadm_id\", None),\n",
        "            \"actions\": actions\n",
        "        }\n",
        "        all_entries.append(entry)\n",
        "\n",
        "# ---------- save raw JSON + flattened CSV ----------\n",
        "raw_json = os.path.join(results_dir, \"phase8_raw_results.json\")\n",
        "with open(raw_json, \"w\") as f:\n",
        "    json.dump(all_entries, f)\n",
        "print(\"Saved raw JSON:\", raw_json)\n",
        "\n",
        "# flatten to dataframe\n",
        "rows = []\n",
        "for e in all_entries:\n",
        "    row = {\n",
        "        \"weight_index\": e[\"weight_index\"],\n",
        "        \"weight\": e[\"weight\"],\n",
        "        \"episode_index\": e[\"episode_index\"],\n",
        "        \"hadm_id\": e[\"hadm_id\"],\n",
        "    }\n",
        "    for i, val in enumerate(e[\"total_rewards\"]):\n",
        "        row[f\"obj{i}\"] = val\n",
        "    row[\"actions\"] = e[\"actions\"]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "flat_csv = os.path.join(results_dir, \"phase8_flat_results.csv\")\n",
        "df.to_csv(flat_csv, index=False)\n",
        "print(\"Saved flattened CSV:\", flat_csv)\n",
        "\n",
        "# ---------- Pareto frontier by mean reward per weight ----------\n",
        "grouped = df.groupby(\"weight_index\")[[f\"obj{i}\" for i in range(reward_dim)]].mean().reset_index()\n",
        "points = grouped[[f\"obj{i}\" for i in range(reward_dim)]].values\n",
        "nd_mask = nondominated(points)\n",
        "pareto_df = grouped.loc[nd_mask].copy()\n",
        "pareto_df[\"weight\"] = grouped.loc[nd_mask, \"weight_index\"].apply(lambda idx: W[int(idx)].tolist())\n",
        "pareto_csv = os.path.join(results_dir, \"pareto_front_mean_by_weight.csv\")\n",
        "pareto_df.to_csv(pareto_csv, index=False)\n",
        "print(\"Saved Pareto CSV:\", pareto_csv)\n",
        "\n",
        "# ---------- Pareto plot ----------\n",
        "plt.figure(figsize=(8,6))\n",
        "if reward_dim == 2:\n",
        "    plt.scatter(points[:,0], points[:,1], alpha=0.5)\n",
        "    plt.scatter(pareto_df[f\"obj0\"], pareto_df[f\"obj1\"], color=\"red\", s=80, label=\"Pareto\")\n",
        "    plt.xlabel(\"obj0\"); plt.ylabel(\"obj1\")\n",
        "elif reward_dim == 3:\n",
        "    sc = plt.scatter(points[:,0], points[:,1], c=points[:,2], cmap=\"viridis\", alpha=0.7)\n",
        "    plt.scatter(pareto_df[f\"obj0\"], pareto_df[f\"obj1\"], facecolors='none', edgecolors='r', s=120, linewidths=1.5)\n",
        "    plt.xlabel(\"obj0\"); plt.ylabel(\"obj1\"); plt.colorbar(sc, label=\"obj2\")\n",
        "else:\n",
        "    plt.scatter(points[:,0], points[:,1], alpha=0.6)\n",
        "    plt.scatter(pareto_df[f\"obj0\"], pareto_df[f\"obj1\"], color=\"red\", s=80)\n",
        "    plt.xlabel(\"obj0\"); plt.ylabel(\"obj1\")\n",
        "plt.title(\"Pareto front (mean rewards per weight)\")\n",
        "plt.grid(alpha=0.2)\n",
        "plt.tight_layout()\n",
        "pareto_plot = os.path.join(results_dir, \"pareto_front_plot.png\")\n",
        "plt.savefig(pareto_plot, dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved Pareto plot:\", pareto_plot)\n",
        "\n",
        "# ---------- action frequency heatmap ----------\n",
        "# list all unique actions seen\n",
        "unique_actions = sorted({a for seq in df[\"actions\"] for a in seq})\n",
        "n_actions = len(unique_actions)\n",
        "freq = np.zeros((len(W), n_actions), dtype=int)\n",
        "for _, r in df.iterrows():\n",
        "    wi = int(r[\"weight_index\"])\n",
        "    for a in r[\"actions\"]:\n",
        "        freq[wi, unique_actions.index(a)] += 1\n",
        "\n",
        "# normalize rows\n",
        "row_sums = freq.sum(axis=1, keepdims=True)\n",
        "row_sums[row_sums==0] = 1\n",
        "freq_norm = freq / row_sums\n",
        "\n",
        "plt.figure(figsize=(10, max(4, len(W)*0.12)))\n",
        "plt.imshow(freq_norm, aspect='auto', interpolation='nearest')\n",
        "plt.colorbar(label=\"fraction\")\n",
        "plt.yticks(np.arange(len(W)), [f\"w{idx}\" for idx in range(len(W))])\n",
        "plt.xticks(np.arange(n_actions), unique_actions, rotation=90)\n",
        "plt.xlabel(\"action\"); plt.ylabel(\"weight index\")\n",
        "plt.title(\"Action frequency (weights x actions)\")\n",
        "plt.tight_layout()\n",
        "heatmap_path = os.path.join(results_dir, \"action_frequency_heatmap.png\")\n",
        "plt.savefig(heatmap_path, dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved heatmap:\", heatmap_path)\n",
        "pd.DataFrame(freq, columns=[f\"action_{a}\" for a in unique_actions]).to_csv(os.path.join(results_dir, \"action_freq_counts.csv\"), index=False)\n",
        "\n",
        "# ---------- radar/tradeoff plots for representative weights ----------\n",
        "# pick representative indices: extremes + some pareto ones\n",
        "rep_idxs = [0, len(W)-1] + list(map(int, grouped.loc[nd_mask, \"weight_index\"].tolist()[:4]))\n",
        "rep_idxs = [i for i in dict.fromkeys(rep_idxs) if 0 <= i < len(W)]\n",
        "labels = [f\"obj{i}\" for i in range(reward_dim)]\n",
        "angles = np.linspace(0, 2*np.pi, reward_dim, endpoint=False).tolist()\n",
        "angles += angles[:1]\n",
        "\n",
        "n_cols = 2\n",
        "n_rows = ceil(len(rep_idxs)/n_cols)\n",
        "plt.figure(figsize=(6*n_cols, 4*n_rows))\n",
        "for k, idx in enumerate(rep_idxs):\n",
        "    vals = grouped[grouped.weight_index==idx][[f\"obj{i}\" for i in range(reward_dim)]].values.flatten().tolist()\n",
        "    vals += vals[:1]\n",
        "    ax = plt.subplot(n_rows, n_cols, k+1, polar=True)\n",
        "    ax.plot(angles, vals, linewidth=2)\n",
        "    ax.fill(angles, vals, alpha=0.2)\n",
        "    ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
        "    ax.set_title(f\"weight idx {idx}\")\n",
        "plt.tight_layout()\n",
        "radar_path = os.path.join(results_dir, \"tradeoff_radar_plots.png\")\n",
        "plt.savefig(radar_path, dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved radar plots:\", radar_path)\n",
        "\n",
        "# ---------- reward correlation matrix ----------\n",
        "objs = [f\"obj{i}\" for i in range(reward_dim)]\n",
        "corr = df[objs].corr()\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.imshow(corr, vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(objs)), objs)\n",
        "plt.yticks(range(len(objs)), objs)\n",
        "for (i,j), val in np.ndenumerate(corr.values):\n",
        "    plt.text(j, i, f\"{val:.2f}\", ha='center', va='center', color='white' if abs(val)>0.5 else 'black')\n",
        "corr_path = os.path.join(results_dir, \"reward_correlation_matrix.png\")\n",
        "plt.savefig(corr_path, dpi=300)\n",
        "plt.close()\n",
        "corr.to_csv(os.path.join(results_dir, \"reward_correlation_matrix.csv\"))\n",
        "print(\"Saved correlation matrix:\", corr_path)\n",
        "\n",
        "# ---------- top/bottom episodes ----------\n",
        "for i in range(reward_dim):\n",
        "    col = f\"obj{i}\"\n",
        "    sorted_df = df.sort_values(col, ascending=False)\n",
        "    top = sorted_df.head(TOP_K)[[\"weight_index\",\"episode_index\",\"hadm_id\",col,\"actions\"]]\n",
        "    bottom = sorted_df.tail(TOP_K)[[\"weight_index\",\"episode_index\",\"hadm_id\",col,\"actions\"]]\n",
        "    top.to_csv(os.path.join(results_dir, f\"top_{i}_episodes.csv\"), index=False)\n",
        "    bottom.to_csv(os.path.join(results_dir, f\"bottom_{i}_episodes.csv\"), index=False)\n",
        "print(\"Saved top/bottom episode tables.\")\n",
        "\n",
        "# ---------- minimal JSON summary ----------\n",
        "summary = {\n",
        "    \"timestamp\": timestamp,\n",
        "    \"n_weights\": len(W),\n",
        "    \"episodes_per_weight\": EPISODES_PER_WEIGHT,\n",
        "    \"pareto_count\": int(nd_mask.sum()),\n",
        "    \"raw_json\": raw_json,\n",
        "    \"flat_csv\": flat_csv,\n",
        "    \"pareto_csv\": pareto_csv,\n",
        "    \"pareto_plot\": pareto_plot,\n",
        "    \"heatmap\": heatmap_path,\n",
        "    \"radar\": radar_path,\n",
        "    \"corr_matrix\": corr_path,\n",
        "}\n",
        "with open(os.path.join(results_dir, \"phase8_summary.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"Phase 8 complete. All artifacts saved to:\", results_dir)\n"
      ],
      "metadata": {
        "id": "C8iwIsHg99GC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d737434-597c-457d-e35a-9b83a49198e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 8 results folder: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009\n",
            "Using existing 'agent' in workspace.\n",
            "Agent reward_dim=3, action_space=Discrete(2)\n",
            "Sampled 40 weights on simplex.\n",
            "Weight 1/40: [0.337 0.328 0.335]\n",
            "Weight 2/40: [0.154 0.048 0.799]\n",
            "Weight 3/40: [0.306 0.677 0.017]\n",
            "Weight 4/40: [0.474 0.032 0.494]\n",
            "Weight 5/40: [0.517 0.115 0.368]\n",
            "Weight 6/40: [0.274 0.163 0.562]\n",
            "Weight 7/40: [0.352 0.161 0.487]\n",
            "Weight 8/40: [0.068 0.559 0.373]\n",
            "Weight 9/40: [0.43  0.273 0.297]\n",
            "Weight 10/40: [0.082 0.191 0.727]\n",
            "Weight 11/40: [0.165 0.535 0.3  ]\n",
            "Weight 12/40: [0.177 0.344 0.479]\n",
            "Weight 13/40: [0.223 0.208 0.569]\n",
            "Weight 14/40: [0.098 0.441 0.46 ]\n",
            "Weight 15/40: [0.473 0.032 0.495]\n",
            "Weight 16/40: [0.491 0.407 0.102]\n",
            "Weight 17/40: [0.382 0.197 0.421]\n",
            "Weight 18/40: [0.014 0.111 0.875]\n",
            "Weight 19/40: [0.825 0.116 0.059]\n",
            "Weight 20/40: [0.448 0.407 0.146]\n",
            "Weight 21/40: [0.273 0.347 0.38 ]\n",
            "Weight 22/40: [0.586 0.232 0.182]\n",
            "Weight 23/40: [0.621 0.353 0.026]\n",
            "Weight 24/40: [0.31  0.201 0.489]\n",
            "Weight 25/40: [0.842 0.138 0.021]\n",
            "Weight 26/40: [0.215 0.594 0.191]\n",
            "Weight 27/40: [0.508 0.179 0.313]\n",
            "Weight 28/40: [0.511 0.238 0.252]\n",
            "Weight 29/40: [0.062 0.029 0.909]\n",
            "Weight 30/40: [0.253 0.218 0.528]\n",
            "Weight 31/40: [0.055 0.565 0.38 ]\n",
            "Weight 32/40: [0.31  0.189 0.501]\n",
            "Weight 33/40: [0.345 0.086 0.57 ]\n",
            "Weight 34/40: [0.355 0.159 0.486]\n",
            "Weight 35/40: [0.408 0.223 0.369]\n",
            "Weight 36/40: [0.334 0.491 0.175]\n",
            "Weight 37/40: [0.087 0.857 0.056]\n",
            "Weight 38/40: [0.128 0.212 0.66 ]\n",
            "Weight 39/40: [0.127 0.21  0.663]\n",
            "Weight 40/40: [0.535 0.406 0.059]\n",
            "Saved raw JSON: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/phase8_raw_results.json\n",
            "Saved flattened CSV: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/phase8_flat_results.csv\n",
            "Saved Pareto CSV: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/pareto_front_mean_by_weight.csv\n",
            "Saved Pareto plot: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/pareto_front_plot.png\n",
            "Saved heatmap: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/action_frequency_heatmap.png\n",
            "Saved radar plots: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/tradeoff_radar_plots.png\n",
            "Saved correlation matrix: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/reward_correlation_matrix.png\n",
            "Saved top/bottom episode tables.\n",
            "Phase 8 complete. All artifacts saved to: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 8 â€” Generate PDF report from Phase8 artifacts\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from PIL import Image\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "# set results_dir to the same folder you used in Phase 8\n",
        "# If you run this immediately after Phase 8, results_dir exists in the session.\n",
        "try:\n",
        "    results_dir  # noqa\n",
        "except NameError:\n",
        "    # fallback: change this path to the timestamped folder created by Phase 8\n",
        "    results_dir = \"/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_YYYYMMDD_HHMMSS\"\n",
        "\n",
        "pdf_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "pdf_path = os.path.join(results_dir, f\"phase8_report_{pdf_timestamp}.pdf\")\n",
        "meta_json = os.path.join(results_dir, \"phase8_summary.json\")\n",
        "flat_csv = os.path.join(results_dir, \"phase8_flat_results.csv\")\n",
        "\n",
        "# Candidate figure files (names used by Phase 8)\n",
        "candidates = {\n",
        "    \"pareto_plot\": os.path.join(results_dir, \"pareto_front_plot.png\"),\n",
        "    \"heatmap\": os.path.join(results_dir, \"action_frequency_heatmap.png\"),\n",
        "    \"radar\": os.path.join(results_dir, \"tradeoff_radar_plots.png\"),\n",
        "    \"corr\": os.path.join(results_dir, \"reward_correlation_matrix.png\"),\n",
        "    # Add any additional images you want to include if they exist\n",
        "    \"pareto_front_mean_by_weight\": os.path.join(results_dir, \"pareto_front_mean_by_weight.csv\"),\n",
        "}\n",
        "\n",
        "# Load summary if present\n",
        "summary = {}\n",
        "if os.path.exists(meta_json):\n",
        "    try:\n",
        "        with open(meta_json, \"r\") as f:\n",
        "            summary = json.load(f)\n",
        "    except Exception:\n",
        "        summary = {}\n",
        "\n",
        "# Helper to safely add an image page\n",
        "def add_image_page(pp: PdfPages, image_path: str, title: str = \"\", caption: str = \"\"):\n",
        "    fig, ax = plt.subplots(figsize=(11.69, 8.27))  # A4 landscape\n",
        "    ax.axis(\"off\")\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        ax.imshow(img)\n",
        "    except Exception as e:\n",
        "        ax.text(0.5, 0.5, f\"Could not load image:\\n{image_path}\\n\\n{e}\", ha=\"center\", va=\"center\", wrap=True)\n",
        "    # Title\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=14, fontweight=\"bold\", y=0.98)\n",
        "    # Caption in footer\n",
        "    if caption:\n",
        "        plt.figtext(0.02, 0.02, caption, wrap=True, ha=\"left\", fontsize=9)\n",
        "    pp.savefig(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "# ---------------- Build PDF ----------------\n",
        "with PdfPages(pdf_path) as pp:\n",
        "    # Cover page\n",
        "    fig, ax = plt.subplots(figsize=(11.69, 8.27))\n",
        "    ax.axis(\"off\")\n",
        "    title = \"Phase 8 â€” Post-Training Analysis Report\"\n",
        "    ax.text(0.5, 0.78, title, ha=\"center\", va=\"center\", fontsize=22, fontweight=\"bold\")\n",
        "    sub = f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "    ax.text(0.5, 0.74, sub, ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "    # Summary block: try to extract basic stats\n",
        "    n_weights = summary.get(\"n_weights\", \"N/A\")\n",
        "    episodes_per_weight = summary.get(\"episodes_per_weight\", summary.get(\"n_eval_episodes_per_weight\", \"N/A\"))\n",
        "    pareto_count = summary.get(\"pareto_count\", \"N/A\")\n",
        "    mean_rewards = summary.get(\"mean_rewards\", {})\n",
        "    std_rewards = summary.get(\"std_rewards\", {})\n",
        "\n",
        "    left_block = (\n",
        "        f\"Results folder: {os.path.basename(results_dir)}\\n\\n\"\n",
        "        f\"Weights sampled: {n_weights}\\n\"\n",
        "        f\"Episodes/weight: {episodes_per_weight}\\n\"\n",
        "        f\"Pareto points found (by mean): {pareto_count}\\n\"\n",
        "    )\n",
        "    ax.text(0.05, 0.55, left_block, fontsize=10, ha=\"left\", va=\"top\", family=\"monospace\")\n",
        "\n",
        "    # Add mean/std table if available\n",
        "    ms_lines = []\n",
        "    if mean_rewards:\n",
        "        ms_lines.append(\"Mean Â± Std (per objective):\")\n",
        "        for k, v in mean_rewards.items():\n",
        "            stdv = std_rewards.get(k, 0.0)\n",
        "            ms_lines.append(f\"  {k}: {v:.4f} Â± {stdv:.4f}\")\n",
        "    else:\n",
        "        ms_lines.append(\"Mean/std not found in summary.\")\n",
        "\n",
        "    ax.text(0.05, 0.35, \"\\n\".join(ms_lines), fontsize=9, ha=\"left\", va=\"top\", family=\"monospace\")\n",
        "\n",
        "    # Auto-generated interpretation (short)\n",
        "    interp = (\n",
        "        \"Interpretation (auto-generated):\\n\"\n",
        "        \"â€¢ The Pareto plot shows trade-offs between objectives across sampled weight vectors.\\n\"\n",
        "        \"â€¢ The action-frequency heatmap highlights which actions (drugs) are preferred under different weights.\\n\"\n",
        "        \"â€¢ Radar plots summarize per-weight tradeoffs; correlation matrix shows objective correlations.\\n\\n\"\n",
        "        \"Limitations:\\n\"\n",
        "        \"â€¢ Evaluation uses agent.eval() with per-weight greedy selection â€” may not capture stochasticity.\\n\"\n",
        "        \"â€¢ Clinical interpretation requires mapping actions â†’ drug names and domain expert review.\\n\"\n",
        "    )\n",
        "    ax.text(0.05, 0.06, interp, fontsize=9, ha=\"left\", va=\"bottom\")\n",
        "    pp.savefig(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Add available figures with captions\n",
        "    if os.path.exists(candidates[\"pareto_plot\"]):\n",
        "        add_image_page(pp, candidates[\"pareto_plot\"], title=\"Pareto Scatter (mean per weight)\",\n",
        "                       caption=\"Pareto scatter: mean reward vector for each sampled weight. Red markers indicate estimated Pareto frontier.\")\n",
        "    else:\n",
        "        add_image_page(pp, candidates[\"pareto_plot\"], title=\"Pareto Scatter\", caption=\"Image not found; skipped.\")\n",
        "\n",
        "    if os.path.exists(candidates[\"heatmap\"]):\n",
        "        add_image_page(pp, candidates[\"heatmap\"], title=\"Action Frequency Heatmap\",\n",
        "                       caption=\"Rows = weight vectors, Columns = action indices. Values = fraction of picks across episodes.\")\n",
        "    if os.path.exists(candidates[\"radar\"]):\n",
        "        add_image_page(pp, candidates[\"radar\"], title=\"Tradeoff Radar Plots\",\n",
        "                       caption=\"Radar plots of mean objective values for representative weight vectors.\")\n",
        "    if os.path.exists(candidates[\"corr\"]):\n",
        "        add_image_page(pp, candidates[\"corr\"], title=\"Reward Correlation Matrix\",\n",
        "                       caption=\"Pearson correlation matrix between objective returns across all evaluated episodes.\")\n",
        "\n",
        "    # Add a raw CSV preview page (first few rows) if exists\n",
        "    if os.path.exists(flat_csv := os.path.join(results_dir, \"phase8_flat_results.csv\")):\n",
        "        try:\n",
        "            df_preview = pd.read_csv(flat_csv).head(20)\n",
        "            fig, ax = plt.subplots(figsize=(11.69, 8.27))\n",
        "            ax.axis(\"off\")\n",
        "            ax.table(cellText=df_preview.values, colLabels=df_preview.columns, loc=\"center\", cellLoc=\"center\")\n",
        "            fig.suptitle(\"Flat results preview (first 20 rows)\", fontsize=12)\n",
        "            pp.savefig(fig)\n",
        "            plt.close(fig)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Final page: recommended next steps\n",
        "    fig, ax = plt.subplots(figsize=(11.69, 8.27))\n",
        "    ax.axis(\"off\")\n",
        "    next_steps = [\n",
        "        \"Recommended next steps:\",\n",
        "        \"1) Map action indices â†’ drug names and produce a clinical interpretation table.\",\n",
        "        \"2) Run targeted weight sweeps around Pareto regions for higher resolution.\",\n",
        "        \"3) Build a Streamlit demo to interactively toggle weights and visualize policy actions.\",\n",
        "        \"4) Prepare a write-up or manuscript with methods, experiments, and clinical validation plan.\"\n",
        "    ]\n",
        "    ax.text(0.05, 0.9, \"Next steps (suggested)\", fontsize=14, fontweight=\"bold\")\n",
        "    ax.text(0.05, 0.6, \"\\n\".join(next_steps), fontsize=11, va=\"top\")\n",
        "    ax.text(0.05, 0.05, f\"Report generated from folder: {os.path.basename(results_dir)}\", fontsize=8, va=\"bottom\")\n",
        "    pp.savefig(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "print(\"âœ… PDF report generated at:\", pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvVd3bMLSSEC",
        "outputId": "28b1d47f-f82e-4cda-d524-747c86f1a372"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… PDF report generated at: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/phase8_report_20251118_050115.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1_update_summary_mean_std.py\n",
        "import os, json, pandas as pd\n",
        "\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009\"\n",
        "SUMMARY_PATH = os.path.join(RESULTS_DIR, \"phase8_summary.json\")\n",
        "FLAT_CSV = os.path.join(RESULTS_DIR, \"phase8_flat_results.csv\")\n",
        "\n",
        "# Load flat results\n",
        "df = pd.read_csv(FLAT_CSV)\n",
        "# Detect objective columns automatically (obj0, obj1, ...)\n",
        "obj_cols = [c for c in df.columns if c.startswith(\"obj\")]\n",
        "if len(obj_cols) == 0:\n",
        "    raise RuntimeError(\"No objective columns (obj0...) found in flat CSV: \" + FLAT_CSV)\n",
        "\n",
        "# Compute mean/std\n",
        "mean_rewards = df[obj_cols].mean().to_dict()\n",
        "std_rewards  = df[obj_cols].std().to_dict()\n",
        "\n",
        "# Load existing summary (or create)\n",
        "if os.path.exists(SUMMARY_PATH):\n",
        "    with open(SUMMARY_PATH, \"r\") as f:\n",
        "        summary = json.load(f)\n",
        "else:\n",
        "    summary = {}\n",
        "\n",
        "# Update\n",
        "summary[\"mean_rewards\"] = {k: float(v) for k,v in mean_rewards.items()}\n",
        "summary[\"std_rewards\"]  = {k: float(v) for k,v in std_rewards.items()}\n",
        "\n",
        "# Save back\n",
        "with open(SUMMARY_PATH, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"Updated summary JSON with mean/std at:\", SUMMARY_PATH)\n",
        "print(\"Mean rewards:\", summary[\"mean_rewards\"])\n",
        "print(\"Std rewards:\", summary[\"std_rewards\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUG7JDvsVp-U",
        "outputId": "854ecdb6-4abc-4a8f-b5b1-9565d9de501a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated summary JSON with mean/std at: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/phase8_summary.json\n",
            "Mean rewards: {'obj0': 0.12897999889403583, 'obj1': -0.02736000108718872, 'obj2': -2.630034459605813}\n",
            "Std rewards: {'obj0': 0.22932434108126395, 'obj1': 0.13769572238106279, 'obj2': 2.525901678121713}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute_policy_stats_and_figures.py\n",
        "import os, json, ast\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009\"\n",
        "FLAT_CSV = os.path.join(RESULTS_DIR, \"phase8_flat_results.csv\")\n",
        "SUMMARY_JSON = os.path.join(RESULTS_DIR, \"phase8_summary.json\")\n",
        "\n",
        "# load\n",
        "df = pd.read_csv(FLAT_CSV)\n",
        "# determine objective columns automatically\n",
        "obj_cols = [c for c in df.columns if c.startswith(\"obj\")]\n",
        "reward_dim = len(obj_cols)\n",
        "print(\"Detected objective columns:\", obj_cols)\n",
        "\n",
        "# parse actions column (if stored as list-like strings)\n",
        "def parse_actions(x):\n",
        "    if isinstance(x, str):\n",
        "        try:\n",
        "            return ast.literal_eval(x)\n",
        "        except Exception:\n",
        "            # fallback: simple split\n",
        "            return [int(v) for v in x.strip(\"[]\").split(\",\") if v!='']\n",
        "    if isinstance(x, (list, tuple, np.ndarray)):\n",
        "        return list(x)\n",
        "    return []\n",
        "\n",
        "df['actions_parsed'] = df['actions'].apply(parse_actions)\n",
        "\n",
        "# -- Frequency: how often each action/policy is taken across ALL episodes --\n",
        "from collections import Counter\n",
        "counter = Counter()\n",
        "for seq in df['actions_parsed']:\n",
        "    counter.update(seq)\n",
        "action_indices = sorted(counter.keys())\n",
        "freq_df = pd.DataFrame({\n",
        "    'action_index': action_indices,\n",
        "    'count': [counter[i] for i in action_indices]\n",
        "})\n",
        "freq_df['fraction_of_total_action_calls'] = freq_df['count'] / freq_df['count'].sum()\n",
        "\n",
        "# -- Per-action episode-level presence & mean per-objective totals --\n",
        "# join per-episode totals to actions: each row in df is an episode's totals\n",
        "exploded_rows = []\n",
        "for _, row in df.iterrows():\n",
        "    acts = row['actions_parsed']\n",
        "    for a in acts:\n",
        "        d = {c: row[c] for c in obj_cols}\n",
        "        d.update({\n",
        "            'weight_index': int(row['weight_index']),\n",
        "            'episode_index': int(row['episode_index']),\n",
        "            'action': int(a)\n",
        "        })\n",
        "        exploded_rows.append(d)\n",
        "expl = pd.DataFrame(exploded_rows)\n",
        "\n",
        "# episodes containing each action (unique episode counts)\n",
        "presence_counts = expl.groupby('action').agg({'episode_index':'nunique'}).rename(columns={'episode_index':'n_episodes_with_action'}).reset_index()\n",
        "\n",
        "# average episode totals for episodes where action appears (coarse proxy)\n",
        "agg = expl.groupby('action')[obj_cols].mean().reset_index()\n",
        "agg = agg.merge(presence_counts, on='action', how='left')\n",
        "agg = agg.merge(freq_df.rename(columns={'action_index':'action'}), left_on='action', right_on='action', how='left')\n",
        "\n",
        "# Save tables\n",
        "freq_path = os.path.join(RESULTS_DIR, \"action_frequency_by_policy.csv\")\n",
        "agg_path  = os.path.join(RESULTS_DIR, \"per_policy_outcome_stats.csv\")\n",
        "freq_df.to_csv(freq_path, index=False)\n",
        "agg.to_csv(agg_path, index=False)\n",
        "\n",
        "print(\"Saved:\", freq_path)\n",
        "print(\"Saved:\", agg_path)\n",
        "print(agg.head())\n",
        "\n",
        "# -- Figures --\n",
        "# 1) Bar chart: frequency of each action (policy)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(freq_df['action_index'].astype(str), freq_df['count'])\n",
        "plt.xlabel(\"Action (policy index)\")\n",
        "plt.ylabel(\"Total action calls\")\n",
        "plt.title(\"Action (policy) call counts across all evaluated episodes\")\n",
        "plt.tight_layout()\n",
        "freq_plot = os.path.join(RESULTS_DIR, \"action_frequency_barplot.png\")\n",
        "plt.savefig(freq_plot, dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved:\", freq_plot)\n",
        "\n",
        "# 2) Radar-like spider for per-policy mean objectives (simple line plot)\n",
        "plt.figure(figsize=(6,4))\n",
        "for _, r in agg.iterrows():\n",
        "    vals = [r[c] for c in obj_cols]\n",
        "    plt.plot(range(len(vals)), vals, marker='o', label=f\"policy {int(r['action'])}\")\n",
        "plt.xticks(range(len(obj_cols)), obj_cols)\n",
        "plt.xlabel(\"Objective\")\n",
        "plt.ylabel(\"Mean episode total (for episodes where policy appears)\")\n",
        "plt.title(\"Per-policy mean objective totals (episodes where policy present)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "radar_plot = os.path.join(RESULTS_DIR, \"per_policy_mean_objectives.png\")\n",
        "plt.savefig(radar_plot, dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved:\", radar_plot)\n",
        "\n",
        "# 3) Optional: save small summary JSON with top/bottom policies by tolerability\n",
        "summary = {}\n",
        "summary['top_policy_by_tolerability'] = agg.sort_values('obj2', ascending=False).head(1)['action'].tolist()\n",
        "summary['bottom_policy_by_tolerability'] = agg.sort_values('obj2', ascending=True).head(1)['action'].tolist()\n",
        "with open(SUMMARY_JSON, 'r') as f:\n",
        "    s = json.load(f)\n",
        "s.update(summary)\n",
        "with open(SUMMARY_JSON, 'w') as f:\n",
        "    json.dump(s, f, indent=2)\n",
        "print(\"Appended quick policy rankings to summary JSON.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG1H8DnWXIc7",
        "outputId": "08482354-2c3f-4938-8778-2f72bca98f55"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected objective columns: ['obj0', 'obj1', 'obj2']\n",
            "Saved: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/action_frequency_by_policy.csv\n",
            "Saved: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/per_policy_outcome_stats.csv\n",
            "   action      obj0      obj1      obj2  n_episodes_with_action  count  \\\n",
            "0       0  0.128877 -0.028047 -2.607255                      50   1951   \n",
            "1       1  0.133061  0.000000 -3.537041                      42     49   \n",
            "\n",
            "   fraction_of_total_action_calls  \n",
            "0                          0.9755  \n",
            "1                          0.0245  \n",
            "Saved: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/action_frequency_barplot.png\n",
            "Saved: /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009/per_policy_mean_objectives.png\n",
            "Appended quick policy rankings to summary JSON.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MANUSCRIPT = \"/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/manuscript.md\"\n",
        "CLINICAL_MD = \"/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/clinical_interp.md\"\n",
        "\n",
        "# write clinical_md using the template above and the computed agg table\n",
        "# (run after compute_policy_stats_and_figures.py so CSVs exist)\n",
        "import pandas as pd, os\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/phase8_results_20251118_050009\"\n",
        "agg = pd.read_csv(os.path.join(RESULTS_DIR, \"per_policy_outcome_stats.csv\"))\n",
        "freq = pd.read_csv(os.path.join(RESULTS_DIR, \"action_frequency_by_policy.csv\"))\n",
        "\n",
        "p0 = agg[agg['action']==0].iloc[0]\n",
        "p1 = agg[agg['action']==1].iloc[0]\n",
        "\n",
        "clinical_text = f'''\n",
        "## Clinical interpretation\n",
        "\n",
        "**Environment & decision abstraction.**\n",
        "The agent operates over a high-level treatment-policy selection environment (PolypharmacyEnv). The action space is binary (Discrete(2)) and corresponds to two treatment policies rather than individual drug selections. We therefore interpret actions as **policy-level decisions**:\n",
        "\n",
        "- **Policy 0** â€” Conservative treatment policy (lower predicted tolerability burden).\n",
        "- **Policy 1** â€” Aggressive treatment policy (higher predicted tolerability burden).\n",
        "\n",
        "**How policies behave in evaluation.**\n",
        "Policy call counts (total calls across all evaluated episodes):\n",
        "- Policy 0 total calls: {int(freq[freq['action_index']==0]['count'].values[0])}\n",
        "- Policy 1 total calls: {int(freq[freq['action_index']==1]['count'].values[0])}\n",
        "\n",
        "Per-policy mean rewards (computed only over episodes where that policy was present):\n",
        "- Policy 0 mean rewards ({\" , \".join(obj_cols)}): {', '.join([f'{c}: {p0[c]:.4f}' for c in obj_cols])}\n",
        "- Policy 1 mean rewards ({\" , \".join(obj_cols)}): {', '.join([f'{c}: {p1[c]:.4f}' for c in obj_cols])}\n",
        "\n",
        "> Interpretation: policy 0 yields relatively better tolerability (less negative `neg_tol`) than policy 1, at similar efficacy levels. Policy 1 tends to produce a more negative tolerability score, indicating higher adverse effect burden in episodes where it is present.\n",
        "\n",
        "**Clinical implications and caveats.**\n",
        "1. These results do **not** directly translate to drug-level prescriptions. Instead, they indicate the agentâ€™s preference between two coarse strategies (conservative vs aggressive).\n",
        "2. The tolerability outcome (`neg_tol`) is a proxy derived from dataset features and the environmentâ€™s reward function; it should be validated with clinicians before any actionable inference.\n",
        "3. The per-policy mean outcomes are associative: an episode where a policy appears and shows worse tolerability is not evidence that the policy causes worse outcomes without causal analysis or controlled experiments.\n",
        "\n",
        "'''\n",
        "# append\n",
        "with open(CLINICAL_MD, 'w') as f:\n",
        "    f.write(clinical_text)\n",
        "\n",
        "with open(MANUSCRIPT, 'a') as f:\n",
        "    f.write(\"\\n\\n\")\n",
        "    f.write(clinical_text)\n",
        "\n",
        "print(\"Wrote clinical interpretation to\", CLINICAL_MD, \"and appended to\", MANUSCRIPT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvMsYeXHYAoN",
        "outputId": "035a9eba-4c3b-4e10-d9fc-c90be85f2991"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote clinical interpretation to /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/clinical_interp.md and appended to /content/drive/MyDrive/ML Patent/polypharmacy_project/phase7_results/manuscript.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean model"
      ],
      "metadata": {
        "id": "KCW3qfh6pgF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "ckpt = torch.load(\"gpipd_model_clean.pt\", map_location=\"cpu\")\n",
        "\n",
        "print(\"=== Q-NET 0 STATE_DICT SHAPES ===\")\n",
        "sd = ckpt[\"q_nets\"][0]\n",
        "for k,v in sd.items():\n",
        "    print(k, v.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MG2BVjubWeF",
        "outputId": "e0c08f79-3f9e-4fb6-e92c-ffe97af66d3b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Q-NET 0 STATE_DICT SHAPES ===\n",
            "weights_features.0.weight torch.Size([128, 3])\n",
            "weights_features.0.bias torch.Size([128])\n",
            "state_features.0.weight torch.Size([128, 4])\n",
            "state_features.0.bias torch.Size([128])\n",
            "net.0.weight torch.Size([128, 128])\n",
            "net.0.bias torch.Size([128])\n",
            "net.2.weight torch.Size([128])\n",
            "net.2.bias torch.Size([128])\n",
            "net.4.weight torch.Size([6, 128])\n",
            "net.4.bias torch.Size([6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sd in enumerate(ckpt[\"q_nets\"]):\n",
        "    print(f\"\\n=== Q-NET {i} ===\")\n",
        "    for k,v in sd.items():\n",
        "        print(k, tuple(v.shape))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IJlBEL1tKDB",
        "outputId": "2019d62e-5289-4954-a5d4-6491352f837d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Q-NET 0 ===\n",
            "weights_features.0.weight (128, 3)\n",
            "weights_features.0.bias (128,)\n",
            "state_features.0.weight (128, 4)\n",
            "state_features.0.bias (128,)\n",
            "net.0.weight (128, 128)\n",
            "net.0.bias (128,)\n",
            "net.2.weight (128,)\n",
            "net.2.bias (128,)\n",
            "net.4.weight (6, 128)\n",
            "net.4.bias (6,)\n",
            "\n",
            "=== Q-NET 1 ===\n",
            "weights_features.0.weight (128, 3)\n",
            "weights_features.0.bias (128,)\n",
            "state_features.0.weight (128, 4)\n",
            "state_features.0.bias (128,)\n",
            "net.0.weight (128, 128)\n",
            "net.0.bias (128,)\n",
            "net.2.weight (128,)\n",
            "net.2.bias (128,)\n",
            "net.4.weight (6, 128)\n",
            "net.4.bias (6,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O1bb2-HytNqz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}